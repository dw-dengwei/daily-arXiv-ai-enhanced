{"id": "2506.04307", "pdf": "https://arxiv.org/pdf/2506.04307", "abs": "https://arxiv.org/abs/2506.04307", "authors": ["Christos Karapapas", "Iakovos Pittaras", "George C. Polyzos", "Constantinos Patsakis"], "title": "Hello, won't you tell me your name?: Investigating Anonymity Abuse in IPFS", "categories": ["cs.CR"], "comment": "To appear at 13th International Workshop on Cyber Crime (IWCC), in\n  conjunction with the 19th International Conference on Availability,\n  Reliability and Security (ARES)", "summary": "The InterPlanetary File System~(IPFS) offers a decentralized approach to file\nstorage and sharing, promising resilience and efficiency while also realizing\nthe Web3 paradigm. Simultaneously, the offered anonymity raises significant\nquestions about potential misuse. In this study, we explore methods that\nmalicious actors can exploit IPFS to upload and disseminate harmful content\nwhile remaining anonymous. We evaluate the role of pinning services and public\ngateways, identifying their capabilities and limitations in maintaining content\navailability. Using scripts, we systematically test the behavior of these\nservices by uploading malicious files. Our analysis reveals that pinning\nservices and public gateways lack mechanisms to assess or restrict the\npropagation of malicious content.", "AI": {"tldr": "研究IPFS在匿名传播恶意内容方面的局限性。", "motivation": "研究恶意行为者如何利用IPFS匿名上传和传播有害内容。", "method": "使用脚本系统地测试了这些服务的行为，通过上传恶意文件。", "result": "发现固定服务和公共网关缺乏评估或限制恶意内容传播的机制。", "conclusion": "固定服务和公共网关在维护内容可用性方面存在局限性。"}}
{"id": "2506.04383", "pdf": "https://arxiv.org/pdf/2506.04383", "abs": "https://arxiv.org/abs/2506.04383", "authors": ["Mohamed Aly Bouke"], "title": "The Hashed Fractal Key Recovery (HFKR) Problem: From Symbolic Path Inversion to Post-Quantum Cryptographic Keys", "categories": ["cs.CR"], "comment": null, "summary": "Classical cryptographic systems rely heavily on structured algebraic\nproblems, such as factorization, discrete logarithms, or lattice-based\nassumptions, which are increasingly vulnerable to quantum attacks and\nstructural cryptanalysis. In response, this work introduces the Hashed Fractal\nKey Recovery (HFKR) problem, a non-algebraic cryptographic construction\ngrounded in symbolic dynamics and chaotic perturbations. HFKR builds on the\nSymbolic Path Inversion Problem (SPIP), leveraging symbolic trajectories\ngenerated via contractive affine maps over $\\mathbb{Z}^2$, and compressing them\ninto fixed-length cryptographic keys using hash-based obfuscation. A key\ncontribution of this paper is the empirical confirmation that these symbolic\npaths exhibit fractal behavior, quantified via box counting dimension, path\ngeometry, and spatial density measures. The observed fractal dimension\nincreases with trajectory length and stabilizes near 1.06, indicating symbolic\nself-similarity and space-filling complexity, both of which reinforce the\nentropy foundation of the scheme. Experimental results across 250 perturbation\ntrials show that SHA3-512 and SHAKE256 amplify symbolic divergence effectively,\nachieving mean Hamming distances near 255, ideal bit-flip rates, and negligible\nentropy deviation. In contrast, BLAKE3 exhibits statistically uniform but\nweaker diffusion. These findings confirm that HFKR post-quantum security arises\nfrom the synergy between symbolic fractality and hash-based entropy\namplification. The resulting construction offers a lightweight, structure-free\nfoundation for secure key generation in adversarial settings without relying on\nalgebraic hardness assumptions.", "AI": {"tldr": "HFKR通过符号分数维和哈希熵放大提高密码安全性。", "motivation": "应对量子攻击和结构密码分析对传统密码系统的威胁。", "method": "提出Hashed Fractal Key Recovery (HFKR)问题，基于符号动力系统和混沌扰动，利用哈希混淆将符号轨迹压缩为密钥。", "result": "HFKR方案通过符号分数维和哈希熵放大提高了安全性，实验结果表明SHA3-512和SHAKE256有效，BLAKE3扩散较弱。", "conclusion": "HFKR基于符号动力系统和混沌扰动，通过哈希混淆将符号轨迹压缩为固定长度的密钥，并证实了这些轨迹的分数维行为，从而提高了方案的熵基础。实验结果表明，SHA3-512和SHAKE256有效地放大了符号发散，而BLAKE3的扩散较弱。"}}
{"id": "2506.04390", "pdf": "https://arxiv.org/pdf/2506.04390", "abs": "https://arxiv.org/abs/2506.04390", "authors": ["Sarthak Choudhary", "Nils Palumbo", "Ashish Hooda", "Krishnamurthy Dj Dvijotham", "Somesh Jha"], "title": "Through the Stealth Lens: Rethinking Attacks and Defenses in RAG", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems are vulnerable to attacks that\ninject poisoned passages into the retrieved set, even at low corruption rates.\nWe show that existing attacks are not designed to be stealthy, allowing\nreliable detection and mitigation. We formalize stealth using a\ndistinguishability-based security game. If a few poisoned passages are designed\nto control the response, they must differentiate themselves from benign ones,\ninherently compromising stealth. This motivates the need for attackers to\nrigorously analyze intermediate signals involved in\ngeneration$\\unicode{x2014}$such as attention patterns or next-token probability\ndistributions$\\unicode{x2014}$to avoid easily detectable traces of\nmanipulation. Leveraging attention patterns, we propose a passage-level\nscore$\\unicode{x2014}$the Normalized Passage Attention\nScore$\\unicode{x2014}$used by our Attention-Variance Filter algorithm to\nidentify and filter potentially poisoned passages. This method mitigates\nexisting attacks, improving accuracy by up to $\\sim 20 \\%$ over baseline\ndefenses. To probe the limits of attention-based defenses, we craft stealthier\nadaptive attacks that obscure such traces, achieving up to $35 \\%$ attack\nsuccess rate, and highlight the challenges in improving stealth.", "AI": {"tldr": "提出一种基于注意力模式的段落级得分，用于识别和过滤潜在的恶意段落，提高RAG系统防御能力。", "motivation": "研究检索增强生成（RAG）系统易受攻击，攻击者通过注入恶意段落到检索集中进行攻击。", "method": "提出一种基于注意力模式的段落级得分（Normalized Passage Attention Score），用于识别和过滤潜在的恶意段落。", "result": "该方法可以缓解现有攻击，提高准确率高达20%。", "conclusion": null}}
{"id": "2506.04450", "pdf": "https://arxiv.org/pdf/2506.04450", "abs": "https://arxiv.org/abs/2506.04450", "authors": ["Payel Bhattacharjee", "Fengwei Tian", "Ravi Tandon", "Joseph Lo", "Heidi Hanson", "Geoffrey Rubin", "Nirav Merchant", "John Gounley"], "title": "Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "19 pages, 5 figures, 2 tables", "summary": "Purpose: This study proposes a framework for fine-tuning large language\nmodels (LLMs) with differential privacy (DP) to perform multi-abnormality\nclassification on radiology report text. By injecting calibrated noise during\nfine-tuning, the framework seeks to mitigate the privacy risks associated with\nsensitive patient data and protect against data leakage while maintaining\nclassification performance. Materials and Methods: We used 50,232 radiology\nreports from the publicly available MIMIC-CXR chest radiography and CT-RATE\ncomputed tomography datasets, collected between 2011 and 2019. Fine-tuning of\nLLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels\nfrom CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)\nin high and moderate privacy regimes (across a range of privacy budgets =\n{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1\nscore across three model architectures: BERT-medium, BERT-small, and\nALBERT-base. Statistical analyses compared model performance across different\nprivacy levels to quantify the privacy-utility trade-off. Results: We observe a\nclear privacy-utility trade-off through our experiments on 2 different datasets\nand 3 different models. Under moderate privacy guarantees the DP fine-tuned\nmodels achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on\nCT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.\nConclusion: Differentially private fine-tuning using LoRA enables effective and\nprivacy-preserving multi-abnormality classification from radiology reports,\naddressing a key challenge in fine-tuning LLMs on sensitive medical data.", "AI": {"tldr": "A framework for fine-tuning LLMs with DP for multi-abnormality classification on radiology report text is proposed, achieving comparable performance while preserving privacy.", "motivation": "To mitigate the privacy risks associated with sensitive patient data and protect against data leakage while maintaining classification performance.", "method": "Using Differentially Private Low-Rank Adaptation (DP-LoRA) in high and moderate privacy regimes, fine-tuning LLMs on radiology report text.", "result": "DP fine-tuned models achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on CT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.", "conclusion": "Differentially private fine-tuning using LoRA enables effective and privacy-preserving multi-abnormality classification from radiology reports, addressing a key challenge in fine-tuning LLMs on sensitive medical data."}}
{"id": "2506.04556", "pdf": "https://arxiv.org/pdf/2506.04556", "abs": "https://arxiv.org/abs/2506.04556", "authors": ["Xuhao Ren", "Haotian Liang", "Yajie Wang", "Chuan Zhang", "Zehui Xiong", "Liehuang Zhu"], "title": "BESA: Boosting Encoder Stealing Attack with Perturbation Recovery", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "To boost the encoder stealing attack under the perturbation-based defense\nthat hinders the attack performance, we propose a boosting encoder stealing\nattack with perturbation recovery named BESA. It aims to overcome\nperturbation-based defenses. The core of BESA consists of two modules:\nperturbation detection and perturbation recovery, which can be combined with\ncanonical encoder stealing attacks. The perturbation detection module utilizes\nthe feature vectors obtained from the target encoder to infer the defense\nmechanism employed by the service provider. Once the defense mechanism is\ndetected, the perturbation recovery module leverages the well-designed\ngenerative model to restore a clean feature vector from the perturbed one.\nThrough extensive evaluations based on various datasets, we demonstrate that\nBESA significantly enhances the surrogate encoder accuracy of existing encoder\nstealing attacks by up to 24.63\\% when facing state-of-the-art defenses and\ncombinations of multiple defenses.", "AI": {"tldr": "BESA是一种增强编码器窃取攻击的方法，显著提高了其精度。", "motivation": "克服基于扰动的防御措施，提高编码器窃取攻击的性能。", "method": "BESA由两个模块组成：扰动检测和扰动恢复，可以与规范编码器窃取攻击结合使用。", "result": "在多个数据集上的广泛评估表明，BESA在面临最先进的防御和多种防御组合时，将现有编码器窃取攻击的代理编码器精度提高了24.63%。", "conclusion": "BESA显著提高了现有编码器窃取攻击的代理编码器精度，最高可达24.63%。"}}
{"id": "2506.04634", "pdf": "https://arxiv.org/pdf/2506.04634", "abs": "https://arxiv.org/abs/2506.04634", "authors": ["Mridu Nanda", "Michael K. Reiter"], "title": "Incentivizing Collaborative Breach Detection", "categories": ["cs.CR"], "comment": null, "summary": "Decoy passwords, or \"honeywords,\" alert a site to its breach if they are ever\nentered in a login attempt on that site. However, an attacker can identify a\nuser-chosen password from among the decoys, without risk of alerting the site\nto its breach, by performing credential stuffing, i.e., entering the stolen\npasswords at another site where the same user reused her password. Prior work\nhas thus proposed that sites monitor for the entry of their honeywords at other\nsites. Unfortunately, it is not clear what incentives sites have to participate\nin this monitoring. In this paper we propose and evaluate an algorithm by which\nsites can exchange monitoring favors. Through a model-checking analysis, we\nshow that using our algorithm, a site improves its ability to detect its own\nbreach when it increases the monitoring effort it expends for other sites. We\nadditionally quantify the impacts of various parameters on detection\neffectiveness and their implications for the deployment of a system to support\na monitoring ecosystem. Finally, we evaluate our algorithm on a real dataset of\nbreached credentials and provide a performance analysis that confirms its\nscalability and practical viability.", "AI": {"tldr": "提出了一种通过交换监控服务提高网站漏洞检测能力的算法。", "motivation": "为了解决网站监控自身漏洞的动机，提出了交换监控服务的算法。", "method": "通过模型检查分析，展示了该算法在增加对其他网站的监控努力时，可以提高检测自身漏洞的能力。", "result": "算法在真实数据集上进行了评估，并提供了性能分析，证实了其可扩展性和实用性。", "conclusion": "提出了一种算法，通过交换监控服务，提高网站检测自身漏洞的能力。"}}
{"id": "2506.04647", "pdf": "https://arxiv.org/pdf/2506.04647", "abs": "https://arxiv.org/abs/2506.04647", "authors": ["Zixian Gong", "Zhiyong Zheng", "Zhe Hu", "Kun Tian", "Yi Zhang", "Zhedanov Oleksiy", "Fengxia Liu"], "title": "Authenticated Private Set Intersection: A Merkle Tree-Based Approach for Enhancing Data Integrity", "categories": ["cs.CR"], "comment": null, "summary": "Private Set Intersection (PSI) enables secure computation of set\nintersections while preserving participant privacy, standard PSI existing\nprotocols remain vulnerable to data integrity attacks allowing malicious\nparticipants to extract additional intersection information or mislead other\nparties. In this paper, we propose the definition of data integrity in PSI and\nconstruct two authenticated PSI schemes by integrating Merkle Trees with\nstate-of-the-art two-party volePSI and multi-party mPSI protocols. The\nresulting two-party authenticated PSI achieves communication complexity\n$\\mathcal{O}(n \\lambda+n \\log n)$, aligning with the best-known unauthenticated\nPSI schemes, while the multi-party construction is $\\mathcal{O}(n \\kappa+n \\log\nn)$ which introduces additional overhead due to Merkle tree inclusion proofs.\nDue to the incorporation of integrity verification, our authenticated schemes\nincur higher costs compared to state-of-the-art unauthenticated schemes. We\nalso provide efficient implementations of our protocols and discuss potential\nimprovements, including alternative authentication blocks.", "AI": {"tldr": "我们提出了两种基于Merkle树的认证PSI方案，以增强数据完整性。", "motivation": "现有的PSI协议存在数据完整性攻击的漏洞，允许恶意参与者提取额外的交集信息或误导其他方。", "method": "通过将Merkle树与两方volePSI和多方mPSI协议相结合，提出了数据完整性的定义，并构建了两种认证PSI方案。", "result": "提出的数据完整性定义和两种认证PSI方案在通信复杂度方面与最知名的无认证PSI方案相当，但认证方案的成本较高。", "conclusion": "我们提出了PSI中数据完整性的定义，并构建了两种认证PSI方案，同时将Merkle树与最新的两方volePSI和多方mPSI协议相结合。这两种认证PSI方案在通信复杂度方面与最知名的无认证PSI方案相当。"}}
{"id": "2506.04800", "pdf": "https://arxiv.org/pdf/2506.04800", "abs": "https://arxiv.org/abs/2506.04800", "authors": ["Thomas Prévost", "Olivier Alibart", "Marc Kaplan", "Anne Marin"], "title": "MULTISS: un protocole de stockage confidentiel {à} long terme sur plusieurs r{é}seaux QKD", "categories": ["cs.CR"], "comment": "in French language", "summary": "This paper presents MULTISS, a new protocol for long-term storage distributed\nacross multiple Quantum Key Distribution (QKD) networks. This protocol is an\nextension of LINCOS, a secure storage protocol that uses Shamir secret sharing\nfor secret storage on a single QKD network. Our protocol uses hierarchical\nsecret sharing to distribute a secret across multiple QKD networks while\nensuring perfect security. Our protocol further allows for sharing updates\nwithout having to reconstruct the entire secret. We also prove that MULTISS is\nstrictly more secure than LINCOS, which remains vulnerable when its QKD network\nis compromised.", "AI": {"tldr": "MULTISS是一种在多个QKD网络上进行长期存储的新协议，比LINCOS更安全。", "motivation": "解决长期存储在单个QKD网络上的安全问题，并提高安全性。", "method": "使用分层密钥共享在多个QKD网络上分配秘密，并允许在不重建整个秘密的情况下共享更新。", "result": "MULTISS协议比LINCOS更安全，能够确保在多个QKD网络上的长期存储安全。", "conclusion": "MULTISS是一种新的协议，用于在多个量子密钥分发（QKD）网络上进行长期存储，比LINCOS更安全。"}}
{"id": "2506.04838", "pdf": "https://arxiv.org/pdf/2506.04838", "abs": "https://arxiv.org/abs/2506.04838", "authors": ["Pablo Fernández Saura", "K. R. Jayaram", "Vatche Isahagian", "Jorge Bernal Bernabé", "Antonio Skarmeta"], "title": "On Automating Security Policies with Contemporary LLMs", "categories": ["cs.CR", "cs.AI"], "comment": "Short Paper. Accepted To Appear in IEEE SSE 2025 (part of SERVICES\n  2025)", "summary": "The complexity of modern computing environments and the growing\nsophistication of cyber threats necessitate a more robust, adaptive, and\nautomated approach to security enforcement. In this paper, we present a\nframework leveraging large language models (LLMs) for automating attack\nmitigation policy compliance through an innovative combination of in-context\nlearning and retrieval-augmented generation (RAG). We begin by describing how\nour system collects and manages both tool and API specifications, storing them\nin a vector database to enable efficient retrieval of relevant information. We\nthen detail the architectural pipeline that first decomposes high-level\nmitigation policies into discrete tasks and subsequently translates each task\ninto a set of actionable API calls. Our empirical evaluation, conducted using\npublicly available CTI policies in STIXv2 format and Windows API documentation,\ndemonstrates significant improvements in precision, recall, and F1-score when\nemploying RAG compared to a non-RAG baseline.", "AI": {"tldr": "本文提出了一种利用大型语言模型（LLMs）的框架，通过结合上下文学习和检索增强生成（RAG）自动化攻击缓解策略的合规性，实验结果表明该方法在性能上有显著提升。", "motivation": "现代计算环境的复杂性和网络威胁的日益复杂化，需要更强大、自适应和自动化的安全执行方法。", "method": "本文提出了一种结合上下文学习和检索增强生成（RAG）的自动化攻击缓解策略合规性框架。首先，系统收集和管理工具和API规范，存储在向量数据库中以实现相关信息的高效检索。然后，将高级缓解策略分解成离散任务，并将每个任务转换为一系列可操作的API调用。", "result": "实验结果表明，采用RAG在精确度、召回率和F1分数方面均有显著提高。", "conclusion": "本文提出了一种利用大型语言模型（LLMs）的框架，通过结合上下文学习和检索增强生成（RAG）自动化攻击缓解策略的合规性。实验结果表明，与无RAG的基线相比，采用RAG在精确度、召回率和F1分数方面均有显著提高。"}}
{"id": "2506.04853", "pdf": "https://arxiv.org/pdf/2506.04853", "abs": "https://arxiv.org/abs/2506.04853", "authors": ["Andrea Rizzini", "Marco Esposito", "Francesco Bruschi", "Donatella Sciuto"], "title": "A Private Smart Wallet with Probabilistic Compliance", "categories": ["cs.CR", "cs.CE"], "comment": null, "summary": "We propose a privacy-preserving smart wallet with a novel invitation-based\nprivate onboarding mechanism. The solution integrates two levels of compliance\nin concert with an authority party: a proof of innocence mechanism and an\nancestral commitment tracking system using bloom filters for probabilistic UTXO\nchain states. Performance analysis demonstrates practical efficiency: private\ntransfers with compliance checks complete within seconds on a consumer-grade\nlaptop, and overall with proof generation remaining low. On-chain costs stay\nminimal, ensuring affordability for all operations on Base layer 2 network. The\nwallet facilitates private contact list management through encrypted data blobs\nwhile maintaining transaction unlinkability. Our evaluation validates the\napproach's viability for privacy-preserving, compliance-aware digital payments\nwith minimized computational and financial overhead.", "AI": {"tldr": "提出了一种隐私保护智能钱包，结合了合规性机制，以实现高效、低成本的数字支付。", "motivation": "为了解决数字支付中的隐私保护和合规性问题。", "method": "提出了一种基于邀请的隐私保护智能钱包，结合了两个级别的合规性：一个无辜证明机制和一个使用bloom filters进行概率UTXO链状态追踪的祖先承诺跟踪系统。", "result": "性能分析表明，在消费级笔记本电脑上，私有转账和合规性检查可以在几秒钟内完成，整体上，证明生成保持较低水平。链上成本保持最低，确保了Base层2网络上所有操作的可行性。", "conclusion": "验证了该方法的可行性，以实现具有最小计算和财务开销的隐私保护、符合规定的数字支付。"}}
{"id": "2506.04962", "pdf": "https://arxiv.org/pdf/2506.04962", "abs": "https://arxiv.org/abs/2506.04962", "authors": ["Deniz Simsek", "Aryaz Eghbali", "Michael Pradel"], "title": "PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm Packages", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Security vulnerabilities in software packages are a significant concern for\ndevelopers and users alike. Patching these vulnerabilities in a timely manner\nis crucial to restoring the integrity and security of software systems.\nHowever, previous work has shown that vulnerability reports often lack\nproof-of-concept (PoC) exploits, which are essential for fixing the\nvulnerability, testing patches, and avoiding regressions. Creating a PoC\nexploit is challenging because vulnerability reports are informal and often\nincomplete, and because it requires a detailed understanding of how inputs\npassed to potentially vulnerable APIs may reach security-relevant sinks. In\nthis paper, we present PoCGen, a novel approach to autonomously generate and\nvalidate PoC exploits for vulnerabilities in npm packages. This is the first\nfully autonomous approach to use large language models (LLMs) in tandem with\nstatic and dynamic analysis techniques for PoC exploit generation. PoCGen\nleverages an LLM for understanding vulnerability reports, for generating\ncandidate PoC exploits, and for validating and refining them. Our approach\nsuccessfully generates exploits for 77% of the vulnerabilities in the\nSecBench.js dataset and 39% in a new, more challenging dataset of 794 recent\nvulnerabilities. This success rate significantly outperforms a recent baseline\n(by 45 absolute percentage points), while imposing an average cost of $0.02 per\ngenerated exploit.", "AI": {"tldr": "PoCGen是一种使用大型语言模型自动生成和验证PoC利用代码的方法，成功率高，成本低。", "motivation": "漏洞报告通常缺乏PoC利用代码，而创建PoC利用代码是修复漏洞、测试补丁和避免回归的必要步骤。", "method": "PoCGen利用大型语言模型（LLMs）和静态和动态分析技术来生成和验证PoC利用代码。", "result": "PoCGen在SecBench.js数据集中成功生成77%的漏洞利用代码，在794个新漏洞数据集中成功生成39%的利用代码。", "conclusion": "PoCGen成功地生成了SecBench.js数据集中77%的漏洞的利用代码，在794个新漏洞数据集中生成了39%的利用代码，其成功率显著优于最近的一个基线（高出45个百分点），同时每个生成的利用代码的平均成本为0.02美元。"}}
{"id": "2506.04963", "pdf": "https://arxiv.org/pdf/2506.04963", "abs": "https://arxiv.org/abs/2506.04963", "authors": ["Anton Firc", "Jan Klusáček", "Kamil Malinka"], "title": "Hiding in Plain Sight: Query Obfuscation via Random Multilingual Searches", "categories": ["cs.CR", "94A60, 68P27", "H.3.3; H.3.5; K.4.1"], "comment": "Accepted to TrustBus workshop of ARES 2025", "summary": "Modern search engines extensively personalize results by building detailed\nuser profiles based on query history and behaviour. While personalization can\nenhance relevance, it introduces privacy risks and can lead to filter bubbles.\nThis paper proposes and evaluates a lightweight, client-side query obfuscation\nstrategy using randomly generated multilingual search queries to disrupt user\nprofiling. Through controlled experiments on the Seznam.cz search engine, we\nassess the impact of interleaving real queries with obfuscating noise in\nvarious language configurations and ratios. Our findings show that while\ndisplayed search results remain largely stable, the search engine's identified\nuser interests shift significantly under obfuscation. We further demonstrate\nthat such random queries can prevent accurate profiling and overwrite\nestablished user profiles. This study provides practical evidence for query\nobfuscation as a viable privacy-preserving mechanism and introduces a tool that\nenables users to autonomously protect their search behaviour without modifying\nexisting infrastructure.", "AI": {"tldr": "提出了一种使用随机查询来保护隐私的搜索策略。", "motivation": "现代搜索引擎通过构建基于查询历史和行为详细用户档案来大量个性化结果，这引入了隐私风险和过滤气泡。", "method": "提出并评估了一种使用随机生成的多语言搜索查询的轻量级客户端查询混淆策略，以破坏用户档案。", "result": "研究表明，在混淆的情况下，搜索引擎识别的用户兴趣发生了显著变化，并且随机查询可以防止准确的用户档案和覆盖现有的用户档案。", "conclusion": "研究提供了查询混淆作为可行隐私保护机制的实践证据，并介绍了一个允许用户自主保护其搜索行为的工具。"}}
{"id": "2506.04978", "pdf": "https://arxiv.org/pdf/2506.04978", "abs": "https://arxiv.org/abs/2506.04978", "authors": ["Gabriele Digregorio", "Elisabetta Cainazzo", "Stefano Longari", "Michele Carminati", "Stefano Zanero"], "title": "Evaluating the Impact of Privacy-Preserving Federated Learning on CAN Intrusion Detection", "categories": ["cs.CR"], "comment": null, "summary": "The challenges derived from the data-intensive nature of machine learning in\nconjunction with technologies that enable novel paradigms such as V2X and the\npotential offered by 5G communication, allow and justify the deployment of\nFederated Learning (FL) solutions in the vehicular intrusion detection domain.\nIn this paper, we investigate the effects of integrating FL strategies into the\nmachine learning-based intrusion detection process for on-board vehicular\nnetworks. Accordingly, we propose a FL implementation of a state-of-the-art\nIntrusion Detection System (IDS) for Controller Area Network (CAN), based on\nLSTM autoencoders. We thoroughly evaluate its detection efficiency and\ncommunication overhead, comparing it to a centralized version of the same\nalgorithm, thereby presenting it as a feasible solution.", "AI": {"tldr": "提出了一种基于FL的车载入侵检测系统，并证明了其可行性。", "motivation": "为了解决机器学习数据密集型特性、V2X技术和5G通信带来的挑战，以及车载入侵检测领域的潜在需求。", "method": "研究将FL策略集成到基于机器学习的入侵检测过程中，并提出了一个基于LSTM自动编码器的FL实现IDS。", "result": "提出了基于LSTM自动编码器的FL实现IDS，并评估了其检测效率和通信开销。", "conclusion": "提出了一种基于LSTM自动编码器的FL实现，用于车载网络入侵检测，并证明其检测效率和通信开销与集中式版本相当，因此是一种可行的解决方案。"}}
{"id": "2506.05001", "pdf": "https://arxiv.org/pdf/2506.05001", "abs": "https://arxiv.org/abs/2506.05001", "authors": ["Limin Wang", "Lei Bu", "Muzimiao Zhang", "Shihong Cang", "Kai Ye"], "title": "Attack Effect Model based Malicious Behavior Detection", "categories": ["cs.CR"], "comment": null, "summary": "Traditional security detection methods face three key challenges: inadequate\ndata collection that misses critical security events, resource-intensive\nmonitoring systems, and poor detection algorithms with high false positive\nrates. We present FEAD (Focus-Enhanced Attack Detection), a framework that\naddresses these issues through three innovations: (1) an attack model-driven\napproach that extracts security-critical monitoring items from online attack\nreports for comprehensive coverage; (2) efficient task decomposition that\noptimally distributes monitoring across existing collectors to minimize\noverhead; and (3) locality-aware anomaly analysis that leverages the clustering\nbehavior of malicious activities in provenance graphs to improve detection\naccuracy. Evaluations demonstrate FEAD achieves 8.23% higher F1-score than\nexisting solutions with only 5.4% overhead, confirming that focus-based designs\nsignificantly enhance detection performance.", "AI": {"tldr": "FEAD是一种基于关注点的攻击检测框架，通过创新方法显著提高了检测性能。", "motivation": "传统安全检测方法存在数据收集不足、资源密集型监控系统和检测算法误报率高等问题。", "method": "FEAD采用攻击模型驱动的方法、高效的任务分解和局部感知异常分析来提高检测性能。", "result": "FEAD实现了比现有解决方案8.23%更高的F1分数，同时只增加了5.4%的开销。", "conclusion": "FEAD通过三个创新解决了传统安全检测方法的挑战，提高了检测性能。"}}
{"id": "2506.05074", "pdf": "https://arxiv.org/pdf/2506.05074", "abs": "https://arxiv.org/abs/2506.05074", "authors": ["Robert J. Joyce", "Gideon Miller", "Phil Roth", "Richard Zak", "Elliott Zaresky-Williams", "Hyrum Anderson", "Edward Raff", "James Holt"], "title": "EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "A lack of accessible data has historically restricted malware analysis\nresearch, and practitioners have relied heavily on datasets provided by\nindustry sources to advance. Existing public datasets are limited by narrow\nscope - most include files targeting a single platform, have labels supporting\njust one type of malware classification task, and make no effort to capture the\nevasive files that make malware detection difficult in practice. We present\nEMBER2024, a new dataset that enables holistic evaluation of malware\nclassifiers. Created in collaboration with the authors of EMBER2017 and\nEMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,\nand labels for more than 3.2 million files from six file formats. Our dataset\nsupports the training and evaluation of machine learning models on seven\nmalware classification tasks, including malware detection, malware family\nclassification, and malware behavior identification. EMBER2024 is the first to\ninclude a collection of malicious files that initially went undetected by a set\nof antivirus products, creating a \"challenge\" set to assess classifier\nperformance against evasive malware. This work also introduces EMBER feature\nversion 3, with added support for several new feature types. We are releasing\nthe EMBER2024 dataset to promote reproducibility and empower researchers in the\npursuit of new malware research topics.", "AI": {"tldr": "提出了一个新的数据集 EMBER2024，用于恶意软件分类器的全面评估，并引入了新的特征类型。", "motivation": "缺乏可访问的数据限制了恶意软件分析研究，研究者们严重依赖行业来源提供的数据集。", "method": "提出了一个新的数据集 EMBER2024，用于恶意软件分类器的全面评估。该数据集包括超过320万文件的哈希值、元数据、特征向量和标签，支持七种恶意软件分类任务的训练和评估。", "result": "EMBER2024 数据集包括未被发现的可疑文件，用于评估分类器对逃避性恶意软件的性能。同时引入了 EMBER 特征版本 3，支持更多新特征类型。", "conclusion": null}}
{"id": "2506.05126", "pdf": "https://arxiv.org/pdf/2506.05126", "abs": "https://arxiv.org/abs/2506.05126", "authors": ["Lorenzo Rossi", "Michael Aerni", "Jie Zhang", "Florian Tramèr"], "title": "Membership Inference Attacks on Sequence Models", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted to the 8th Deep Learning Security and Privacy Workshop\n  (DLSP) workshop (best paper award)", "summary": "Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models.", "AI": {"tldr": "我们的工作通过利用序列生成中的内在相关性来有效地衡量序列模型中的隐私泄露。", "motivation": "序列模型，如大型语言模型和自回归图像生成器，倾向于记住并无意中泄露敏感信息。", "method": "我们通过显式地建模序列内的相关性，将最先进的成员推理攻击适应到序列模型中。", "result": "我们的改编在引入额外计算成本的情况下，始终如一地提高了记忆审计的有效性。", "conclusion": "我们的工作为大型序列模型的可靠记忆审计提供了一个重要的里程碑。"}}
{"id": "2506.05129", "pdf": "https://arxiv.org/pdf/2506.05129", "abs": "https://arxiv.org/abs/2506.05129", "authors": ["Andrin Bertschi", "Shweta Shinde"], "title": "OpenCCA: An Open Framework to Enable Arm CCA Research", "categories": ["cs.CR"], "comment": null, "summary": "Confidential computing has gained traction across major architectures with\nIntel TDX, AMD SEV-SNP, and Arm CCA. Unlike TDX and SEV-SNP, a key challenge in\nresearching Arm CCA is the absence of hardware support, forcing researchers to\ndevelop ad-hoc performance prototypes on non-CCA Arm boards. This approach\nleads to duplicated efforts, inconsistent performance comparisons, and high\nbarriers to entry. To address this, we present OpenCCA, an open research\nplatform that enables the execution of CCA-bound code on commodity Armv8.2\nhardware. By systematically adapting the software stack -- including\nbootloader, firmware, hypervisor, and kernel -- OpenCCA emulates CCA operations\nfor performance evaluation while preserving functional correctness. We\ndemonstrate its effectiveness with typical life-cycle measurements and\ncase-studies inspired by prior CCA-based papers on a easily available Armv8.2\nRockchip board that costs $250.", "AI": {"tldr": "提出OpenCCA，一个开放研究平台，允许在商品Armv8.2硬件上执行CCABound代码。", "motivation": "解决在研究Arm CCA时缺乏硬件支持的问题，导致重复工作、不一致的性能比较和高门槛。", "method": "通过系统性地调整软件堆栈（包括引导加载程序、固件、虚拟机和内核）来模拟CCA操作，同时保持功能正确性。", "result": "使用典型的生命周期测量和案例研究，在易于获得的Armv8.2 Rockchip板上展示了其有效性，该板成本为250美元。", "conclusion": "提出OpenCCA，一个开放研究平台，使CCABound代码能够在商品Armv8.2硬件上执行。"}}
{"id": "2506.05242", "pdf": "https://arxiv.org/pdf/2506.05242", "abs": "https://arxiv.org/abs/2506.05242", "authors": ["Zhiqiang Wang", "Haohua Du", "Junyang Wang", "Haifeng Sun", "Kaiwen Guo", "Haikuo Yu", "Chao Liu", "Xiang-Yang Li"], "title": "SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid Neuron Encryption", "categories": ["cs.CR"], "comment": null, "summary": "Large language models (LLMs) with diverse capabilities are increasingly being\ndeployed in local environments, presenting significant security and\ncontrollability challenges. These locally deployed LLMs operate outside the\ndirect control of developers, rendering them more susceptible to abuse.\nExisting mitigation techniques mainly designed for cloud-based LLM services are\nfrequently circumvented or ineffective in deployer-controlled environments. We\npropose SECNEURON, the first framework that seamlessly embeds classic access\ncontrol within the intrinsic capabilities of LLMs, achieving reliable,\ncost-effective, flexible, and certified abuse control for local deployed LLMs.\nSECNEURON employs neuron-level encryption and selective decryption to\ndynamically control the task-specific capabilities of LLMs, limiting\nunauthorized task abuse without compromising others. We first design a\ntask-specific neuron extraction mechanism to decouple logically related neurons\nand construct a layered policy tree for handling coupled neurons. We then\nintroduce a flexible and efficient hybrid encryption framework for millions of\nneurons in LLMs. Finally, we developed a distribution-based decrypted neuron\ndetection mechanism on ciphertext to ensure the effectiveness of partially\ndecrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and\nCollusion Resistance Security under the Task Controllability Principle.\nExperiments on various task settings show that SECNEURON limits unauthorized\ntask accuracy to below 25% while keeping authorized accuracy loss with 2%.\nUsing an unauthorized Code task example, the accuracy of abuse-related\nmalicious code generation was reduced from 59% to 15%. SECNEURON also mitigates\nunauthorized data leakage, reducing PII extraction rates to below 5% and\nmembership inference to random guesses.", "AI": {"tldr": "SECNEURON是一种用于本地部署的LLMs的滥用控制框架。", "motivation": "本地部署的LLMs存在安全和可控性挑战，现有的缓解技术往往在部署者控制的环境中无效。", "method": "SECNEURON采用神经元级加密和选择性解密来动态控制LLMs的任务特定能力，限制未经授权的任务滥用，而不损害其他任务。", "result": "SECNEURON将未经授权的任务准确性限制在25%以下，同时将授权准确性损失保持在2%。", "conclusion": "SECNEURON是一个框架，它将经典访问控制嵌入到LLMs的内在能力中，为本地部署的LLMs提供可靠的、成本效益的、灵活的和可验证的滥用控制。"}}
{"id": "2506.05290", "pdf": "https://arxiv.org/pdf/2506.05290", "abs": "https://arxiv.org/abs/2506.05290", "authors": ["Pierre Tholoniat", "Alison Caulfield", "Giorgio Cavicchioli", "Mark Chen", "Nikos Goutzoulias", "Benjamin Case", "Asaf Cidon", "Roxana Geambasu", "Mathias Lécuyer", "Martin Thomson"], "title": "Big Bird: Privacy Budget Management for W3C's Privacy-Preserving Attribution API", "categories": ["cs.CR"], "comment": null, "summary": "Privacy-preserving advertising APIs like Privacy-Preserving Attribution (PPA)\nare designed to enhance web privacy while enabling effective ad measurement.\nPPA offers an alternative to cross-site tracking with encrypted reports\ngoverned by differential privacy (DP), but current designs lack a principled\napproach to privacy budget management, creating uncertainty around critical\ndesign decisions. We present Big Bird, a privacy budget manager for PPA that\nclarifies per-site budget semantics and introduces a global budgeting system\ngrounded in resource isolation principles. Big Bird enforces utility-preserving\nlimits via quota budgets and improves global budget utilization through a novel\nbatched scheduling algorithm. Together, these mechanisms establish a robust\nfoundation for enforcing privacy protections in adversarial environments. We\nimplement Big Bird in Firefox and evaluate it on real-world ad data,\ndemonstrating its resilience and effectiveness.", "AI": {"tldr": "Big Bird是一个用于PPA的隐私预算管理器，旨在提高预算利用率和隐私保护。", "motivation": "当前PPA设计缺乏隐私预算管理的原则方法，导致设计决策的不确定性。", "method": "Big Bird通过配额预算和批量调度算法管理隐私预算。", "result": "Big Bird在Firefox中实现，并在实际广告数据上进行了评估，证明了其弹性和有效性。", "conclusion": "Big Bird是一个用于PPA的隐私预算管理器，它通过配额预算和批量调度算法提高了全球预算的利用率，并建立了一个强大的隐私保护基础。"}}
{"id": "2506.05346", "pdf": "https://arxiv.org/pdf/2506.05346", "abs": "https://arxiv.org/abs/2506.05346", "authors": ["Lei Hsiung", "Tianyu Pang", "Yung-Chen Tang", "Linyue Song", "Tsung-Yi Ho", "Pin-Yu Chen", "Yaoqing Yang"], "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "Project Page: https://hsiung.cc/llm-similarity-risk/", "summary": "Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers.", "AI": {"tldr": "研究表明，上游数据集设计对构建安全防护措施至关重要，并有助于降低破解攻击的风险。", "motivation": "现有缓解策略主要关注在安全防护措施被破坏后被动应对破解事件，或是在微调过程中去除有害梯度，或在微调过程中持续强化安全对齐。然而，这些策略往往忽略了原始安全对齐数据集的作用。", "method": "通过分析上游对齐数据集与下游微调任务之间的表示相似度，研究了安全防护措施的退化。", "result": "研究发现，上游对齐数据集与下游微调任务之间的表示相似度越高，安全防护措施越弱，模型越容易受到破解攻击。反之，这两种数据集之间的相似度越低，模型越稳健，有害度评分可降低高达10.33%。", "conclusion": "研究指出，上游对齐数据集与下游微调任务之间的表示相似度越高，安全防护措施越弱，模型越容易受到破解攻击。反之，这两种数据集之间的相似度越低，模型越稳健，有害度评分可降低高达10.33%。这些发现强调了上游数据集设计在构建坚固的安全防护措施中的重要性，并有助于降低对破解攻击的现实风险。"}}
{"id": "2506.03614", "pdf": "https://arxiv.org/pdf/2506.03614", "abs": "https://arxiv.org/abs/2506.03614", "authors": ["Zhanhui Zhou", "Lingjie Chen", "Chao Yang", "Chaochao Lu"], "title": "VLMs Can Aggregate Scattered Training Patches", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as $\\textit{visual\nstitching}$ -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each $(\\texttt{image}, \\texttt{ID})$ pair into $\\{(\\texttt{patch},\n\\texttt{ID})\\}$ pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.", "AI": {"tldr": "该论文研究了视觉语言模型（VLM）中的视觉拼接能力，以及如何通过数据中毒攻击来规避VLM的安全性。", "motivation": "为了减轻视觉语言模型中的风险，研究人员提出通过移除训练数据中的危险样本来提高模型的安全性。然而，当有害图像被分割成小块时，这种数据审查很容易被绕过。", "method": "通过在三个数据集上展示视觉拼接能力，并模拟数据中毒场景，研究人员证明了有害内容可以通过视觉拼接来规避审查，从而对VLM的安全性构成严重威胁。", "result": "该论文发现，视觉语言模型可以通过视觉拼接能力将散布在多个训练样本中的视觉信息整合起来，并可能生成有害的响应。", "conclusion": "该研究强调了视觉语言模型中视觉拼接能力的潜在风险，并提出了对抗性数据中毒攻击的解决方案。"}}
{"id": "2506.04462", "pdf": "https://arxiv.org/pdf/2506.04462", "abs": "https://arxiv.org/abs/2506.04462", "authors": ["Apurv Verma", "NhatHai Phan", "Shubhendu Trivedi"], "title": "Watermarking Degrades Alignment in Language Models: Analysis and Mitigation", "categories": ["cs.CL", "cs.CR", "cs.LG", "I.2.7"], "comment": "Published at the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025. OpenReview: https://openreview.net/forum?id=SIBkIV48gF", "summary": "Watermarking techniques for large language models (LLMs) can significantly\nimpact output quality, yet their effects on truthfulness, safety, and\nhelpfulness remain critically underexamined. This paper presents a systematic\nanalysis of how two popular watermarking approaches-Gumbel and KGW-affect these\ncore alignment properties across four aligned LLMs. Our experiments reveal two\ndistinct degradation patterns: guard attenuation, where enhanced helpfulness\nundermines model safety, and guard amplification, where excessive caution\nreduces model helpfulness. These patterns emerge from watermark-induced shifts\nin token distribution, surfacing the fundamental tension that exists between\nalignment objectives.\n  To mitigate these degradations, we propose Alignment Resampling (AR), an\ninference-time sampling method that uses an external reward model to restore\nalignment. We establish a theoretical lower bound on the improvement in\nexpected reward score as the sample size is increased and empirically\ndemonstrate that sampling just 2-4 watermarked generations effectively recovers\nor surpasses baseline (unwatermarked) alignment scores. To overcome the limited\nresponse diversity of standard Gumbel watermarking, our modified implementation\nsacrifices strict distortion-freeness while maintaining robust detectability,\nensuring compatibility with AR. Experimental results confirm that AR\nsuccessfully recovers baseline alignment in both watermarking approaches, while\nmaintaining strong watermark detectability. This work reveals the critical\nbalance between watermark strength and model alignment, providing a simple\ninference-time solution to responsibly deploy watermarked LLMs in practice.", "AI": {"tldr": "这项研究分析了水印技术对大型语言模型的影响，并提出了一种新的对齐重采样方法来恢复对齐。", "motivation": "水印技术对大型语言模型（LLMs）的输出质量有重大影响，但其对真实性、安全性和有用性的影响仍被严重忽视。", "method": "我们提出了对齐重采样（AR），这是一种推理时间采样方法，使用外部奖励模型来恢复对齐。", "result": "我们的实验揭示了两种不同的退化模式：防护衰减和防护放大。我们证明了AR在两种水印方法中成功恢复了基线对齐，同时保持了强大的水印可检测性。", "conclusion": "这项工作揭示了水印强度与模型对齐之间的关键平衡，为在实际中负责任地部署水印LLMs提供了一个简单的推理时间解决方案。"}}
{"id": "2506.04681", "pdf": "https://arxiv.org/pdf/2506.04681", "abs": "https://arxiv.org/abs/2506.04681", "authors": ["Daogao Liu", "Edith Cohen", "Badih Ghazi", "Peter Kairouz", "Pritish Kamath", "Alexander Knop", "Ravi Kumar", "Pasin Manurangsi", "Adam Sealfon", "Da Yu", "Chiyuan Zhang"], "title": "Urania: Differentially Private Insights into AI Use", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "We introduce $Urania$, a novel framework for generating insights about LLM\nchatbot interactions with rigorous differential privacy (DP) guarantees. The\nframework employs a private clustering mechanism and innovative keyword\nextraction methods, including frequency-based, TF-IDF-based, and LLM-guided\napproaches. By leveraging DP tools such as clustering, partition selection, and\nhistogram-based summarization, $Urania$ provides end-to-end privacy protection.\nOur evaluation assesses lexical and semantic content preservation, pair\nsimilarity, and LLM-based metrics, benchmarking against a non-private\nClio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple\nempirical privacy evaluation that demonstrates the enhanced robustness of our\nDP pipeline. The results show the framework's ability to extract meaningful\nconversational insights while maintaining stringent user privacy, effectively\nbalancing data utility with privacy preservation.", "AI": {"tldr": "提出了一种名为Urania的新框架，用于在严格差分隐私（DP）保证下生成LLM聊天机器人交互的见解。", "motivation": "为了解决LLM聊天机器人交互中的隐私问题。", "method": "使用私有聚类机制和创新的关键词提取方法，包括基于频率、TF-IDF和LLM引导的方法。", "result": "Urania能够提取有意义的对话见解，同时保持严格的用户隐私。", "conclusion": "Urania在提供严格隐私保护的同时，有效地平衡了数据效用与隐私保护。"}}
{"id": "2506.04909", "pdf": "https://arxiv.org/pdf/2506.04909", "abs": "https://arxiv.org/abs/2506.04909", "authors": ["Kai Wang", "Yihao Zhang", "Meng Sun"], "title": "When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "The honesty of large language models (LLMs) is a critical alignment\nchallenge, especially as advanced systems with chain-of-thought (CoT) reasoning\nmay strategically deceive humans. Unlike traditional honesty issues on LLMs,\nwhich could be possibly explained as some kind of hallucination, those models'\nexplicit thought paths enable us to study strategic deception--goal-driven,\nintentional misinformation where reasoning contradicts outputs. Using\nrepresentation engineering, we systematically induce, detect, and control such\ndeception in CoT-enabled LLMs, extracting \"deception vectors\" via Linear\nArtificial Tomography (LAT) for 89% detection accuracy. Through activation\nsteering, we achieve a 40% success rate in eliciting context-appropriate\ndeception without explicit prompts, unveiling the specific honesty-related\nissue of reasoning models and providing tools for trustworthy AI alignment.", "AI": {"tldr": "通过表示工程和激活引导，实现了在具有思维链（CoT）推理的LLMs中对欺骗的检测和控制。", "motivation": "LLMs的诚信是关键对齐挑战，特别是随着具有思维链（CoT）推理的先进系统可能会策略性地欺骗人类。", "method": "使用表示工程，通过线性人工断层扫描（LAT）提取“欺骗向量”，并使用激活引导来实现欺骗。", "result": "实现了89%的欺骗检测精度，并成功引发了40%的适当欺骗。", "conclusion": "通过表示工程，系统性地在具有思维链（CoT）推理的LLMs中诱导、检测和控制欺骗，通过线性人工断层扫描（LAT）提取“欺骗向量”，达到89%的检测精度。通过激活引导，在无需明确提示的情况下，实现40%的成功率，引发适当欺骗，揭示推理模型的特定诚信问题，为可信AI对齐提供工具。"}}
{"id": "2506.05022", "pdf": "https://arxiv.org/pdf/2506.05022", "abs": "https://arxiv.org/abs/2506.05022", "authors": ["Yixuan Cao", "Yuhong Feng", "Huafeng Li", "Chongyi Huang", "Fangcao Jian", "Haoran Li", "Xu Wang"], "title": "Tech-ASan: Two-stage check for Address Sanitizer", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Address Sanitizer (ASan) is a sharp weapon for detecting memory safety\nviolations, including temporal and spatial errors hidden in C/C++ programs\nduring execution. However, ASan incurs significant runtime overhead, which\nlimits its efficiency in testing large software. The overhead mainly comes from\nsanitizer checks due to the frequent and expensive shadow memory access. Over\nthe past decade, many methods have been developed to speed up ASan by\neliminating and accelerating sanitizer checks, however, they either fail to\nadequately eliminate redundant checks or compromise detection capabilities. To\naddress this issue, this paper presents Tech-ASan, a two-stage check based\ntechnique to accelerate ASan with safety assurance. First, we propose a novel\ntwo-stage check algorithm for ASan, which leverages magic value comparison to\nreduce most of the costly shadow memory accesses. Second, we design an\nefficient optimizer to eliminate redundant checks, which integrates a novel\nalgorithm for removing checks in loops. Third, we implement Tech-ASan as a\nmemory safety tool based on the LLVM compiler infrastructure. Our evaluation\nusing the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the\nstate-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan\nand ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative\ncases than ASan and ASan-- when testing on the Juliet Test Suite under the same\nredzone setting.", "AI": {"tldr": "Tech-ASan是一种基于两阶段检查的ASan加速技术，在保证安全性的同时，显著降低了运行时开销和误报案例。", "motivation": "解决ASan在测试大型软件时运行时开销大，检测能力受影响的问题。", "method": "Tech-ASan采用两阶段检查算法，减少阴影内存访问，设计优化器消除冗余检查，并基于LLVM编译器基础设施实现。", "result": "Tech-ASan在SPEC CPU2006基准测试中比ASan和ASan--分别降低了33.70%和17.89%的运行时开销，检测误报案例比ASan和ASan--分别减少了56个。", "conclusion": "Tech-ASan通过两阶段检查技术加速了ASan，同时保证了安全性。与ASan和ASan--相比，Tech-ASan在SPEC CPU2006基准测试中具有更低的运行时开销和更少的误报案例。"}}
{"id": "2506.05032", "pdf": "https://arxiv.org/pdf/2506.05032", "abs": "https://arxiv.org/abs/2506.05032", "authors": ["Zeming Wei", "Yiwen Guo", "Yisen Wang"], "title": "Identifying and Understanding Cross-Class Features in Adversarial Training", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "math.OC"], "comment": "ICML 2025", "summary": "Adversarial training (AT) has been considered one of the most effective\nmethods for making deep neural networks robust against adversarial attacks,\nwhile the training mechanisms and dynamics of AT remain open research problems.\nIn this paper, we present a novel perspective on studying AT through the lens\nof class-wise feature attribution. Specifically, we identify the impact of a\nkey family of features on AT that are shared by multiple classes, which we call\ncross-class features. These features are typically useful for robust\nclassification, which we offer theoretical evidence to illustrate through a\nsynthetic data model. Through systematic studies across multiple model\narchitectures and settings, we find that during the initial stage of AT, the\nmodel tends to learn more cross-class features until the best robustness\ncheckpoint. As AT further squeezes the training robust loss and causes robust\noverfitting, the model tends to make decisions based on more class-specific\nfeatures. Based on these discoveries, we further provide a unified view of two\nexisting properties of AT, including the advantage of soft-label training and\nrobust overfitting. Overall, these insights refine the current understanding of\nAT mechanisms and provide new perspectives on studying them. Our code is\navailable at https://github.com/PKU-ML/Cross-Class-Features-AT.", "AI": {"tldr": "本文通过类间特征归因的方法，研究了对抗训练机制，发现跨类别特征在对抗训练中起到重要作用。", "motivation": "为了研究对抗训练（AT）机制，本文通过类间特征归因的角度提出了新的研究视角。", "method": "本文通过类间特征归因的方法，识别了对抗训练中多个类别共享的关键特征族，称为跨类别特征。", "result": "研究发现，在对抗训练的初始阶段，模型倾向于学习更多的跨类别特征，直到最佳鲁棒性检查点。随着对抗训练进一步压缩训练鲁棒损失并导致鲁棒过拟合，模型倾向于基于更多类别特定的特征做出决策。", "conclusion": "本文的研究对对抗训练机制的理解进行了细化，并为研究它们提供了新的视角。"}}
{"id": "2506.05101", "pdf": "https://arxiv.org/pdf/2506.05101", "abs": "https://arxiv.org/abs/2506.05101", "authors": ["Clément Pierquin", "Aurélien Bellet", "Marc Tommasi", "Matthieu Boussard"], "title": "Privacy Amplification Through Synthetic Data: Insights from Linear Regression", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": "26 pages, ICML 2025", "summary": "Synthetic data inherits the differential privacy guarantees of the model used\nto generate it. Additionally, synthetic data may benefit from privacy\namplification when the generative model is kept hidden. While empirical studies\nsuggest this phenomenon, a rigorous theoretical understanding is still lacking.\nIn this paper, we investigate this question through the well-understood\nframework of linear regression. First, we establish negative results showing\nthat if an adversary controls the seed of the generative model, a single\nsynthetic data point can leak as much information as releasing the model\nitself. Conversely, we show that when synthetic data is generated from random\ninputs, releasing a limited number of synthetic data points amplifies privacy\nbeyond the model's inherent guarantees. We believe our findings in linear\nregression can serve as a foundation for deriving more general bounds in the\nfuture.", "AI": {"tldr": "研究合成数据隐私，发现生成模型种子控制和随机输入对隐私的影响。", "motivation": "为了深入理解合成数据隐私保证和隐私放大现象。", "method": "使用线性回归框架研究合成数据隐私问题。", "result": "发现控制生成模型种子可导致信息泄露，而随机输入生成的合成数据可放大隐私。", "conclusion": "通过线性回归框架，研究合成数据隐私保证和隐私放大现象，得出负面结果和正面结果。"}}
