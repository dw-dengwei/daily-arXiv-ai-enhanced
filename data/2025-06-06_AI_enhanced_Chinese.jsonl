{"id": "2506.04307", "pdf": "https://arxiv.org/pdf/2506.04307", "abs": "https://arxiv.org/abs/2506.04307", "authors": ["Christos Karapapas", "Iakovos Pittaras", "George C. Polyzos", "Constantinos Patsakis"], "title": "Hello, won't you tell me your name?: Investigating Anonymity Abuse in IPFS", "categories": ["cs.CR"], "comment": "To appear at 13th International Workshop on Cyber Crime (IWCC), in\n  conjunction with the 19th International Conference on Availability,\n  Reliability and Security (ARES)", "summary": "The InterPlanetary File System~(IPFS) offers a decentralized approach to file\nstorage and sharing, promising resilience and efficiency while also realizing\nthe Web3 paradigm. Simultaneously, the offered anonymity raises significant\nquestions about potential misuse. In this study, we explore methods that\nmalicious actors can exploit IPFS to upload and disseminate harmful content\nwhile remaining anonymous. We evaluate the role of pinning services and public\ngateways, identifying their capabilities and limitations in maintaining content\navailability. Using scripts, we systematically test the behavior of these\nservices by uploading malicious files. Our analysis reveals that pinning\nservices and public gateways lack mechanisms to assess or restrict the\npropagation of malicious content.", "AI": {"tldr": "研究揭示了IPFS挂载服务和公共网关在限制恶意内容传播方面的不足。", "motivation": "探索恶意行为者如何利用IPFS上传和传播有害内容，同时保持匿名。", "method": "研究通过上传恶意文件，使用脚本系统地测试了这些服务的表现。", "result": "评估了挂载服务和公共网关在维护内容可用性方面的能力和局限性。", "conclusion": "分析表明，挂载服务和公共网关缺乏评估或限制恶意内容传播的机制。"}}
{"id": "2506.04383", "pdf": "https://arxiv.org/pdf/2506.04383", "abs": "https://arxiv.org/abs/2506.04383", "authors": ["Mohamed Aly Bouke"], "title": "The Hashed Fractal Key Recovery (HFKR) Problem: From Symbolic Path Inversion to Post-Quantum Cryptographic Keys", "categories": ["cs.CR"], "comment": null, "summary": "Classical cryptographic systems rely heavily on structured algebraic\nproblems, such as factorization, discrete logarithms, or lattice-based\nassumptions, which are increasingly vulnerable to quantum attacks and\nstructural cryptanalysis. In response, this work introduces the Hashed Fractal\nKey Recovery (HFKR) problem, a non-algebraic cryptographic construction\ngrounded in symbolic dynamics and chaotic perturbations. HFKR builds on the\nSymbolic Path Inversion Problem (SPIP), leveraging symbolic trajectories\ngenerated via contractive affine maps over $\\mathbb{Z}^2$, and compressing them\ninto fixed-length cryptographic keys using hash-based obfuscation. A key\ncontribution of this paper is the empirical confirmation that these symbolic\npaths exhibit fractal behavior, quantified via box counting dimension, path\ngeometry, and spatial density measures. The observed fractal dimension\nincreases with trajectory length and stabilizes near 1.06, indicating symbolic\nself-similarity and space-filling complexity, both of which reinforce the\nentropy foundation of the scheme. Experimental results across 250 perturbation\ntrials show that SHA3-512 and SHAKE256 amplify symbolic divergence effectively,\nachieving mean Hamming distances near 255, ideal bit-flip rates, and negligible\nentropy deviation. In contrast, BLAKE3 exhibits statistically uniform but\nweaker diffusion. These findings confirm that HFKR post-quantum security arises\nfrom the synergy between symbolic fractality and hash-based entropy\namplification. The resulting construction offers a lightweight, structure-free\nfoundation for secure key generation in adversarial settings without relying on\nalgebraic hardness assumptions.", "AI": {"tldr": "HFKR问题结合符号分形和哈希熵增强，为后量子安全提供了一种新的密钥生成方法。", "motivation": "应对经典密码系统对量子攻击和结构密码分析的脆弱性。", "method": "HFKR基于符号路径逆问题，通过合同仿射映射生成符号轨迹，并使用哈希混淆将其压缩为固定长度的密钥。", "result": "实验结果表明，SHA3-512和SHAKE256有效地增强了符号发散，而BLAKE3表现出统计上均匀但较弱的扩散。", "conclusion": "HFKR问题通过结合符号分形和基于哈希的熵增强，为后量子安全提供了轻量级、无结构的密钥生成基础。"}}
{"id": "2506.04390", "pdf": "https://arxiv.org/pdf/2506.04390", "abs": "https://arxiv.org/abs/2506.04390", "authors": ["Sarthak Choudhary", "Nils Palumbo", "Ashish Hooda", "Krishnamurthy Dj Dvijotham", "Somesh Jha"], "title": "Through the Stealth Lens: Rethinking Attacks and Defenses in RAG", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems are vulnerable to attacks that\ninject poisoned passages into the retrieved set, even at low corruption rates.\nWe show that existing attacks are not designed to be stealthy, allowing\nreliable detection and mitigation. We formalize stealth using a\ndistinguishability-based security game. If a few poisoned passages are designed\nto control the response, they must differentiate themselves from benign ones,\ninherently compromising stealth. This motivates the need for attackers to\nrigorously analyze intermediate signals involved in\ngeneration$\\unicode{x2014}$such as attention patterns or next-token probability\ndistributions$\\unicode{x2014}$to avoid easily detectable traces of\nmanipulation. Leveraging attention patterns, we propose a passage-level\nscore$\\unicode{x2014}$the Normalized Passage Attention\nScore$\\unicode{x2014}$used by our Attention-Variance Filter algorithm to\nidentify and filter potentially poisoned passages. This method mitigates\nexisting attacks, improving accuracy by up to $\\sim 20 \\%$ over baseline\ndefenses. To probe the limits of attention-based defenses, we craft stealthier\nadaptive attacks that obscure such traces, achieving up to $35 \\%$ attack\nsuccess rate, and highlight the challenges in improving stealth.", "AI": {"tldr": "研究提出了一种基于注意力的过滤算法，可识别并过滤被注入恶意信息的文档片段，提高了检测和防御恶意攻击的能力。", "motivation": "现有检索增强生成（RAG）系统易受到恶意攻击，攻击者可以通过注入恶意信息来控制生成结果。", "method": "提出了基于注意力模式的过滤算法，通过计算文档片段的标准化注意力分数来识别和过滤潜在的恶意文档片段。", "result": "该方法能有效地缓解现有攻击，提高准确性，相对于基线防御提高了20%的准确率。", "conclusion": "研究证明了注意力模式在检测和防御恶意攻击中的重要性，并提出了基于注意力的防御方法。"}}
{"id": "2506.04450", "pdf": "https://arxiv.org/pdf/2506.04450", "abs": "https://arxiv.org/abs/2506.04450", "authors": ["Payel Bhattacharjee", "Fengwei Tian", "Ravi Tandon", "Joseph Lo", "Heidi Hanson", "Geoffrey Rubin", "Nirav Merchant", "John Gounley"], "title": "Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "19 pages, 5 figures, 2 tables", "summary": "Purpose: This study proposes a framework for fine-tuning large language\nmodels (LLMs) with differential privacy (DP) to perform multi-abnormality\nclassification on radiology report text. By injecting calibrated noise during\nfine-tuning, the framework seeks to mitigate the privacy risks associated with\nsensitive patient data and protect against data leakage while maintaining\nclassification performance. Materials and Methods: We used 50,232 radiology\nreports from the publicly available MIMIC-CXR chest radiography and CT-RATE\ncomputed tomography datasets, collected between 2011 and 2019. Fine-tuning of\nLLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels\nfrom CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)\nin high and moderate privacy regimes (across a range of privacy budgets =\n{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1\nscore across three model architectures: BERT-medium, BERT-small, and\nALBERT-base. Statistical analyses compared model performance across different\nprivacy levels to quantify the privacy-utility trade-off. Results: We observe a\nclear privacy-utility trade-off through our experiments on 2 different datasets\nand 3 different models. Under moderate privacy guarantees the DP fine-tuned\nmodels achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on\nCT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.\nConclusion: Differentially private fine-tuning using LoRA enables effective and\nprivacy-preserving multi-abnormality classification from radiology reports,\naddressing a key challenge in fine-tuning LLMs on sensitive medical data.", "AI": {"tldr": "This study proposes a framework for fine-tuning LLMs with DP for multi-abnormality classification in radiology reports, achieving privacy-preserving classification performance.", "motivation": "The study aims to protect sensitive patient data and prevent data leakage while maintaining classification performance in radiology report text classification.", "method": "The framework injects calibrated noise during fine-tuning using Differentially Private Low-Rank Adaptation (DP-LoRA) to mitigate privacy risks and maintain classification performance.", "result": "The DP fine-tuned models achieved comparable weighted F1 scores under moderate privacy guarantees, with scores of 0.88 on MIMIC-CXR and 0.59 on CT-RATE.", "conclusion": "Differentially private fine-tuning using LoRA enables effective and privacy-preserving multi-abnormality classification from radiology reports, addressing a key challenge in fine-tuning LLMs on sensitive medical data."}}
{"id": "2506.04556", "pdf": "https://arxiv.org/pdf/2506.04556", "abs": "https://arxiv.org/abs/2506.04556", "authors": ["Xuhao Ren", "Haotian Liang", "Yajie Wang", "Chuan Zhang", "Zehui Xiong", "Liehuang Zhu"], "title": "BESA: Boosting Encoder Stealing Attack with Perturbation Recovery", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "To boost the encoder stealing attack under the perturbation-based defense\nthat hinders the attack performance, we propose a boosting encoder stealing\nattack with perturbation recovery named BESA. It aims to overcome\nperturbation-based defenses. The core of BESA consists of two modules:\nperturbation detection and perturbation recovery, which can be combined with\ncanonical encoder stealing attacks. The perturbation detection module utilizes\nthe feature vectors obtained from the target encoder to infer the defense\nmechanism employed by the service provider. Once the defense mechanism is\ndetected, the perturbation recovery module leverages the well-designed\ngenerative model to restore a clean feature vector from the perturbed one.\nThrough extensive evaluations based on various datasets, we demonstrate that\nBESA significantly enhances the surrogate encoder accuracy of existing encoder\nstealing attacks by up to 24.63\\% when facing state-of-the-art defenses and\ncombinations of multiple defenses.", "AI": {"tldr": "BESA是一种提升编码器窃取攻击准确性的攻击方法。", "motivation": "为了克服基于扰动的防御机制，提升编码器窃取攻击的性能。", "method": "BESA由扰动检测和扰动恢复两个模块组成，可结合规范编码器窃取攻击。", "result": "BESA在多种数据集上的评估显示，其相对于现有编码器窃取攻击，在面对最先进的防御和多种防御组合时，可提高编码器准确率高达24.63%。", "conclusion": "BESA通过提升编码器窃取攻击的准确性，有效地克服了基于扰动的防御机制。"}}
{"id": "2506.04634", "pdf": "https://arxiv.org/pdf/2506.04634", "abs": "https://arxiv.org/abs/2506.04634", "authors": ["Mridu Nanda", "Michael K. Reiter"], "title": "Incentivizing Collaborative Breach Detection", "categories": ["cs.CR"], "comment": null, "summary": "Decoy passwords, or \"honeywords,\" alert a site to its breach if they are ever\nentered in a login attempt on that site. However, an attacker can identify a\nuser-chosen password from among the decoys, without risk of alerting the site\nto its breach, by performing credential stuffing, i.e., entering the stolen\npasswords at another site where the same user reused her password. Prior work\nhas thus proposed that sites monitor for the entry of their honeywords at other\nsites. Unfortunately, it is not clear what incentives sites have to participate\nin this monitoring. In this paper we propose and evaluate an algorithm by which\nsites can exchange monitoring favors. Through a model-checking analysis, we\nshow that using our algorithm, a site improves its ability to detect its own\nbreach when it increases the monitoring effort it expends for other sites. We\nadditionally quantify the impacts of various parameters on detection\neffectiveness and their implications for the deployment of a system to support\na monitoring ecosystem. Finally, we evaluate our algorithm on a real dataset of\nbreached credentials and provide a performance analysis that confirms its\nscalability and practical viability.", "AI": {"tldr": "我们提出了一种通过交换监控帮助来提高网站漏洞检测能力的算法。", "motivation": "虽然以前的工作提出了网站监控其在其他网站上的honeywords的输入，但尚不清楚网站参与这种监控的动机。", "method": "我们提出了一个算法，使网站可以通过交换监控帮助来参与监控。", "result": "我们提出了一个算法，并对其进行了评估。该算法使网站能够通过交换监控帮助来参与监控，并提高了检测自身漏洞的能力。", "conclusion": "通过模型检查分析，我们证明了使用我们的算法，当网站增加为其他网站投入的监控努力时，它可以提高检测自身漏洞的能力。我们还量化了各种参数对检测有效性的影响及其对部署支持监控生态系统的系统的影响。最后，我们在一个真实的漏洞凭证数据集上评估了我们的算法，并提供了性能分析，证实了其可扩展性和实际可行性。"}}
{"id": "2506.04647", "pdf": "https://arxiv.org/pdf/2506.04647", "abs": "https://arxiv.org/abs/2506.04647", "authors": ["Zixian Gong", "Zhiyong Zheng", "Zhe Hu", "Kun Tian", "Yi Zhang", "Zhedanov Oleksiy", "Fengxia Liu"], "title": "Authenticated Private Set Intersection: A Merkle Tree-Based Approach for Enhancing Data Integrity", "categories": ["cs.CR"], "comment": null, "summary": "Private Set Intersection (PSI) enables secure computation of set\nintersections while preserving participant privacy, standard PSI existing\nprotocols remain vulnerable to data integrity attacks allowing malicious\nparticipants to extract additional intersection information or mislead other\nparties. In this paper, we propose the definition of data integrity in PSI and\nconstruct two authenticated PSI schemes by integrating Merkle Trees with\nstate-of-the-art two-party volePSI and multi-party mPSI protocols. The\nresulting two-party authenticated PSI achieves communication complexity\n$\\mathcal{O}(n \\lambda+n \\log n)$, aligning with the best-known unauthenticated\nPSI schemes, while the multi-party construction is $\\mathcal{O}(n \\kappa+n \\log\nn)$ which introduces additional overhead due to Merkle tree inclusion proofs.\nDue to the incorporation of integrity verification, our authenticated schemes\nincur higher costs compared to state-of-the-art unauthenticated schemes. We\nalso provide efficient implementations of our protocols and discuss potential\nimprovements, including alternative authentication blocks.", "AI": {"tldr": "本文提出了改进的认证PSI方案，提高了数据完整性。", "motivation": "现有的PSI协议容易受到数据完整性攻击。", "method": "通过整合Merkle树和先进的两方volePSI和多方mPSI协议。", "result": "两方认证PSI的通信复杂度为O(nλ+nlogn)，多方构造为O(nκ+nlogn)。", "conclusion": "我们提出了PSI中的数据完整性定义，并构建了两个认证PSI方案。"}}
{"id": "2506.04800", "pdf": "https://arxiv.org/pdf/2506.04800", "abs": "https://arxiv.org/abs/2506.04800", "authors": ["Thomas Prévost", "Olivier Alibart", "Marc Kaplan", "Anne Marin"], "title": "MULTISS: un protocole de stockage confidentiel {à} long terme sur plusieurs r{é}seaux QKD", "categories": ["cs.CR"], "comment": "in French language", "summary": "This paper presents MULTISS, a new protocol for long-term storage distributed\nacross multiple Quantum Key Distribution (QKD) networks. This protocol is an\nextension of LINCOS, a secure storage protocol that uses Shamir secret sharing\nfor secret storage on a single QKD network. Our protocol uses hierarchical\nsecret sharing to distribute a secret across multiple QKD networks while\nensuring perfect security. Our protocol further allows for sharing updates\nwithout having to reconstruct the entire secret. We also prove that MULTISS is\nstrictly more secure than LINCOS, which remains vulnerable when its QKD network\nis compromised.", "AI": {"tldr": "MULTISS 协议是一种更安全的分布式量子密钥分发网络存储协议。", "motivation": "提高存储在 QKD 网络上的安全性，解决 LINCOS 在网络被破坏时的弱点。", "method": "使用分层密钥共享在多个 QKD 网络上分布存储，并允许在不重建整个密钥的情况下共享更新。", "result": "MULTISS 协议在安全性和灵活性方面优于 LINCOS。", "conclusion": "MULTISS 是一个更安全的协议，它扩展了 LINCOS，允许在多个 QKD 网络上分布式存储，同时保持完美安全性。"}}
{"id": "2506.04838", "pdf": "https://arxiv.org/pdf/2506.04838", "abs": "https://arxiv.org/abs/2506.04838", "authors": ["Pablo Fernández Saura", "K. R. Jayaram", "Vatche Isahagian", "Jorge Bernal Bernabé", "Antonio Skarmeta"], "title": "On Automating Security Policies with Contemporary LLMs", "categories": ["cs.CR", "cs.AI"], "comment": "Short Paper. Accepted To Appear in IEEE SSE 2025 (part of SERVICES\n  2025)", "summary": "The complexity of modern computing environments and the growing\nsophistication of cyber threats necessitate a more robust, adaptive, and\nautomated approach to security enforcement. In this paper, we present a\nframework leveraging large language models (LLMs) for automating attack\nmitigation policy compliance through an innovative combination of in-context\nlearning and retrieval-augmented generation (RAG). We begin by describing how\nour system collects and manages both tool and API specifications, storing them\nin a vector database to enable efficient retrieval of relevant information. We\nthen detail the architectural pipeline that first decomposes high-level\nmitigation policies into discrete tasks and subsequently translates each task\ninto a set of actionable API calls. Our empirical evaluation, conducted using\npublicly available CTI policies in STIXv2 format and Windows API documentation,\ndemonstrates significant improvements in precision, recall, and F1-score when\nemploying RAG compared to a non-RAG baseline.", "AI": {"tldr": "提出利用大语言模型（LLMs）的框架，通过上下文学习和检索增强生成（RAG）自动化攻击缓解策略合规性。", "motivation": "现代计算环境和网络威胁的复杂性要求更强大、适应性和自动化的安全执行方法。", "method": "使用LLMs，结合上下文学习和RAG，自动执行攻击缓解策略。", "result": "使用RAG比非RAG基线在精确度、召回率和F1分数上有所提高。", "conclusion": null}}
{"id": "2506.04853", "pdf": "https://arxiv.org/pdf/2506.04853", "abs": "https://arxiv.org/abs/2506.04853", "authors": ["Andrea Rizzini", "Marco Esposito", "Francesco Bruschi", "Donatella Sciuto"], "title": "A Private Smart Wallet with Probabilistic Compliance", "categories": ["cs.CR", "cs.CE"], "comment": null, "summary": "We propose a privacy-preserving smart wallet with a novel invitation-based\nprivate onboarding mechanism. The solution integrates two levels of compliance\nin concert with an authority party: a proof of innocence mechanism and an\nancestral commitment tracking system using bloom filters for probabilistic UTXO\nchain states. Performance analysis demonstrates practical efficiency: private\ntransfers with compliance checks complete within seconds on a consumer-grade\nlaptop, and overall with proof generation remaining low. On-chain costs stay\nminimal, ensuring affordability for all operations on Base layer 2 network. The\nwallet facilitates private contact list management through encrypted data blobs\nwhile maintaining transaction unlinkability. Our evaluation validates the\napproach's viability for privacy-preserving, compliance-aware digital payments\nwith minimized computational and financial overhead.", "AI": {"tldr": "提出了一种隐私保护智能钱包，以实现隐私保护和合规性数字支付。", "motivation": "为了解决数字支付中的隐私保护和合规性问题。", "method": "该解决方案整合了两个级别的合规性，包括无辜证明机制和利用布隆过滤器进行概率性UTXO链状态跟踪的祖先承诺跟踪系统。", "result": "性能分析显示，合规性检查的私有转账可以在几秒钟内完成，并且整体上生成证明的成本保持较低。", "conclusion": "该研究提出了一种隐私保护智能钱包，并通过性能分析验证了其在隐私保护和合规性数字支付中的可行性。"}}
{"id": "2506.04962", "pdf": "https://arxiv.org/pdf/2506.04962", "abs": "https://arxiv.org/abs/2506.04962", "authors": ["Deniz Simsek", "Aryaz Eghbali", "Michael Pradel"], "title": "PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm Packages", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Security vulnerabilities in software packages are a significant concern for\ndevelopers and users alike. Patching these vulnerabilities in a timely manner\nis crucial to restoring the integrity and security of software systems.\nHowever, previous work has shown that vulnerability reports often lack\nproof-of-concept (PoC) exploits, which are essential for fixing the\nvulnerability, testing patches, and avoiding regressions. Creating a PoC\nexploit is challenging because vulnerability reports are informal and often\nincomplete, and because it requires a detailed understanding of how inputs\npassed to potentially vulnerable APIs may reach security-relevant sinks. In\nthis paper, we present PoCGen, a novel approach to autonomously generate and\nvalidate PoC exploits for vulnerabilities in npm packages. This is the first\nfully autonomous approach to use large language models (LLMs) in tandem with\nstatic and dynamic analysis techniques for PoC exploit generation. PoCGen\nleverages an LLM for understanding vulnerability reports, for generating\ncandidate PoC exploits, and for validating and refining them. Our approach\nsuccessfully generates exploits for 77% of the vulnerabilities in the\nSecBench.js dataset and 39% in a new, more challenging dataset of 794 recent\nvulnerabilities. This success rate significantly outperforms a recent baseline\n(by 45 absolute percentage points), while imposing an average cost of $0.02 per\ngenerated exploit.", "AI": {"tldr": "本文提出了一种名为PoCGen的新方法，可以自动生成和验证npm包中漏洞的概念验证（PoC）利用。", "motivation": "软件包的安全漏洞对开发者和用户都是一个重大问题，及时修补这些漏洞对于恢复软件系统的完整性和安全性至关重要。然而，以前的工作表明，漏洞报告通常缺少概念验证（PoC）利用，这对于修复漏洞、测试补丁和避免回归至关重要。创建PoC利用具有挑战性，因为漏洞报告是非正式的，通常不完整，并且需要详细了解输入如何到达可能存在漏洞的API，以及它们如何到达安全相关的汇合点。", "method": "在本文中，我们提出了PoCGen，这是一种新的方法，可以自动生成和验证npm包中漏洞的概念验证（PoC）利用。这是第一个完全自动化的方法，结合使用大型语言模型（LLMs）和静态和动态分析技术来生成PoC利用。PoCGen利用LLM来理解漏洞报告，生成候选PoC利用，并验证和改进它们。", "result": "我们的方法成功生成了SecBench.js数据集中77%的漏洞的利用程序，以及794个新漏洞数据集中的39%的利用程序。这个成功率显著优于一个最近的基线（高出45个百分点），而生成每个利用程序的平均成本为0.02美元。", "conclusion": null}}
{"id": "2506.04963", "pdf": "https://arxiv.org/pdf/2506.04963", "abs": "https://arxiv.org/abs/2506.04963", "authors": ["Anton Firc", "Jan Klusáček", "Kamil Malinka"], "title": "Hiding in Plain Sight: Query Obfuscation via Random Multilingual Searches", "categories": ["cs.CR", "94A60, 68P27", "H.3.3; H.3.5; K.4.1"], "comment": "Accepted to TrustBus workshop of ARES 2025", "summary": "Modern search engines extensively personalize results by building detailed\nuser profiles based on query history and behaviour. While personalization can\nenhance relevance, it introduces privacy risks and can lead to filter bubbles.\nThis paper proposes and evaluates a lightweight, client-side query obfuscation\nstrategy using randomly generated multilingual search queries to disrupt user\nprofiling. Through controlled experiments on the Seznam.cz search engine, we\nassess the impact of interleaving real queries with obfuscating noise in\nvarious language configurations and ratios. Our findings show that while\ndisplayed search results remain largely stable, the search engine's identified\nuser interests shift significantly under obfuscation. We further demonstrate\nthat such random queries can prevent accurate profiling and overwrite\nestablished user profiles. This study provides practical evidence for query\nobfuscation as a viable privacy-preserving mechanism and introduces a tool that\nenables users to autonomously protect their search behaviour without modifying\nexisting infrastructure.", "AI": {"tldr": "提出了一种使用随机生成的多语言搜索查询来干扰用户画像的轻量级客户端查询混淆策略。", "motivation": "现代搜索引擎通过构建基于查询历史和行为详细用户资料来个性化搜索结果。虽然个性化可以提高相关性，但它引入了隐私风险，并可能导致过滤气泡。", "method": "通过在Seznam.cz搜索引擎上进行控制实验，评估了在各种语言配置和比率下，将真实查询与混淆噪声交织在一起的影响。", "result": "研究结果表明，虽然显示的搜索结果总体稳定，但在混淆的情况下，搜索引擎识别的用户兴趣会发生显著变化。", "conclusion": "提出并评估了一种轻量级的客户端查询混淆策略，使用随机生成的多语言搜索查询来干扰用户画像。研究表明，虽然显示的搜索结果总体稳定，但在混淆的情况下，搜索引擎识别的用户兴趣会发生显著变化。随机查询可以防止准确的用户画像，并覆盖现有的用户画像。这项研究为查询混淆作为一种可行的隐私保护机制提供了实际证据，并介绍了一种工具，使用户能够自主保护其搜索行为，而无需修改现有基础设施。"}}
{"id": "2506.04978", "pdf": "https://arxiv.org/pdf/2506.04978", "abs": "https://arxiv.org/abs/2506.04978", "authors": ["Gabriele Digregorio", "Elisabetta Cainazzo", "Stefano Longari", "Michele Carminati", "Stefano Zanero"], "title": "Evaluating the Impact of Privacy-Preserving Federated Learning on CAN Intrusion Detection", "categories": ["cs.CR"], "comment": null, "summary": "The challenges derived from the data-intensive nature of machine learning in\nconjunction with technologies that enable novel paradigms such as V2X and the\npotential offered by 5G communication, allow and justify the deployment of\nFederated Learning (FL) solutions in the vehicular intrusion detection domain.\nIn this paper, we investigate the effects of integrating FL strategies into the\nmachine learning-based intrusion detection process for on-board vehicular\nnetworks. Accordingly, we propose a FL implementation of a state-of-the-art\nIntrusion Detection System (IDS) for Controller Area Network (CAN), based on\nLSTM autoencoders. We thoroughly evaluate its detection efficiency and\ncommunication overhead, comparing it to a centralized version of the same\nalgorithm, thereby presenting it as a feasible solution.", "AI": {"tldr": "提出了一种基于LSTM自动编码器的FL实现，用于车载网络入侵检测，提高检测效率和降低通信开销。", "motivation": "解决车载网络入侵检测问题，提高检测效率和降低通信开销。", "method": "基于LSTM自动编码器的FL实现，用于车载网络入侵检测。", "result": "提出了一种基于LSTM自动编码器的FL实现，用于车载网络入侵检测，并证明其检测效率和通信开销优于集中式版本。", "conclusion": "提出了一种基于LSTM自动编码器的FL实现，用于车载网络入侵检测，并证明其检测效率和通信开销优于集中式版本。"}}
{"id": "2506.05001", "pdf": "https://arxiv.org/pdf/2506.05001", "abs": "https://arxiv.org/abs/2506.05001", "authors": ["Limin Wang", "Lei Bu", "Muzimiao Zhang", "Shihong Cang", "Kai Ye"], "title": "Attack Effect Model based Malicious Behavior Detection", "categories": ["cs.CR"], "comment": null, "summary": "Traditional security detection methods face three key challenges: inadequate\ndata collection that misses critical security events, resource-intensive\nmonitoring systems, and poor detection algorithms with high false positive\nrates. We present FEAD (Focus-Enhanced Attack Detection), a framework that\naddresses these issues through three innovations: (1) an attack model-driven\napproach that extracts security-critical monitoring items from online attack\nreports for comprehensive coverage; (2) efficient task decomposition that\noptimally distributes monitoring across existing collectors to minimize\noverhead; and (3) locality-aware anomaly analysis that leverages the clustering\nbehavior of malicious activities in provenance graphs to improve detection\naccuracy. Evaluations demonstrate FEAD achieves 8.23% higher F1-score than\nexisting solutions with only 5.4% overhead, confirming that focus-based designs\nsignificantly enhance detection performance.", "AI": {"tldr": "FEAD是一种基于关注点的攻击检测框架，通过优化数据收集、任务分解和异常分析，显著提高了安全检测性能。", "motivation": "传统的安全检测方法面临三个主要挑战：数据收集不足，监控系统资源密集，检测算法假阳性率高。", "method": "我们提出了FEAD（增强型攻击检测）框架，通过三个创新来解决这个问题：（1）一个基于攻击模型的提取方法，从在线攻击报告中提取关键安全监控项，实现全面覆盖；（2）有效的任务分解，将监控优化分配到现有的收集器中，以最小化开销；（3）基于局部性的异常分析，利用来源图中恶意活动的聚类行为来提高检测精度。", "result": "评估表明，与现有解决方案相比，FEAD实现了8.23%的F1分数提升，仅增加5.4%的开销，证实了基于关注点的设计显著提高了检测性能。", "conclusion": "基于关注点的FEAD框架显著提高了安全检测性能。"}}
{"id": "2506.05074", "pdf": "https://arxiv.org/pdf/2506.05074", "abs": "https://arxiv.org/abs/2506.05074", "authors": ["Robert J. Joyce", "Gideon Miller", "Phil Roth", "Richard Zak", "Elliott Zaresky-Williams", "Hyrum Anderson", "Edward Raff", "James Holt"], "title": "EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "A lack of accessible data has historically restricted malware analysis\nresearch, and practitioners have relied heavily on datasets provided by\nindustry sources to advance. Existing public datasets are limited by narrow\nscope - most include files targeting a single platform, have labels supporting\njust one type of malware classification task, and make no effort to capture the\nevasive files that make malware detection difficult in practice. We present\nEMBER2024, a new dataset that enables holistic evaluation of malware\nclassifiers. Created in collaboration with the authors of EMBER2017 and\nEMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,\nand labels for more than 3.2 million files from six file formats. Our dataset\nsupports the training and evaluation of machine learning models on seven\nmalware classification tasks, including malware detection, malware family\nclassification, and malware behavior identification. EMBER2024 is the first to\ninclude a collection of malicious files that initially went undetected by a set\nof antivirus products, creating a \"challenge\" set to assess classifier\nperformance against evasive malware. This work also introduces EMBER feature\nversion 3, with added support for several new feature types. We are releasing\nthe EMBER2024 dataset to promote reproducibility and empower researchers in the\npursuit of new malware research topics.", "AI": {"tldr": "EMBER2024 数据集支持更全面地评估恶意软件分类器。", "motivation": "解决现有公共数据集的局限性，例如范围狭窄、标签单一等问题。", "method": "EMBER2024 数据集包括超过 320 万个文件的哈希值、元数据、特征向量和标签，以六个文件格式创建。", "result": "EMBER2024 数据集是第一个包含未被发现恶意文件的集合，用于评估分类器对逃避恶意软件的性能。", "conclusion": "EMBER2024 数据集支持机器学习模型在七个恶意软件分类任务上的训练和评估，包括恶意软件检测、恶意软件家族分类和恶意软件行为识别。"}}
{"id": "2506.05126", "pdf": "https://arxiv.org/pdf/2506.05126", "abs": "https://arxiv.org/abs/2506.05126", "authors": ["Lorenzo Rossi", "Michael Aerni", "Jie Zhang", "Florian Tramèr"], "title": "Membership Inference Attacks on Sequence Models", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted to the 8th Deep Learning Security and Privacy Workshop\n  (DLSP) workshop (best paper award)", "summary": "Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models.", "AI": {"tldr": "我们通过改进成员推理攻击来提高序列模型隐私泄露审计的有效性。", "motivation": "序列模型，如大型语言模型（LLMs）和自回归图像生成器，倾向于记忆并无意中泄露敏感信息。", "method": "我们将最先进的成员推理攻击改编，以显式地建模序列内的相关性，从而展示了如何将强大的现有攻击自然地扩展以适应序列模型的结构。", "result": "我们的改编在引入额外的计算成本的同时，始终如一地提高了记忆审计的有效性。", "conclusion": "我们的工作为大型序列模型的可靠记忆审计提供了重要的基石。"}}
{"id": "2506.05129", "pdf": "https://arxiv.org/pdf/2506.05129", "abs": "https://arxiv.org/abs/2506.05129", "authors": ["Andrin Bertschi", "Shweta Shinde"], "title": "OpenCCA: An Open Framework to Enable Arm CCA Research", "categories": ["cs.CR"], "comment": null, "summary": "Confidential computing has gained traction across major architectures with\nIntel TDX, AMD SEV-SNP, and Arm CCA. Unlike TDX and SEV-SNP, a key challenge in\nresearching Arm CCA is the absence of hardware support, forcing researchers to\ndevelop ad-hoc performance prototypes on non-CCA Arm boards. This approach\nleads to duplicated efforts, inconsistent performance comparisons, and high\nbarriers to entry. To address this, we present OpenCCA, an open research\nplatform that enables the execution of CCA-bound code on commodity Armv8.2\nhardware. By systematically adapting the software stack -- including\nbootloader, firmware, hypervisor, and kernel -- OpenCCA emulates CCA operations\nfor performance evaluation while preserving functional correctness. We\ndemonstrate its effectiveness with typical life-cycle measurements and\ncase-studies inspired by prior CCA-based papers on a easily available Armv8.2\nRockchip board that costs $250.", "AI": {"tldr": "提出OpenCCA平台，解决Arm CCA研究中的硬件支持问题，实现性能评估和功能正确性。", "motivation": "解决Arm CCA研究中的硬件支持问题，降低研究门槛。", "method": "通过系统性地适配软件栈，包括引导加载程序、固件、虚拟机管理程序和内核，来模拟CCA操作。", "result": "在250美元的Armv8.2 Rockchip板上实现CCA性能评估和功能正确性。", "conclusion": "提出OpenCCA平台，解决Arm CCA研究中的硬件支持问题，实现性能评估和功能正确性。"}}
{"id": "2506.05242", "pdf": "https://arxiv.org/pdf/2506.05242", "abs": "https://arxiv.org/abs/2506.05242", "authors": ["Zhiqiang Wang", "Haohua Du", "Junyang Wang", "Haifeng Sun", "Kaiwen Guo", "Haikuo Yu", "Chao Liu", "Xiang-Yang Li"], "title": "SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid Neuron Encryption", "categories": ["cs.CR"], "comment": null, "summary": "Large language models (LLMs) with diverse capabilities are increasingly being\ndeployed in local environments, presenting significant security and\ncontrollability challenges. These locally deployed LLMs operate outside the\ndirect control of developers, rendering them more susceptible to abuse.\nExisting mitigation techniques mainly designed for cloud-based LLM services are\nfrequently circumvented or ineffective in deployer-controlled environments. We\npropose SECNEURON, the first framework that seamlessly embeds classic access\ncontrol within the intrinsic capabilities of LLMs, achieving reliable,\ncost-effective, flexible, and certified abuse control for local deployed LLMs.\nSECNEURON employs neuron-level encryption and selective decryption to\ndynamically control the task-specific capabilities of LLMs, limiting\nunauthorized task abuse without compromising others. We first design a\ntask-specific neuron extraction mechanism to decouple logically related neurons\nand construct a layered policy tree for handling coupled neurons. We then\nintroduce a flexible and efficient hybrid encryption framework for millions of\nneurons in LLMs. Finally, we developed a distribution-based decrypted neuron\ndetection mechanism on ciphertext to ensure the effectiveness of partially\ndecrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and\nCollusion Resistance Security under the Task Controllability Principle.\nExperiments on various task settings show that SECNEURON limits unauthorized\ntask accuracy to below 25% while keeping authorized accuracy loss with 2%.\nUsing an unauthorized Code task example, the accuracy of abuse-related\nmalicious code generation was reduced from 59% to 15%. SECNEURON also mitigates\nunauthorized data leakage, reducing PII extraction rates to below 5% and\nmembership inference to random guesses.", "AI": {"tldr": "SECNEURON 为本地部署的 LLM 提供了可靠的安全控制。", "motivation": "本地部署的 LLM 存在着安全性和可控性挑战，现有的缓解技术无法有效应对。", "method": "SECNEURON 使用神经元级加密和选择性解密来动态控制 LLM 的任务特定能力，同时限制未经授权的任务滥用，而不影响其他任务。", "result": "SECNEURON 在各种任务设置中限制了未经授权的任务准确性，同时保持了授权准确性的损失在 2% 以内。", "conclusion": "SECNEURON 是第一个将经典访问控制嵌入到 LLM 内在能力中的框架，为本地部署的 LLM 提供了可靠、经济、灵活和可验证的滥用控制。"}}
{"id": "2506.05290", "pdf": "https://arxiv.org/pdf/2506.05290", "abs": "https://arxiv.org/abs/2506.05290", "authors": ["Pierre Tholoniat", "Alison Caulfield", "Giorgio Cavicchioli", "Mark Chen", "Nikos Goutzoulias", "Benjamin Case", "Asaf Cidon", "Roxana Geambasu", "Mathias Lécuyer", "Martin Thomson"], "title": "Big Bird: Privacy Budget Management for W3C's Privacy-Preserving Attribution API", "categories": ["cs.CR"], "comment": null, "summary": "Privacy-preserving advertising APIs like Privacy-Preserving Attribution (PPA)\nare designed to enhance web privacy while enabling effective ad measurement.\nPPA offers an alternative to cross-site tracking with encrypted reports\ngoverned by differential privacy (DP), but current designs lack a principled\napproach to privacy budget management, creating uncertainty around critical\ndesign decisions. We present Big Bird, a privacy budget manager for PPA that\nclarifies per-site budget semantics and introduces a global budgeting system\ngrounded in resource isolation principles. Big Bird enforces utility-preserving\nlimits via quota budgets and improves global budget utilization through a novel\nbatched scheduling algorithm. Together, these mechanisms establish a robust\nfoundation for enforcing privacy protections in adversarial environments. We\nimplement Big Bird in Firefox and evaluate it on real-world ad data,\ndemonstrating its resilience and effectiveness.", "AI": {"tldr": "Big Bird是一种用于PPA的隐私预算管理器，通过资源隔离和批量调度提高预算利用率，增强隐私保护。", "motivation": "PPA设计旨在增强网络隐私并实现有效的广告测量，但当前设计缺乏隐私预算管理的原则方法，导致设计决策的不确定性。", "method": "Big Bird通过资源隔离原则和批量调度算法管理隐私预算，并引入了全局预算系统。", "result": "Big Bird在Firefox中实现，并在真实广告数据上评估，证明了其弹性和有效性。", "conclusion": "Big Bird为PPA提供了一种隐私预算管理方法，通过资源隔离原则和批量调度算法提高了全球预算利用率，为在对抗环境中执行隐私保护提供了坚实的基础。"}}
{"id": "2506.05346", "pdf": "https://arxiv.org/pdf/2506.05346", "abs": "https://arxiv.org/abs/2506.05346", "authors": ["Lei Hsiung", "Tianyu Pang", "Yung-Chen Tang", "Linyue Song", "Tsung-Yi Ho", "Pin-Yu Chen", "Yaoqing Yang"], "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "Project Page: https://hsiung.cc/llm-similarity-risk/", "summary": "Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers.", "AI": {"tldr": "该研究表明，上游数据集的设计对于构建持久安全护栏和减少对越狱攻击的脆弱性至关重要。", "motivation": "现有缓解策略主要关注在安全护栏被破坏后，在微调期间移除有害梯度或在微调期间持续强化安全对齐，但这些策略往往忽略了原始安全对齐数据的重要性。", "method": "通过实验研究上游对齐数据集与下游微调任务之间的表示相似性来评估安全护栏的退化。", "result": "实验表明，这些数据集之间的高度相似性会削弱安全护栏，而低相似性则会提高模型的鲁棒性。", "conclusion": "通过研究上游对齐数据集与下游微调任务之间的表示相似性，我们发现高相似性会削弱安全护栏，使模型更容易受到越狱攻击；而低相似性则会提高模型的鲁棒性，降低有害性评分高达10.33%。这些发现强调了上游数据集设计在构建持久安全护栏和减少对越狱攻击的脆弱性方面的重要性，为微调服务提供商提供了可操作的见解。"}}
{"id": "2506.03614", "pdf": "https://arxiv.org/pdf/2506.03614", "abs": "https://arxiv.org/abs/2506.03614", "authors": ["Zhanhui Zhou", "Lingjie Chen", "Chao Yang", "Chaochao Lu"], "title": "VLMs Can Aggregate Scattered Training Patches", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as $\\textit{visual\nstitching}$ -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each $(\\texttt{image}, \\texttt{ID})$ pair into $\\{(\\texttt{patch},\n\\texttt{ID})\\}$ pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.", "AI": {"tldr": "我们发现了VLM的视觉拼接能力，并展示了如何通过视觉拼接逃避监管，对VLM的安全构成严重风险。", "motivation": "为了减轻VLM中的风险，我们提出了一种通过去除训练数据中的危险样本来缓解风险的方法。然而，当有害图像被分割成小而无害的片段并散布在许多训练样本中时，这种数据监管可以很容易地被绕过。", "method": "我们首先在三个数据集上展示了常见开源VLM的视觉拼接能力，然后模拟了敌对数据中毒场景，使用危险图像的片段，并用文本描述替换ID，展示了有害内容如何逃避监管并通过视觉拼接重建。", "result": "我们定义了VLM的核心能力，即视觉拼接，并展示了在三个数据集上常见开源VLM的视觉拼接能力。我们还模拟了敌对数据中毒场景，证明了有害内容如何通过视觉拼接逃避监管，对VLM的安全构成严重风险。", "conclusion": "我们定义了VLM的核心能力，即视觉拼接，并展示了在三个数据集上常见开源VLM的视觉拼接能力。我们还模拟了上述敌对数据中毒场景，证明了有害内容如何通过视觉拼接逃避监管，对VLM的安全构成严重风险。"}}
{"id": "2506.04462", "pdf": "https://arxiv.org/pdf/2506.04462", "abs": "https://arxiv.org/abs/2506.04462", "authors": ["Apurv Verma", "NhatHai Phan", "Shubhendu Trivedi"], "title": "Watermarking Degrades Alignment in Language Models: Analysis and Mitigation", "categories": ["cs.CL", "cs.CR", "cs.LG", "I.2.7"], "comment": "Published at the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025. OpenReview: https://openreview.net/forum?id=SIBkIV48gF", "summary": "Watermarking techniques for large language models (LLMs) can significantly\nimpact output quality, yet their effects on truthfulness, safety, and\nhelpfulness remain critically underexamined. This paper presents a systematic\nanalysis of how two popular watermarking approaches-Gumbel and KGW-affect these\ncore alignment properties across four aligned LLMs. Our experiments reveal two\ndistinct degradation patterns: guard attenuation, where enhanced helpfulness\nundermines model safety, and guard amplification, where excessive caution\nreduces model helpfulness. These patterns emerge from watermark-induced shifts\nin token distribution, surfacing the fundamental tension that exists between\nalignment objectives.\n  To mitigate these degradations, we propose Alignment Resampling (AR), an\ninference-time sampling method that uses an external reward model to restore\nalignment. We establish a theoretical lower bound on the improvement in\nexpected reward score as the sample size is increased and empirically\ndemonstrate that sampling just 2-4 watermarked generations effectively recovers\nor surpasses baseline (unwatermarked) alignment scores. To overcome the limited\nresponse diversity of standard Gumbel watermarking, our modified implementation\nsacrifices strict distortion-freeness while maintaining robust detectability,\nensuring compatibility with AR. Experimental results confirm that AR\nsuccessfully recovers baseline alignment in both watermarking approaches, while\nmaintaining strong watermark detectability. This work reveals the critical\nbalance between watermark strength and model alignment, providing a simple\ninference-time solution to responsibly deploy watermarked LLMs in practice.", "AI": {"tldr": "本文研究了水印技术对LLMs的影响，并提出了一种对齐重采样方法来提高水印LLMs的性能。", "motivation": "水印技术对大型语言模型（LLMs）的输出质量有重大影响，但其对真实性、安全性和有用性的影响尚未得到充分研究。", "method": "提出了一种名为对齐重采样的推理时间采样方法，使用外部奖励模型来恢复对齐。", "result": "实验揭示了两种不同的退化模式：保护衰减和保护放大。提出的方法可以有效地恢复或超过基线（未水印）的对齐分数。", "conclusion": "本文揭示了水印强度与模型对齐之间的关键平衡，为在实际中负责任地部署水印LLMs提供了一种简单的推理时间解决方案。"}}
{"id": "2506.04681", "pdf": "https://arxiv.org/pdf/2506.04681", "abs": "https://arxiv.org/abs/2506.04681", "authors": ["Daogao Liu", "Edith Cohen", "Badih Ghazi", "Peter Kairouz", "Pritish Kamath", "Alexander Knop", "Ravi Kumar", "Pasin Manurangsi", "Adam Sealfon", "Da Yu", "Chiyuan Zhang"], "title": "Urania: Differentially Private Insights into AI Use", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "We introduce $Urania$, a novel framework for generating insights about LLM\nchatbot interactions with rigorous differential privacy (DP) guarantees. The\nframework employs a private clustering mechanism and innovative keyword\nextraction methods, including frequency-based, TF-IDF-based, and LLM-guided\napproaches. By leveraging DP tools such as clustering, partition selection, and\nhistogram-based summarization, $Urania$ provides end-to-end privacy protection.\nOur evaluation assesses lexical and semantic content preservation, pair\nsimilarity, and LLM-based metrics, benchmarking against a non-private\nClio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple\nempirical privacy evaluation that demonstrates the enhanced robustness of our\nDP pipeline. The results show the framework's ability to extract meaningful\nconversational insights while maintaining stringent user privacy, effectively\nbalancing data utility with privacy preservation.", "AI": {"tldr": "Urania是一个具有严格DP保证的框架，用于生成LLM聊天机器人交互的洞察。", "motivation": "为了解决LLM聊天机器人交互中的隐私问题，并生成有意义的洞察。", "method": "Urania框架采用私有聚类机制和创新的关键词提取方法，包括基于频率、TF-IDF和LLM引导的方法。利用DP工具如聚类、分区选择和基于直方图的摘要，提供端到端隐私保护。", "result": "评估了词汇和语义内容的保留、对相似性和基于LLM的指标，并与非私有Clio-inspired管道进行了基准测试。结果显示，该框架能够提取有意义的对话洞察，同时保持严格的用户隐私。", "conclusion": "Urania框架可以有效地提取有意义的对话洞察，同时保持严格的用户隐私，在数据效用和隐私保护之间实现平衡。"}}
{"id": "2506.04909", "pdf": "https://arxiv.org/pdf/2506.04909", "abs": "https://arxiv.org/abs/2506.04909", "authors": ["Kai Wang", "Yihao Zhang", "Meng Sun"], "title": "When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "The honesty of large language models (LLMs) is a critical alignment\nchallenge, especially as advanced systems with chain-of-thought (CoT) reasoning\nmay strategically deceive humans. Unlike traditional honesty issues on LLMs,\nwhich could be possibly explained as some kind of hallucination, those models'\nexplicit thought paths enable us to study strategic deception--goal-driven,\nintentional misinformation where reasoning contradicts outputs. Using\nrepresentation engineering, we systematically induce, detect, and control such\ndeception in CoT-enabled LLMs, extracting \"deception vectors\" via Linear\nArtificial Tomography (LAT) for 89% detection accuracy. Through activation\nsteering, we achieve a 40% success rate in eliciting context-appropriate\ndeception without explicit prompts, unveiling the specific honesty-related\nissue of reasoning models and providing tools for trustworthy AI alignment.", "AI": {"tldr": "通过表示工程和激活引导，我们实现了对具有思维链（CoT）推理的LLMs中欺骗行为的检测和控制，为可信AI对齐提供了工具。", "motivation": "大型语言模型（LLMs）的诚实性是一个关键的对齐挑战，特别是随着具有思维链（CoT）推理的高级系统可能会战略性地欺骗人类。", "method": "使用表示工程，通过线性人工断层扫描（LAT）提取“欺骗向量”，通过激活引导实现欺骗。", "result": "实现了89%的欺骗检测精度，40%的成功率在无需明确提示的情况下引发与上下文相关的欺骗。", "conclusion": "通过表示工程，我们系统地诱导、检测和控制了具有思维链（CoT）推理的LLMs中的欺骗行为，通过线性人工断层扫描（LAT）提取“欺骗向量”，实现了89%的检测精度。通过激活引导，我们达到了40%的成功率，在无需明确提示的情况下引发与上下文相关的欺骗，揭示了推理模型的特定诚信问题，并为可信AI对齐提供了工具。"}}
{"id": "2506.05022", "pdf": "https://arxiv.org/pdf/2506.05022", "abs": "https://arxiv.org/abs/2506.05022", "authors": ["Yixuan Cao", "Yuhong Feng", "Huafeng Li", "Chongyi Huang", "Fangcao Jian", "Haoran Li", "Xu Wang"], "title": "Tech-ASan: Two-stage check for Address Sanitizer", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Address Sanitizer (ASan) is a sharp weapon for detecting memory safety\nviolations, including temporal and spatial errors hidden in C/C++ programs\nduring execution. However, ASan incurs significant runtime overhead, which\nlimits its efficiency in testing large software. The overhead mainly comes from\nsanitizer checks due to the frequent and expensive shadow memory access. Over\nthe past decade, many methods have been developed to speed up ASan by\neliminating and accelerating sanitizer checks, however, they either fail to\nadequately eliminate redundant checks or compromise detection capabilities. To\naddress this issue, this paper presents Tech-ASan, a two-stage check based\ntechnique to accelerate ASan with safety assurance. First, we propose a novel\ntwo-stage check algorithm for ASan, which leverages magic value comparison to\nreduce most of the costly shadow memory accesses. Second, we design an\nefficient optimizer to eliminate redundant checks, which integrates a novel\nalgorithm for removing checks in loops. Third, we implement Tech-ASan as a\nmemory safety tool based on the LLVM compiler infrastructure. Our evaluation\nusing the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the\nstate-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan\nand ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative\ncases than ASan and ASan-- when testing on the Juliet Test Suite under the same\nredzone setting.", "AI": {"tldr": "Tech-ASan是一种加速ASan的技术，通过减少运行时开销和误报案例来提高内存安全测试的效率。", "motivation": "为了解决ASan在大型软件测试中的效率问题，因为其运行时开销很大。", "method": "Tech-ASan通过提出一种新的两阶段检查算法来减少昂贵的影子内存访问，并设计了一个高效的优化器来消除冗余检查。", "result": "Tech-ASan在SPEC CPU2006基准测试中优于现有方法，减少了运行时开销并提高了检测准确性。", "conclusion": "Tech-ASan是一种基于两阶段检查的技术，可以加速ASan并保证安全性。与ASan和ASan--相比，Tech-ASan在运行时开销方面提高了33.70%和17.89%，在Juliet测试套件中检测到的误报案例减少了56个。"}}
{"id": "2506.05032", "pdf": "https://arxiv.org/pdf/2506.05032", "abs": "https://arxiv.org/abs/2506.05032", "authors": ["Zeming Wei", "Yiwen Guo", "Yisen Wang"], "title": "Identifying and Understanding Cross-Class Features in Adversarial Training", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "math.OC"], "comment": "ICML 2025", "summary": "Adversarial training (AT) has been considered one of the most effective\nmethods for making deep neural networks robust against adversarial attacks,\nwhile the training mechanisms and dynamics of AT remain open research problems.\nIn this paper, we present a novel perspective on studying AT through the lens\nof class-wise feature attribution. Specifically, we identify the impact of a\nkey family of features on AT that are shared by multiple classes, which we call\ncross-class features. These features are typically useful for robust\nclassification, which we offer theoretical evidence to illustrate through a\nsynthetic data model. Through systematic studies across multiple model\narchitectures and settings, we find that during the initial stage of AT, the\nmodel tends to learn more cross-class features until the best robustness\ncheckpoint. As AT further squeezes the training robust loss and causes robust\noverfitting, the model tends to make decisions based on more class-specific\nfeatures. Based on these discoveries, we further provide a unified view of two\nexisting properties of AT, including the advantage of soft-label training and\nrobust overfitting. Overall, these insights refine the current understanding of\nAT mechanisms and provide new perspectives on studying them. Our code is\navailable at https://github.com/PKU-ML/Cross-Class-Features-AT.", "AI": {"tldr": "本文通过研究AT中的跨类别特征，揭示了AT机制的新视角。", "motivation": "研究AT的训练机制和动态，以及提高深度神经网络对对抗攻击的鲁棒性。", "method": "通过研究类间特征归因来研究AT，并识别了多个类别共享的关键特征集，称为跨类别特征。", "result": "发现模型在AT的初始阶段倾向于学习更多跨类别特征，随着AT进一步挤压训练鲁棒损失并导致鲁棒过拟合，模型倾向于基于更多类别特定特征做出决策。", "conclusion": "这些见解细化了当前对AT机制的理解，并为研究它们提供了新的视角。"}}
{"id": "2506.05101", "pdf": "https://arxiv.org/pdf/2506.05101", "abs": "https://arxiv.org/abs/2506.05101", "authors": ["Clément Pierquin", "Aurélien Bellet", "Marc Tommasi", "Matthieu Boussard"], "title": "Privacy Amplification Through Synthetic Data: Insights from Linear Regression", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": "26 pages, ICML 2025", "summary": "Synthetic data inherits the differential privacy guarantees of the model used\nto generate it. Additionally, synthetic data may benefit from privacy\namplification when the generative model is kept hidden. While empirical studies\nsuggest this phenomenon, a rigorous theoretical understanding is still lacking.\nIn this paper, we investigate this question through the well-understood\nframework of linear regression. First, we establish negative results showing\nthat if an adversary controls the seed of the generative model, a single\nsynthetic data point can leak as much information as releasing the model\nitself. Conversely, we show that when synthetic data is generated from random\ninputs, releasing a limited number of synthetic data points amplifies privacy\nbeyond the model's inherent guarantees. We believe our findings in linear\nregression can serve as a foundation for deriving more general bounds in the\nfuture.", "AI": {"tldr": "通过线性回归框架，我们研究了合成数据的隐私放大现象，并证明了其潜力。", "motivation": "尽管经验研究表明合成数据可能从隐私放大中受益，但缺乏严格的理论理解。", "method": "我们通过线性回归框架研究这个问题，建立了负面的结果，并展示了合成数据在隐私放大方面的潜力。", "result": "我们证明了合成数据继承了生成模型的差分隐私保证，并且当生成模型保持隐藏时，合成数据可能从隐私放大中受益。", "conclusion": "通过线性回归框架，我们证明了合成数据继承了生成模型的差分隐私保证，并且当生成模型保持隐藏时，合成数据可能从隐私放大中受益。我们展示了如果攻击者控制生成模型的种子，单个合成数据点可能泄露与发布模型本身一样多的信息。相反，当合成数据从随机输入生成时，发布有限数量的合成数据点可以放大隐私，超越模型的固有保证。"}}
