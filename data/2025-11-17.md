<div id=toc></div>

# Table of Contents

- [math.OC](#math.OC) [Total: 8]
- [cs.AI](#cs.AI) [Total: 33]
- [cs.LG](#cs.LG) [Total: 77]


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [1] [TSP integrality gap via 2-edge-connected multisubgraph problem under coincident IP optima](https://arxiv.org/abs/2511.11215)
*Toshiaki Yamanaka*

Main category: math.OC

TL;DR: 提出了一个转移原理：当2ECM问题的整数最优解是唯一哈密顿回路时，任何输出哈密顿回路的2ECM α-近似算法都能得到TSP的α-近似。建立了割边距唯一性框架来认证唯一性，并证明如果存在2ECM同时具有唯一哈密顿回路整数最优解和半整数LP解，则TSP的整数性间隙最多为4/3。


<details>
  <summary>Details</summary>
Motivation: 解决度量旅行商问题线性规划松弛的整数性间隙这一长期开放问题，通过建立2ECM问题与TSP之间的联系来推进对TSP近似比的理解。

Method: 引入转移原理和割边距唯一性框架，该框架能够认证哈密顿回路作为唯一整数最优解，并且在ℓ∞有界扰动下保持稳定。

Result: 证明如果存在同时满足唯一哈密顿回路整数最优解和半整数LP解的2ECM实例，则TSP的整数性间隙最多为4/3。

Conclusion: 构造同时满足唯一哈密顿回路整数最优解和半整数LP解的2ECM实例仍然是一个开放问题，但该理论框架为理解TSP的整数性间隙提供了新的途径。

Abstract: Determining the integrality gap of the linear programming (LP) relaxation of the metric traveling salesman problem (TSP) remains a long-standing open problem. We introduce a transfer principle: when the integer optimum of the 2-edge-connected multisubgraph problem (2ECM) is a unique Hamiltonian cycle $T$, any $α$-approximation algorithm for 2ECM that outputs a Hamiltonian cycle yields an $α$-approximation for TSP. We further develop a cut-margin uniqueness framework that certifies $T$ as the unique integer optimum for both problems and is stable under $\ell_\infty$-bounded perturbations. We show that, if instances exist where the 2ECM has both a unique Hamiltonian cycle integer optimum and a half-integral LP solution, then the TSP integrality gap is at most 4/3 by the algorithm of Boyd et al. (SIAM Journal on Discrete Mathematics 36:1730--1747, 2022). Constructing such instances remains an open problem.

</details>


### [2] [Risk averse deterministic Kalman filters for uncertain dynamical systems](https://arxiv.org/abs/2511.11350)
*Karl Kunisch,Jesper Schröder*

Main category: math.OC

TL;DR: 本文研究具有状态算子参数不确定性的系统的卡尔曼-布西滤波器扩展，重点在于降低大重构误差概率的风险规避设计。


<details>
  <summary>Details</summary>
Motivation: 研究状态算子存在参数不确定性的系统状态重构问题，特别关注降低大重构误差概率的风险规避设计方法。

Method: 采用确定性视角，推导包含参数不确定性的卡尔曼-布西滤波器扩展，通过理论分析得出不确定性方差相关的误差界限。

Result: 推导了不确定性方差相关的误差界限，并通过两个数值算例实现了风险中性和风险规避估计器的比较。

Conclusion: 该研究为参数不确定性系统的状态重构提供了风险规避的滤波器设计方法，并通过理论分析和数值实验验证了其有效性。

Abstract: Taking a deterministic viewpoint this work investigates extensions of the Kalman-Bucy filter for state reconstruction to systems containing parametric uncertainty in the state operator. The emphasis lies on risk averse designs reducing the probability of large reconstruction errors. In a theoretical analysis error bounds in terms of the variance of the uncertainties are derived. The article concludes with a numerical implementation of two examples allowing for a comparison of risk neutral and risk averse estimators.

</details>


### [3] [Linear-Space Extragradient Methods for Fast, Large-Scale Optimal Transport](https://arxiv.org/abs/2511.11359)
*Matthew X. Burns,Jiaming Liang*

Main category: math.OC

TL;DR: 提出了首个仅使用O(n)内存就能实现O(n²ε⁻¹)复杂度的最优传输算法——对偶外梯度方法(DXG)，解决了传统方法需要O(n²)存储空间的问题。


<details>
  <summary>Details</summary>
Motivation: 最优传输(OT)及其熵正则化形式(EOT)在机器学习和统计学中应用广泛，但现有的一阶方法在获得最优收敛率时需要O(n²)存储空间来进行遍历原始平均，存在存储效率瓶颈。

Method: 采用原始-对偶外梯度方法(PDXG)，完全在对偶空间中实现，仅需O(n)存储空间。证明了重新表述的OT问题正则化等价于EOT，并可扩展到熵正则化重心问题。

Result: 提出的对偶外梯度方法(DXG)是首个在O(n)内存下实现O(n²ε⁻¹)复杂度的ε-近似OT算法。数值实验显示在非/弱正则化情况下具有良好扩展性。

Conclusion: 该方法显著降低了最优传输问题的存储需求，为大规模应用提供了可行方案，但在某些问题类别中性能仍需进一步改进。

Abstract: Optimal transport (OT) and its entropy-regularized form (EOT) have become increasingly prominent computational problems, with applications in machine learning and statistics. Recent years have seen a commensurate surge in first-order methods aiming to improve the complexity of large-scale (E)OT. However, there has been a consistent tradeoff: attaining state-of-the-art rates requires $\mathcal{O}(n^2)$ storage to enable ergodic primal averaging. In this work, we demonstrate that recently proposed primal-dual extragradient methods (PDXG) can be implemented entirely in the dual with $\mathcal{O}(n)$ storage. Additionally, we prove that regularizing the reformulated OT problem is equivalent to EOT with extensions to entropy-regularized barycenter problems, further widening the applications of the proposed method. The proposed dual-only extragradient method (DXG) is the first algorithm to achieve $\mathcal{O}(n^2\varepsilon^{-1})$ complexity for $\varepsilon$-approximate OT with $\mathcal{O}(n)$ memory. Numerical experiments demonstrate that the dual extragradient method scales favorably in non/weakly-regularized regimes compared to existing algorithms, though future work is needed to improve performance in certain problem classes.

</details>


### [4] [Optimal Dividend, Reinsurance and Capital Injection Strategies for Collaborating Business Lines: The Case of Excess-of-Loss Reinsurance](https://arxiv.org/abs/2511.11383)
*Tim J. Boonen,Engel John C. Dela Vega*

Main category: math.OC

TL;DR: 保险公司在两个业务线之间优化分红、再保险和资本注入策略，目标是最大化加权总贴现分红。最优策略包括阈值型分红、纯超额损失再保险和资本转移以防止破产。


<details>
  <summary>Details</summary>
Motivation: 研究保险公司如何在两个协作业务线之间协调分红、再保险和资本注入决策，以最大化股东价值同时管理破产风险。

Method: 使用扩散近似模型描述各业务线的准备金水平，通过动态规划方法求解最优控制问题，获得闭式解。

Result: 证明有界分红率下最优分红策略为阈值型，无界分红率下为障碍型；最优再保险组合为纯超额损失再保险；风险转移随总准备金增加而减少；最优资本注入涉及转移准备金以防止单线破产。

Conclusion: 该研究为多业务线保险公司提供了完整的风险管理框架，明确了分红、再保险和资本转移的最优策略，有助于提升公司价值和风险管理效率。

Abstract: This paper considers an insurer with two collaborating business lines that must make three critical decisions: (1) dividend payout, (2) a combination of proportional and excess-of-loss reinsurance coverage, and (3) capital injection between the lines. The reserve level of each line is modeled using a diffusion approximation, with the insurer's objective being to maximize the weighted total discounted dividends paid until the first ruin time. We obtain the value function and the optimal strategies in closed form. We then prove that the optimal dividend payout strategy for bounded dividend rates is of threshold type, while for unbounded dividend rates it is of barrier type. The optimal combination of proportional and excess-of-loss reinsurance is shown to be pure excess-of-loss reinsurance. We also show that the optimal level of risk ceded to the reinsurer decreases as the aggregate reserve level increases. The optimal capital injection strategy involves transferring reserves to prevent the ruin of one line. Finally, numerical examples are presented to illustrate these optimal strategies.

</details>


### [5] [Lispchitz modulus of the argmin mapping in convex quadratic optimization](https://arxiv.org/abs/2511.11455)
*María Josefa Cánovas,Masao Fukushima,Juan Parra*

Main category: math.OC

TL;DR: 本文研究了规范扰动凸二次问题中argmin映射的Lipschitz模计算，并给出了仅依赖于标称数据的点基公式，将线性规划中的结果扩展到二次设置。


<details>
  <summary>Details</summary>
Motivation: 最初动机是计算欧几里得空间中多面体凸集上度量投影的Lipschitz模，当参考点和投影多面体都受到扰动时。

Method: 在规范扰动凸二次问题的框架下，计算argmin映射的Lipschitz模，并推导出仅依赖于标称数据的点基公式。

Result: 成功获得了argmin映射Lipschitz模的点基公式，并将线性规划中的相关结果扩展到二次设置。

Conclusion: 作为应用，提供了多面体凸集上度量投影Lipschitz模的点基公式，扩展了线性规划中的已有结果。

Abstract: This paper was initially motivated by the computation of the Lipschitz modulus of the metric projection on polyhedral convex sets in the Euclidean space when both the reference point and the polyhedron where it is projected are subject to perturbations. The paper tackles the more general problem of computing the Lipschitz modulus of the argmin mapping in the framework of canonically perturbed convex quadratic problems. We point out the fact that a point-based formula (depending only on the nominal data) for such a modulus is provided. In this way, the paper extends to the current quadratic setting some results previously developed in linear programming. As an application, we provide a point-based formula for the Lipschitz modulus of the metric projection on a polyhedral convex set.

</details>


### [6] [Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates](https://arxiv.org/abs/2511.11466)
*Dmitry Kovalev,Ekaterina Borodich*

Main category: math.OC

TL;DR: 本文为几种非欧几里得SGD方法（如SignSGD、Lion、Muon）提供了统一收敛分析，证明它们能够利用Hessian和梯度噪声的上界稀疏性或低秩结构，并匹配自适应优化算法的最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有理论分析无法解释非欧几里得SGD方法在深度神经网络训练中的优越性能，因为它们未能超越传统欧几里得SGD的收敛速率。

Method: 在结构化平滑性和梯度噪声假设下开发新的统一收敛分析框架。

Result: 证明非欧几里得SGD能够：(i)利用Hessian和梯度噪声上界的稀疏性或低秩结构；(ii)从外推法或动量方差减少等算法工具中获益；(iii)匹配AdaGrad和Shampoo等自适应算法的收敛速率。

Conclusion: 解决了非欧几里得SGD理论分析的重要开放问题，为其实际成功提供了理论依据。

Abstract: Recently, several instances of non-Euclidean SGD, including SignSGD, Lion, and Muon, have attracted significant interest from the optimization community due to their practical success in training deep neural networks. Consequently, a number of works have attempted to explain this success by developing theoretical convergence analyses. Unfortunately, these results cannot properly justify the superior performance of these methods, as they could not beat the convergence rate of vanilla Euclidean SGD. We resolve this important open problem by developing a new unified convergence analysis under the structured smoothness and gradient noise assumption. In particular, our results indicate that non-Euclidean SGD (i) can exploit the sparsity or low-rank structure of the upper bounds on the Hessian and gradient noise, (ii) can provably benefit from popular algorithmic tools such as extrapolation or momentum variance reduction, and (iii) can match the state-of-the-art convergence rates of adaptive and more complex optimization algorithms such as AdaGrad and Shampoo.

</details>


### [7] [Distributed Optimization of Pairwise Polynomial Graph Spectral Functions via Subgraph Optimization](https://arxiv.org/abs/2511.11517)
*Jitian Liu,Nicolas Kozachuk,Subhrajit Bhattacharya*

Main category: math.OC

TL;DR: 提出了一种分布式优化方法，用于在固定拓扑和全局权重预算下优化有限度多项式拉普拉斯谱目标，通过局部子图问题和SVD测试实现近似全局下降方向，结合度正则化预热和学习型提议器，达到接近集中式优化的性能。


<details>
  <summary>Details</summary>
Motivation: 研究整个拉普拉斯谱的集体行为优化，而非仅关注极值特征值，旨在开发适用于大规模几何图且保持约束的实用谱感知权重调优方法。

Method: 将全局成本重新表述为双线性形式，推导局部子图问题；使用基于SVD的ZC矩阵测试确保梯度近似对齐全局下降方向；采用迭代嵌入方案在不相交的1跳邻域上操作；引入度正则化预热和学习型提议器预测单次边更新。

Result: 实现了约95%相对于集中式优化的性能，方法能够扩展到大型几何图，同时保持可行性约束（正性和预算）。

Conclusion: 这些组件形成了一个实用、模块化的谱感知权重调优流程，适用于更广泛的整体谱成本类别，同时保持约束条件。

Abstract: We study distributed optimization of finite-degree polynomial Laplacian spectral objectives under fixed topology and a global weight budget, targeting the collective behavior of the entire spectrum rather than a few extremal eigenvalues. By re-formulating the global cost in a bilinear form, we derive local subgraph problems whose gradients approximately align with the global descent direction via an SVD-based test on the $ZC$ matrix. This leads to an iterate-and-embed scheme over disjoint 1-hop neighborhoods that preserves feasibility by construction (positivity and budget) and scales to large geometric graphs. For objectives that depend on pairwise eigenvalue differences $h(λ_i-λ_j)$, we obtain a quadratic upper bound in the degree vector, which motivates a ``warm-start'' by degree-regularization. The warm start uses randomized gossip to estimate global average degree, accelerating subsequent local descent while maintaining decentralization, and realizing $\sim95\%{}$ of the performance with respect to centralized optimization. We further introduce a learning-based proposer that predicts one-shot edge updates on maximal 1-hop embeddings, yielding immediate objective reductions. Together, these components form a practical, modular pipeline for spectrum-aware weight tuning that preserves constraints and applies across a broader class of whole-spectrum costs.

</details>


### [8] [Drone Swarm Energy Management](https://arxiv.org/abs/2511.11557)
*Michael Z. Zgurovsky,Pavlo O. Kasyanov,Liliia S. Paliichuk*

Main category: math.OC

TL;DR: 提出了一个基于POMDP与DDPG强化学习集成的无人机群决策分析框架，用于不确定环境下的自适应控制和协作行为。


<details>
  <summary>Details</summary>
Motivation: 解决无人机群在部分可观测环境中的决策问题，提高任务成功率和能源效率，为可扩展的认知群体自主性提供基础。

Method: 将标准DDPG架构扩展为基于贝叶斯滤波的信念状态表示，结合POMDP进行决策，在连续问题离散化版本中比较DDPG策略与最优策略。

Result: 仿真结果表明，POMDP-DDPG群体控制模型相比基线方法显著提高了任务成功率和能源效率。

Conclusion: 该框架为智能多智能体系统的能源感知控制算法发展做出贡献，可应用于安全、环境监测和基础设施检查等场景。

Abstract: This note presents an analytical framework for decision-making in drone swarm systems operating under uncertainty, based on the integration of Partially Observable Markov Decision Processes (POMDP) with Deep Deterministic Policy Gradient (DDPG) reinforcement learning. The proposed approach enables adaptive control and cooperative behavior of unmanned aerial vehicles (UAVs) within a cognitive AI platform, where each agent learns optimal energy management and navigation policies from dynamic environmental states. We extend the standard DDPG architecture with a belief-state representation derived from Bayesian filtering, allowing for robust decision-making in partially observable environments. In this paper, for the Gaussian case, we numerically compare the performance of policies derived from DDPG to optimal policies for discretized versions of the original continuous problem. Simulation results demonstrate that the POMDP-DDPG-based swarm control model significantly improves mission success rates and energy efficiency compared to baseline methods. The developed framework supports distributed learning and decision coordination across multiple agents, providing a foundation for scalable cognitive swarm autonomy. The outcomes of this research contribute to the advancement of energy-aware control algorithms for intelligent multi-agent systems and can be applied in security, environmental monitoring, and infrastructure inspection scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: Co-EPG是一个自迭代训练框架，通过规划和接地模型的协同进化来提升GUI任务自动化能力，无需外部数据即可在3次迭代内超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前GUI代理方法存在的两个基本限制：(1)跨模型协同利用不足，(2)过度依赖合成数据生成而利用不足。

Method: 提出Co-EPG框架，建立迭代正反馈循环：规划模型在接地奖励指导下通过GRPO探索更优策略，生成多样化数据优化接地模型；同时优化的接地模型为后续规划模型训练提供更有效奖励。

Result: 在Multimodal-Mind2Web和AndroidControl基准测试中，仅经过3次迭代就超越了现有最优方法，且每次迭代都持续改进。

Conclusion: 为GUI代理建立了新的训练范式，从孤立优化转向集成的、自驱动的协同进化方法。

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [10] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 本文研究了多目标优化中的Pareto修剪问题，将其重新定义为多赢家投票问题，提出了新的质量度量方法directed coverage，并分析了不同质量度量的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 现实决策问题通常涉及多目标优化，决策者需要从Pareto最优解集中选择代表性子集，现有质量度量方法存在不直观行为，需要更好的度量方法和计算分析。

Method: 将Pareto修剪重构为多赢家投票问题，进行公理分析，提出新的directed coverage度量，分析不同质量度量的计算复杂性边界，并进行实验评估。

Result: 发现现有质量度量存在不直观行为，提出的directed coverage度量在各种设置下表现竞争性甚至更优，确定了质量度量优化问题的可处理性边界。

Conclusion: 质量度量的选择对所选解集特征有决定性影响，新提出的directed coverage度量是有效的替代方案，为多目标决策支持提供了理论和方法基础。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [11] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文研究了基于团宽度的编码能力，针对抽象论证框架设计了新的归约方法，将论证问题转化为(Q)SAT问题，并在线性保持团宽度的前提下实现了高效的编码。


<details>
  <summary>Details</summary>
Motivation: 图的结构度量（如树宽度）在计算复杂性中很重要，但团宽度作为更一般的参数，在稠密图上也能保持较小值。目前对团宽度的编码能力了解甚少，特别是在抽象论证框架中。

Method: 设计了从论证问题到(Q)SAT的新归约方法，称为有向分解引导(DDG)归约，能够线性保持团宽度。

Result: 为所有论证语义（包括计数）建立了新结果，证明DDG归约的额外开销在合理假设下无法显著改进。

Conclusion: 这项工作开启了理解团宽度编码能力的研究，为抽象论证问题提供了高效的(Q)SAT编码方案，并证明了归约效率的紧致性。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [12] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 本文提出了两种新的反事实决策指标：潜在结果排序概率(PoR)和实现最佳潜在结果概率(PoB)，用于在不确定性下进行因果推理的决策制定。


<details>
  <summary>Details</summary>
Motivation: 决策制定者通常通过比较候选行动的期望潜在结果来做出选择，但现有方法可能无法充分捕捉个体层面的结果排序概率信息。

Method: 建立了PoR和PoB指标的识别定理和边界，并提出了相应的估计方法。

Result: 通过数值实验验证了估计器的有限样本性质，并在真实世界数据集上展示了应用效果。

Conclusion: PoR和PoB为反事实决策提供了新的有效工具，能够更好地反映个体层面的结果偏好和概率信息。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [13] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 这篇论文重新审视了大型语言模型中的推理能力，提出了自适应推理的概念，即根据任务难度和不确定性动态分配推理资源，并建立了系统的分类框架来组织现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs对所有任务采用统一的推理策略，导致简单问题过度推理而困难问题推理不足，缺乏根据任务特性调整推理强度的能力。

Method: 首先形式化了演绎、归纳和溯因推理在LLM中的实现；然后将自适应推理定义为控制增强的策略优化问题；最后提出了系统分类法，将方法分为基于训练和免训练两类。

Result: 建立了自适应推理的形式化框架，澄清了不同机制如何在实际中实现自适应推理，并支持跨策略的系统比较。

Conclusion: 识别了自我评估、元推理和人类对齐推理控制等开放挑战，为未来研究提供了方向。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [14] [HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments](https://arxiv.org/abs/2511.10810)
*Ran Elgedawy,Sanjay Das,Ethan Seefried,Gavin Wiggins,Ryan Burchfield,Dana Hewit,Sudarshan Srinivasan,Todd Thomas,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: HARNESS是一个模块化AI框架，用于预测危险事件和分析美国能源部环境中的操作风险，结合LLM、结构化工作数据和专家反馈来主动识别潜在危险。


<details>
  <summary>Details</summary>
Motivation: 在任务关键工作场所确保操作安全至关重要，因为这些环境中的日常任务复杂且危险。

Method: 集成大型语言模型与结构化工作数据、历史事件检索和风险分析，采用人在回路机制让领域专家优化预测，形成自适应学习循环。

Result: 初步部署显示有希望的结果，通过专家协作和迭代智能推理提高了预测安全系统的可靠性和效率。

Conclusion: HARNESS通过结合专家协作和迭代智能推理，改善了预测安全系统的可靠性和效率，未来工作将关注准确性、专家一致性和决策延迟的定量评估。

Abstract: Operational safety at mission-critical work sites is a top priority given the complex and hazardous nature of daily tasks. This paper presents the Human-Agent Risk Navigation and Event Safety System (HARNESS), a modular AI framework designed to forecast hazardous events and analyze operational risks in U.S. Department of Energy (DOE) environments. HARNESS integrates Large Language Models (LLMs) with structured work data, historical event retrieval, and risk analysis to proactively identify potential hazards. A human-in-the-loop mechanism allows subject matter experts (SMEs) to refine predictions, creating an adaptive learning loop that enhances performance over time. By combining SME collaboration with iterative agentic reasoning, HARNESS improves the reliability and efficiency of predictive safety systems. Preliminary deployment shows promising results, with future work focusing on quantitative evaluation of accuracy, SME agreement, and decision latency reduction.

</details>


### [15] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx是一种混合知识图谱嵌入框架，通过注意力机制自适应结合双曲、复数和欧几里得空间，解决了现有方法在处理不同类型关系时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法在处理大规模多样化关系类型时存在关键限制：欧几里得模型难以处理层次结构，向量空间模型无法捕捉不对称关系，双曲模型在对称关系上表现不佳。

Method: 提出关系特定的空间加权策略，通过学习的注意力机制为每种关系类型动态选择最优几何空间，同时使用多空间一致性损失确保跨空间预测的连贯性。

Result: 在计算机科学研究知识图谱上评估，范围从1K论文到10M论文，相比TransE、RotatE、DistMult等基线模型获得持续改进。在10M论文数据集上达到0.612 MRR，相对最佳基线提升4.8%，同时保持高效训练和85ms/三元组的推理速度。

Conclusion: HyperComplEx通过自适应维度分配实现与图大小的近线性扩展，为可扩展知识图谱嵌入研究提供了有效解决方案。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [16] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 开发了一个多智能体AI框架，用于从碎片化的碰撞数据中重建事故前场景和推断车辆行为，在复杂碰撞案例中达到100%准确率，超越人类专家的92%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统交通事故重建依赖人类专家，在处理不完整多模态数据时结果不一致，需要更精确和一致的重建方法。

Method: 采用两阶段协作框架：第一阶段从多模态输入生成自然语言事故重建；第二阶段结合重建结果与时间性事件数据记录器进行深度事故推理。

Result: 在39个复杂追尾事故案例中，框架在所有测试案例中达到完美准确率，成功识别最相关EDR事件并正确区分撞击与被撞车辆，在处理不完整数据时保持稳健性能。

Conclusion: 该研究展示了AI在处理异构碰撞数据方面的卓越能力，在重建碰撞动力学和表征事故前行为方面提供了前所未有的精确度。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [17] [Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning](https://arxiv.org/abs/2511.10857)
*Seyedeh Mobina Noorani,Shangde Gao,Changjie Chen,Karla Saldana Ochoa*

Main category: cs.AI

TL;DR: 提出了一种基于智能AI的规划支持系统，用于生成面向需求的动态规划单元，以改进灾害规划。该系统结合人类参与原则，通过改进的自组织映射算法和AI代理指导，实现交互式区域划分和风险评估。


<details>
  <summary>Details</summary>
Motivation: 传统的规划单元（如人口普查区、邮政编码等）往往无法准确反映当地社区的具体需求，缺乏灵活性来实施有效的灾害预防或应对策略。需要创建能够动态适应需求的规划单元。

Method: 开发了基于代表性初始化空间约束自组织映射（RepSC-SOM）的规划支持系统，扩展了传统SOM算法，加入自适应地理过滤和区域增长细化。AI代理能够推理、规划和行动，指导输入特征选择、空间约束设置，并支持交互式探索。

Result: 通过佛罗里达州杰克逊维尔市洪水相关风险的案例研究，展示了平台能够使用户交互式探索、生成和评估区域划分，将计算严谨性与用户驱动决策相结合。

Conclusion: 该平台成功实现了动态规划单元的生成，为灾害规划提供了更灵活、需求导向的方法，结合了AI的计算能力和人类的决策智慧。

Abstract: Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.

</details>


### [18] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: 提出了一种利用大型语言模型作为专家指导来学习神经退行性疾病进展的新框架，能够同时优化个体疾病轨迹构建和生物约束的脑区交互图结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法在预测神经退行性疾病（如阿尔茨海默病）进展时过于简化脑连接性，假设单一模态的脑连接组作为疾病传播基质，导致长期进展预测不准确。而纯数据驱动的方法由于缺乏适当约束面临可识别性问题。

Method: 使用大型语言模型作为专家指导，利用其整合多模态关系和多样化疾病驱动机制的能力，从不规则采样的纵向患者数据中同时优化：1）个体层面的长期疾病轨迹构建；2）具有更好可识别性的生物约束脑区交互图结构。

Result: 在阿尔茨海默病队列的tau-PET成像数据上验证，新框架相比传统方法展现出更优的预测准确性和可解释性，并揭示了超越传统连接性测量的额外疾病驱动因素。

Conclusion: 该框架通过结合LLM的专家知识和生物约束，有效解决了神经退行性疾病进展建模中的复杂性和可识别性问题，为理解疾病机制提供了新视角。

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [19] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: 提出了一个多智能体法律验证器，通过专门化的智能体分工协作来检查AI驱动的数据传输规划的法律合规性，在200个日本个人信息保护法修正案第16条案例上达到72%的准确率，比单智能体基线提高21个百分点。


<details>
  <summary>Details</summary>
Motivation: 在严格的隐私法规（如日本个人信息保护法APPI）下，AI驱动的数据传输规划的法律合规性变得越来越关键，需要开发可扩展且符合法规的自动化合规验证框架。

Method: 多智能体法律验证器，将合规检查分解为专门化的智能体：法规解释、业务背景评估和风险评估，通过结构化合成协议进行协调。

Result: 在200个分层APPI第16条修正案案例数据集上，系统达到72%准确率，比单智能体基线提高21个百分点；在明确合规案例上达到90%准确率（基线为16%），同时保持对明确违规的完美检测。

Conclusion: 领域专业化和协调推理能够显著提高法律AI性能，为可信赖和可解释的自动化合规验证提供了可扩展且符合法规的框架，尽管在模糊场景中仍存在挑战。

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [20] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 论文探讨了自主AI系统在遇到训练数据未覆盖的复杂场景时，如何构建、评估和证明候选行动方案，以满足人类期望和价值观。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在实际部署中必然会遇到训练数据未覆盖的场景，需要超越训练策略来构建和评估行动方案，以实现与人类期望和价值观一致的目标。

Method: 通过理论分析和实证案例研究，识别智能体决策所需的知识类型，包括规范性、实用性和情境性理解，以在复杂现实环境中选择更符合期望的行动方案。

Result: 确定了智能体在复杂环境中做出稳健决策所需的知识类型，包括整合规范性、实用性和情境性理解，以选择更符合人类期望的行动方案。

Conclusion: 自主AI系统需要超越训练策略，整合多种知识类型来构建、评估和证明候选行动方案，才能在复杂现实环境中实现与人类期望和价值观一致的目标。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [21] [AI Agent-Driven Framework for Automated Product Knowledge Graph Construction in E-Commerce](https://arxiv.org/abs/2511.11017)
*Dimitar Peshevski,Riste Stojanov,Dimitar Trajanov*

Main category: cs.AI

TL;DR: 提出一个基于AI代理的自动化框架，直接从非结构化产品描述构建产品知识图谱，使用LLM驱动的三阶段方法：本体创建与扩展、本体精炼和知识图谱填充。


<details>
  <summary>Details</summary>
Motivation: 电商平台产生大量非结构化产品数据，给信息检索、推荐系统和数据分析带来挑战。知识图谱能提供结构化格式，但构建产品特定知识图谱仍是复杂且手动的工作。

Method: 使用LLM驱动的AI代理框架，包含三个专用代理：本体创建与扩展代理、本体精炼代理和知识图谱填充代理，不依赖预定义模式或手工提取规则。

Result: 在真实空调产品数据集上评估，本体生成和知识图谱填充表现优异，达到超过97%的属性覆盖率和最小冗余度。

Conclusion: 该框架验证了LLM在零售领域自动化结构化知识提取的潜力，为智能产品数据集成和利用提供了可扩展路径。

Abstract: The rapid expansion of e-commerce platforms generates vast amounts of unstructured product data, creating significant challenges for information retrieval, recommendation systems, and data analytics. Knowledge Graphs (KGs) offer a structured, interpretable format to organize such data, yet constructing product-specific KGs remains a complex and manual process. This paper introduces a fully automated, AI agent-driven framework for constructing product knowledge graphs directly from unstructured product descriptions. Leveraging Large Language Models (LLMs), our method operates in three stages using dedicated agents: ontology creation and expansion, ontology refinement, and knowledge graph population. This agent-based approach ensures semantic coherence, scalability, and high-quality output without relying on predefined schemas or handcrafted extraction rules. We evaluate the system on a real-world dataset of air conditioner product descriptions, demonstrating strong performance in both ontology generation and KG population. The framework achieves over 97\% property coverage and minimal redundancy, validating its effectiveness and practical applicability. Our work highlights the potential of LLMs to automate structured knowledge extraction in retail, providing a scalable path toward intelligent product data integration and utilization.

</details>


### [22] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 提出了一种新的不完全方法来打破抽象结构的对称性，通过更好地利用其表示来处理不可区分对象产生的对称性，比现有方法更高效。


<details>
  <summary>Details</summary>
Motivation: 在约束编程中，抽象结构（如嵌套集合）需要转换为求解器支持的表示形式。对称性破坏技术可以显著加速求解过程，但应用于抽象变量时会产生大量复杂约束，实际性能较差。

Method: 开发了一种新的不完全对称性破坏方法，通过更好地利用抽象结构的表示形式来打破对称性，特别针对不可区分对象产生的对称性。

Result: 该方法比Akgün等人2025年提出的先前方法更快。

Conclusion: 新方法通过改进抽象结构的表示利用，有效解决了对称性破坏中的性能问题，为约束编程中的抽象结构处理提供了更高效的解决方案。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [23] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 研究发现角色分配策略对多智能体辩论性能有显著影响，提出了"Truth Last"策略可提升22%推理性能，并开发了MADC策略来解决实际应用中未知真相的问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论在提升LLM推理能力方面有潜力，但角色分配策略这一关键方面尚未充分探索，特别是在实际应用中真相未知的情况。

Method: 提出"Truth Last"角色分配策略，并开发了MADC策略，通过路径一致性评估独立角色间的一致性，模拟最高一致性得分的角色作为真相。

Result: 在9个LLM模型上的验证显示，MADC策略持续表现出先进性能，有效克服了MAD的性能瓶颈，推理任务性能提升达22%。

Conclusion: MADC策略为LLM智能体扩展提供了关键改进路径，通过系统化模拟和优化多智能体辩论的核心机制，显著提升了推理能力。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [24] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 提出了DSS框架，利用可微分模拟器Waymax作为状态预测器和评估器，通过梯度下降优化动作序列，显著提升了自动驾驶的跟踪和路径规划精度。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，规划对于避免碰撞和在复杂密集交通场景中导航至关重要。传统搜索方法在学习策略、状态预测器和评估器时面临挑战。

Method: 使用可微分模拟器Waymax作为状态预测器和评估器，利用其硬编码动力学实现准确状态预测，通过可微分特性在动作序列上进行有效搜索，使用梯度下降优化想象轨迹中的动作。

Result: DSS（规划梯度和随机搜索的组合）相比序列预测、模仿学习、无模型RL和其他规划方法，显著提高了跟踪和路径规划精度。

Conclusion: DSS框架通过结合规划梯度和随机搜索，为自动驾驶规划提供了有效的解决方案，优于现有方法。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [25] [ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving](https://arxiv.org/abs/2511.11079)
*Sejin Kim,Hayan Choi,Seokki Lee,Sundong Kim*

Main category: cs.AI

TL;DR: ARCTraj是一个用于建模人类抽象推理过程的数据集和方法框架，通过记录人类在解决ARC视觉任务时的时序动作轨迹，揭示中间推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有ARC研究方法主要依赖静态输入-输出监督，无法洞察推理的时间展开过程，限制了我们对人类推理机制的理解。

Method: 通过O2ARC网络接口收集约10,000条带时间戳的对象级动作轨迹，涵盖400个训练任务，并构建包含数据收集、动作抽象、MDP建模的统一推理流程。

Result: 创建了包含任务标识、时间戳和成功标签的轨迹数据集，分析了空间选择、颜色归因和策略收敛等人类推理结构特征。

Conclusion: ARCTraj为研究类人推理提供了结构化、可解释的基础，推动了可解释性、对齐和通用智能的发展。

Abstract: We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.

</details>


### [26] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 提出了一种新的广义规划方法，通过从训练问题中学习条件-动作规则来生成可解决相关规划问题族的程序。


<details>
  <summary>Details</summary>
Motivation: 广义规划旨在合成能够解决相关规划问题族的程序，现有方法在合成成本、规划覆盖率和解决方案质量方面存在改进空间。

Method: 对每个训练问题按顺序计算每个目标原子的最优计划，执行目标回归，并将输出提升为一阶条件-动作规则集合。

Result: 实验表明在经典和数值规划领域中，在合成成本、规划覆盖率和解决方案质量三个指标上显著优于最先进的广义规划器。

Conclusion: 该方法能够学习有效的广义规划和状态空间剪枝公理，为广义规划提供了一种简单而有效的新方法。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [27] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: 提出了GGBench基准，用于评估几何生成推理能力，填补了统一多模态模型在生成推理评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估判别性理解或无约束图像生成，无法衡量生成推理的整合认知过程，几何构造需要语言理解和精确视觉生成的融合，是理想的测试平台。

Method: 设计GGBench基准，通过几何构造任务系统性地诊断模型的理解、推理和主动构建解决方案的能力。

Result: 为下一代智能系统设定了更严格的标准，提供了评估几何生成推理的综合框架。

Conclusion: 几何构造是评估统一多模态模型生成推理能力的理想测试平台，GGBench填补了现有评估方法的空白。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [28] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 提出MUG协议，通过多模态反事实测试来检测幻觉代理，提升多模态推理的可靠性


<details>
  <summary>Details</summary>
Motivation: 解决多智能体辩论中所有代理都是理性且反思的不现实假设问题，特别是当代理本身容易产生幻觉时

Method: 引入多智能体卧底游戏协议，通过修改参考图像引入反事实证据，观察代理是否能准确识别这些变化

Result: MUG在三个关键维度上改进了MAD协议：实现超越统计共识的事实验证、引入跨证据推理、促进主动推理

Conclusion: MUG为LLMs中的多模态推理提供了一个更可靠和有效的框架

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [29] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR框架通过慢思考机制增强LLM的表格推理能力，使用两阶段强化学习和不确定性量化来提升推理深度和稳定性


<details>
  <summary>Details</summary>
Motivation: 现有LLM表格推理方法缺乏人类认知的深度迭代优化过程，且推理过程不稳定，影响下游应用可靠性

Method: 采用慢思考机制，通过两阶段难度感知强化学习从简单到复杂查询逐步学习，并在推理时进行轨迹级不确定性量化

Result: 在基准测试中取得优越性能，推理稳定性显著增强，在领域外数据集上表现出强泛化能力

Conclusion: STaR为LLM表格推理提供了可靠且认知启发的解决方案，具有实际应用潜力

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [30] [UAVBench: An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flight Scenarios](https://arxiv.org/abs/2511.11252)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.AI

TL;DR: UAVBench是一个用于评估自主空中系统语言模型推理能力的标准化基准数据集，包含5万个验证过的无人机飞行场景和5万个多选题，涵盖认知和伦理推理。


<details>
  <summary>Details</summary>
Motivation: 当前自主空中系统依赖大语言模型进行任务规划、感知和决策，但缺乏标准化和物理基础的基准来系统评估其推理能力。

Method: 通过分类引导的LLM提示和多阶段安全验证生成5万个无人机飞行场景，并创建包含10种推理风格的5万道多选题。

Result: 评估32个最先进LLM，发现在感知和政策推理方面表现强劲，但在伦理意识和资源受限决策方面仍存在挑战。

Conclusion: UAVBench为自主空中系统中的代理AI基准测试建立了可重现和物理基础的基础，推动下一代无人机推理智能的发展。

Abstract: Autonomous aerial systems increasingly rely on large language models (LLMs) for mission planning, perception, and decision-making, yet the lack of standardized and physically grounded benchmarks limits systematic evaluation of their reasoning capabilities. To address this gap, we introduce UAVBench, an open benchmark dataset comprising 50,000 validated UAV flight scenarios generated through taxonomy-guided LLM prompting and multi-stage safety validation. Each scenario is encoded in a structured JSON schema that includes mission objectives, vehicle configuration, environmental conditions, and quantitative risk labels, providing a unified representation of UAV operations across diverse domains. Building on this foundation, we present UAVBench_MCQ, a reasoning-oriented extension containing 50,000 multiple-choice questions spanning ten cognitive and ethical reasoning styles, ranging from aerodynamics and navigation to multi-agent coordination and integrated reasoning. This framework enables interpretable and machine-checkable assessment of UAV-specific cognition under realistic operational contexts. We evaluate 32 state-of-the-art LLMs, including GPT-5, ChatGPT-4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, and ERNIE 4.5 300B, and find strong performance in perception and policy reasoning but persistent challenges in ethics-aware and resource-constrained decision-making. UAVBench establishes a reproducible and physically grounded foundation for benchmarking agentic AI in autonomous aerial systems and advancing next-generation UAV reasoning intelligence. To support open science and reproducibility, we release the UAVBench dataset, the UAVBench_MCQ benchmark, evaluation scripts, and all related materials on GitHub at https://github.com/maferrag/UAVBench

</details>


### [31] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: AIonopedia是首个基于大语言模型的离子液体发现智能体，通过多模态领域基础模型实现准确性质预测和分层搜索架构，在真实湿实验验证中表现出色。


<details>
  <summary>Details</summary>
Motivation: 离子液体发现面临数据有限、模型精度差和工作流程碎片化等关键挑战，需要更高效的发现方法。

Method: 构建LLM增强的多模态离子液体领域基础模型，采用分层搜索架构进行分子筛选和设计，并在新构建的全面数据集上训练评估。

Result: 模型表现优异，在文献报道系统评估中能有效进行离子液体修饰，在真实湿实验验证中展现出对挑战性分布外任务的出色泛化能力。

Conclusion: AIonopedia能够加速真实世界的离子液体发现，具有实际应用价值。

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [32] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 本文提出了一种通过强制记录AI决策过程中所有组件来确保决策可追溯性的工作流程，扩展了DBOM概念并利用机密计算技术生成防篡改、可验证的完整AI决策痕迹。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在决策过程文档化方面存在不足，这阻碍了追溯决策原因和建立责任链的能力，特别是当AI决策违反法律时，缺乏能在法庭上站得住脚的文档记录。

Method: 采用DBOM概念扩展，利用机密计算技术构建工作流程，强制记录训练和推理过程中的每个组件，生成防篡改、可验证的决策痕迹。

Result: 开发了首个支持生成防篡改、可验证且详尽AI决策痕迹的运行工作流程，并通过蘑菇识别应用案例展示了其内部工作机制。

Conclusion: 该方法为解决AI决策可追溯性和责任归属问题提供了实用且根本的解决方案，能够为法律环境下的AI决策提供可靠的文档支持。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [33] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 提出对比性ABox解释概念，用于回答"为什么a是C的实例而b不是？"这类问题，同时考虑正蕴含和缺失蕴含，聚焦a和b之间的相关共性和差异。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能单独解释正蕴含（为什么C(a)被知识库蕴含）或缺失蕴含（为什么C(b)不被蕴含），缺乏同时考虑两者的对比性解释方法。

Method: 为描述逻辑本体论中的ABox推理开发了对比性解释的适当概念，分析了不同变体在不同最优性标准下的计算复杂性，涵盖轻量级和更具表达力的描述逻辑。

Result: 实现了计算对比性解释变体的首个方法，并在现实知识库的生成问题上进行了评估。

Conclusion: 对比性解释能够同时处理正蕴含和缺失蕴含，聚焦于实例间的相关共性和差异，为描述逻辑推理提供了更全面的解释能力。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [34] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign是一个推理时框架，将大型视觉语言模型对齐重新定义为经济理性搜索问题，通过前瞻性评估函数动态权衡安全性、效用和成本，在降低计算成本的同时实现更好的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前的对齐方法在安全性、实用性和运营成本之间存在权衡困难，仅关注最终输出的过程盲目性会浪费大量计算预算在不安全的推理上，有害推理可能通过良性理由伪装来规避简单安全评分。

Method: 将LVLM视为有限理性智能体，增量扩展思维图，使用前瞻性函数（类似净现值）对行动评分，动态权衡预期安全性、效用和成本与剩余预算，并通过最弱环节原则强制执行路径安全性。

Result: 在3个闭源和2个开源模型的6个数据集上进行的广泛实验表明，EcoAlign以更低的计算成本匹配或超越了最先进的安全性和实用性。

Conclusion: EcoAlign为稳健的LVLM对齐提供了一个原则性、经济性的路径，解决了安全性与效率之间的根本权衡问题。

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [35] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: RLSLM是一个结合强化学习和基于规则的社会运动模型的混合框架，通过将心理学实验验证的社会舒适度场整合到奖励函数中，实现既高效又符合人类直觉的社会导航。


<details>
  <summary>Details</summary>
Motivation: 解决社会感知智能体在人群环境中导航时，现有基于规则方法缺乏泛化性而数据驱动方法缺乏可解释性的问题，需要一种既能保持人类直觉对齐又能高效学习的方法。

Method: 提出RLSLM混合强化学习框架，将基于经验行为实验的社会运动模型整合到奖励函数中，生成方向敏感的社会舒适度场，联合优化机械能和社会舒适度。

Result: 在沉浸式VR实验中，RLSLM在用户体验方面优于最先进的基于规则模型，消融和敏感性分析显示其相比传统数据驱动方法显著提高了可解释性。

Conclusion: 这项工作提出了一个可扩展的、以人为中心的方法论，有效整合认知科学和机器学习，用于现实世界的社会导航应用。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [36] [KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics](https://arxiv.org/abs/2511.11357)
*Haixin Li,Yanke Li,Diego Paez-Granados*

Main category: cs.AI

TL;DR: KarmaTS是一个交互式框架，用于构建具有滞后索引的可执行时空因果图模型，用于多变量时间序列模拟。


<details>
  <summary>Details</summary>
Motivation: 解决生理数据访问受限的挑战，生成具有已知因果动态的合成多变量时间序列，并用专家知识增强真实世界数据集。

Method: 通过混合主动、人在回路的工作流程，结合专家知识和算法建议构建离散时间结构因果过程（DSCP）。

Result: 生成的DSCP支持模拟和因果干预，包括用户指定的分布偏移下的干预，能够处理混合变量类型、同时和滞后边，以及模块化边函数。

Conclusion: 这些特性使得能够通过专家知情的模拟灵活验证和基准测试因果发现算法。

Abstract: We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.

</details>


### [37] [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)
*Shulin Liu,Dong Du,Tao Yang,Yang Li,Boyu Qiu*

Main category: cs.AI

TL;DR: MarsRL是一个新颖的强化学习框架，通过代理管道并行性联合优化多代理推理系统中的所有代理，显著提升了开源模型在复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在单次推理过程中的输出长度限制了推理深度，而多代理推理系统虽然有效但难以在开源模型中推广，因为开源模型缺乏足够的批评和修正能力。

Method: 提出MarsRL框架，采用代理特定的奖励机制来减轻奖励噪声，并使用管道式训练来处理长轨迹，联合优化系统中的所有代理。

Result: 在Qwen3-30B-A3B-Thinking-2507上应用MarsRL，将AIME2025准确率从86.5%提升到93.3%，BeyondAIME从64.9%提升到73.8%，甚至超过了Qwen3-235B-A22B-Thinking-2507。

Conclusion: MarsRL展示了在多代理推理系统中推进强化学习的潜力，能够扩展其在不同推理任务中的适用性。

Abstract: Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.

</details>


### [38] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 本调查系统回顾了在多智能体强化学习中，面对现实通信约束（如消息扰动、传输延迟和有限带宽）时的鲁棒高效通信策略的最新进展。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法大多假设通信是瞬时、可靠且带宽无限的，但这些条件在现实部署中很少满足。需要研究在真实约束下的通信策略。

Method: 系统综述了在消息扰动、传输延迟和有限带宽等现实约束下的MARL通信策略，重点关注三个应用领域：协同自动驾驶、分布式SLAM和联邦学习。

Result: 识别了低延迟可靠性、带宽密集型数据共享和通信隐私权衡等核心挑战，这些对实际MARL系统至关重要。

Conclusion: 提出了统一的方法论，主张共同设计通信、学习和鲁棒性，以弥合理论MARL模型与实际实现之间的差距，并指出了关键开放挑战和未来研究方向。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [39] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet是一个多模态模型，整合了非结构化临床笔记、实验室测试和患者时间序列数据，使用LLM处理临床文本和实验室测试，使用transformer编码器处理纵向就诊序列，在慢性病预测中达到94%以上准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大多数预测模型未能充分捕捉多模态EHR数据中的交互、冗余和时间模式，通常只关注单一数据类型或忽略这些复杂性，而医生需要综合多模态和时间序列数据来形成全面的患者健康视图。

Method: 使用大型语言模型处理临床文本和实验室测试，使用transformer编码器处理纵向就诊序列，整合非结构化临床笔记、实验室测试和患者时间序列数据。

Result: 在MIMIC-III和FEMH数据集上，CURENet在多标签框架中预测前10种慢性病的准确率超过94%。

Conclusion: 多模态EHR整合有潜力增强临床决策制定并改善患者预后。

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [40] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一个动态生成定制化策略的AI系统，能够在推理时基于累积经验调整所有策略组件（提示、采样参数、工具配置和控制逻辑），在多个挑战性基准测试中显著提升性能并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在推理时只能通过修改文本输入来引导，无法灵活调整采样参数、移除工具、修改系统提示或在代理和工作流范式间切换，而更灵活的系统需要离线优化且部署后保持静态。

Method: 使用基于LLM的元策略（输出策略的策略），通过Guide组件基于当前问题和结构化记忆生成候选策略，Consolidator组件整合执行反馈来改进未来策略生成。

Result: 在五个挑战性基准测试（AIME 2025、3-SAT和三个Big Bench Extra Hard任务）中，EGuR比最强基线准确率提升高达14%，计算成本降低高达111倍，且随着经验积累两个指标持续改善。

Conclusion: EGuR通过动态生成完整计算程序实现了AI系统在推理时的灵活适应，显著提升了问题解决能力并大幅降低了资源消耗。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


### [41] [Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping](https://arxiv.org/abs/2511.11551)
*Dena Mujtaba,Brian Hu,Anthony Hoogs,Arslan Basharat*

Main category: cs.AI

TL;DR: 提出了一种基于模型引导策略塑造的测试时对齐技术，能够在复杂动态环境中精确控制个体行为属性，在不重新训练智能体的情况下实现伦理对齐与奖励最大化之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 决策AI智能体在复杂动态环境中运行时，面临着保持与人类价值观或指导方针对齐的关键挑战。仅训练实现目标的智能体可能采取有害行为，暴露出奖励最大化与保持对齐之间的关键权衡。对于预训练智能体，确保对齐尤其具有挑战性，因为重新训练成本高昂且过程缓慢。

Method: 基于模型引导策略塑造的测试时对齐技术。首先在各自游戏中训练RL智能体以最大化奖励，然后在测试时通过场景-动作属性分类器应用策略塑造，确保决策与伦理属性对齐。

Result: 在包含134个基于文本的游戏环境和数千个涉及伦理决策的标注场景的MACHIAVELLI基准测试中，测试时策略塑造提供了跨不同环境和对齐属性减轻不道德行为的有效且可扩展的解决方案。

Conclusion: 测试时策略塑造为缓解跨不同环境和伦理属性的不道德行为提供了有效且可扩展的解决方案，无需智能体重新训练即可实现伦理对齐与奖励最大化之间的原则性权衡。

Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices](https://arxiv.org/abs/2511.10680)
*Jean-Philippe Lignier*

Main category: cs.LG

TL;DR: LAD-BNet是一种专为边缘设备优化的实时能耗预测神经网络，在Google Coral TPU上实现快速推理，相比传统方法在精度和速度上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决智能电网和智能建筑中边缘设备实时能耗预测的挑战，需要兼顾预测精度和计算效率。

Method: 提出LAD-BNet（滞后感知双分支网络），结合显式利用时间滞后的分支和具有扩张卷积的时间卷积网络（TCN），同时捕捉短期和长期依赖关系。

Result: 在10分钟分辨率的真实能耗数据上，LAD-BNet在1小时预测范围内达到14.49% MAPE，Edge TPU推理时间仅18ms，比CPU加速8-12倍，内存占用180MB。

Conclusion: 该模型为实时能源优化、需求管理和运营规划等工业应用铺平了道路，在嵌入式设备约束下实现了精度和效率的平衡。

Abstract: Real-time energy forecasting on edge devices represents a major challenge for smart grid optimization and intelligent buildings. We present LAD-BNet (Lag-Aware Dual-Branch Network), an innovative neural architecture optimized for edge inference with Google Coral TPU. Our hybrid approach combines a branch dedicated to explicit exploitation of temporal lags with a Temporal Convolutional Network (TCN) featuring dilated convolutions, enabling simultaneous capture of short and long-term dependencies. Tested on real energy consumption data with 10-minute temporal resolution, LAD-BNet achieves 14.49% MAPE at 1-hour horizon with only 18ms inference time on Edge TPU, representing an 8-12 x acceleration compared to CPU. The multi-scale architecture enables predictions up to 12 hours with controlled performance degradation. Our model demonstrates a 2.39% improvement over LSTM baselines and 3.04% over pure TCN architectures, while maintaining a 180MB memory footprint suitable for embedded device constraints. These results pave the way for industrial applications in real-time energy optimization, demand management, and operational planning.

</details>


### [43] [LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups](https://arxiv.org/abs/2511.10683)
*Masih Aminbeidokhti,Subhankar Roy,Eric Granger,Elisa Ricci,Marco Pedersoli*

Main category: cs.LG

TL;DR: 提出LT-Soups方法解决长尾数据集中PEFT方法在提升尾部类别性能时牺牲头部类别准确率的问题，通过两阶段模型融合框架实现更好的性能平衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集通常呈现长尾分布，现有PEFT方法虽然能保持尾部类别性能，但会牺牲头部类别准确率。研究发现头尾类别比例是影响这种权衡的关键因素。

Method: 提出LT-Soups两阶段框架：第一阶段在平衡子集上微调模型并平均以减少头部类别偏置；第二阶段仅在完整数据集上微调分类器以恢复头部类别准确率。

Result: 在六个基准数据集上的实验表明，LT-Soups相比PEFT和传统模型融合方法，在各种不平衡情况下都能实现更优的性能权衡。

Conclusion: LT-Soups通过创新的两阶段模型融合策略，有效解决了长尾数据集中PEFT方法的局限性，在不同不平衡情况下都能实现更好的性能平衡。

Abstract: Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($ρ$) and head-tail ratio ($η$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.

</details>


### [44] [Differentiable Sparse Identification of Lagrangian Dynamics](https://arxiv.org/abs/2511.10706)
*Zitong Zhang,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种可微分稀疏识别框架，通过三次B样条逼近、鲁棒方程发现机制和递归导数计算方案，解决了拉格朗日系统识别中的噪声敏感性和数据限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏回归技术在识别有理函数和处理复杂机械系统中的噪声敏感性方面存在困难，而现有的拉格朗日识别方法受测量噪声和数据可用性的显著影响。

Method: 集成三次B样条逼近到拉格朗日系统识别中，开发鲁棒方程发现机制，并基于B样条基函数实现递归导数计算方案。

Result: 该方法在复杂机械系统中表现出优越性能，相比基线方法能够从噪声数据中更准确可靠地提取物理规律。

Conclusion: 所提出的框架解决了拉格朗日系统识别中的关键限制，为从噪声数据中发现物理规律提供了更有效的解决方案。

Abstract: Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.

</details>


### [45] [Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning](https://arxiv.org/abs/2511.10707)
*Sirui Liang,Pengfei Cao,Jian Zhao,Cong Huang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: BREP ReFT通过截断训练数据优化推理前缀生成、干预早期推理阶段防止错误累积、约束干预向量幅度避免干扰数值编码，显著提升了ReFT在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: ReFT方法虽然在多个任务上优于PEFT，但在数学推理任务上表现显著下降，主要原因是无法生成有效的推理前缀以及干扰数值编码导致错误累积。

Method: 提出BREP ReFT方法：1）截断训练数据优化初始推理前缀生成；2）干预早期推理阶段防止错误累积；3）约束干预向量幅度避免干扰数值编码。

Result: 在多种模型架构上的广泛实验表明，BREP在数学推理任务上优于标准ReFT和基于权重的PEFT方法，展现出卓越的有效性、效率和鲁棒泛化能力。

Conclusion: BREP ReFT通过针对性地解决ReFT在数学推理中的局限性，显著提升了性能，证明了其在数学推理任务上的优越性。

Abstract: Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.

</details>


### [46] [Towards Uncertainty Quantification in Generative Model Learning](https://arxiv.org/abs/2511.10710)
*Giorgio Morales,Frederic Jurie,Jalal Fadili*

Main category: cs.LG

TL;DR: 本文提出了生成模型学习中不确定性量化的问题，讨论了使用集成方法的精确率-召回率曲线等研究方向，并在合成数据集上验证了聚合精确率-召回率曲线在捕捉模型近似不确定性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 生成模型在各个领域日益普及，但关于其可靠性的基本担忧仍然存在。当前评估方法主要关注学习分布与目标分布之间的接近程度，忽略了这些测量中固有的不确定性。

Method: 形式化生成模型学习中的不确定性量化问题，提出使用基于集成的精确率-召回率曲线等方法，并在合成数据集上进行初步实验验证。

Result: 在合成数据集上的实验表明，聚合精确率-召回率曲线能够有效捕捉模型近似不确定性，使基于不确定性特征对不同模型架构进行系统比较成为可能。

Conclusion: 生成模型学习中的不确定性量化是一个重要但研究不足的领域，需要开发新的评估方法来更好地理解和比较不同模型的不确定性特征。

Abstract: While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.

</details>


### [47] [Movement-Specific Analysis for FIM Score Classification Using Spatio-Temporal Deep Learning](https://arxiv.org/abs/2511.10713)
*Jun Masaki,Ariaki Higashi,Naoko Shinagawa,Kazuhiko Hirata,Yuichi Kurita,Akira Furui*

Main category: cs.LG

TL;DR: 提出一种自动评估FIM评分的方法，使用不同于传统FIM评估动作的简单练习，通过深度学习模型估计FIM运动项目分数。


<details>
  <summary>Details</summary>
Motivation: 传统FIM评估对患者和医疗专业人员造成显著负担，需要开发自动化评估方法。

Method: 采用深度神经网络架构，整合空间-时间图卷积网络(ST-GCN)、双向长短期记忆网络(BiLSTM)和注意力机制，捕捉长期时间依赖关系并识别关键身体关节贡献。

Result: 在277名康复患者研究中，方法能有效区分完全独立患者和需要协助患者，在不同FIM项目上达到70.09-78.79%的平衡准确率，并识别出特定运动模式作为可靠预测因子。

Conclusion: 该方法为自动化FIM评估提供了可行方案，减轻了传统评估的负担，同时保持了良好的区分能力。

Abstract: The functional independence measure (FIM) is widely used to evaluate patients' physical independence in activities of daily living. However, traditional FIM assessment imposes a significant burden on both patients and healthcare professionals. To address this challenge, we propose an automated FIM score estimation method that utilizes simple exercises different from the designated FIM assessment actions. Our approach employs a deep neural network architecture integrating a spatial-temporal graph convolutional network (ST-GCN), bidirectional long short-term memory (BiLSTM), and an attention mechanism to estimate FIM motor item scores. The model effectively captures long-term temporal dependencies and identifies key body-joint contributions through learned attention weights. We evaluated our method in a study of 277 rehabilitation patients, focusing on FIM transfer and locomotion items. Our approach successfully distinguishes between completely independent patients and those requiring assistance, achieving balanced accuracies of 70.09-78.79 % across different FIM items. Additionally, our analysis reveals specific movement patterns that serve as reliable predictors for particular FIM evaluation items.

</details>


### [48] [Fast Neural Tangent Kernel Alignment, Norm and Effective Rank via Trace Estimation](https://arxiv.org/abs/2511.10796)
*James Hazelden*

Main category: cs.LG

TL;DR: 提出了一种基于迹估计的矩阵自由方法，用于快速计算神经正切核(NTK)的迹、Frobenius范数、有效秩和对齐度，相比传统方法可提高多个数量级的计算速度。


<details>
  <summary>Details</summary>
Motivation: 计算完整的NTK矩阵通常不可行，特别是对于循环架构。需要一种快速分析有限宽度经验NTK的方法。

Method: 使用Hutch++迹估计器进行矩阵自由分析，并提出了仅需前向或反向自动微分的单边估计器，特别适用于模型状态与参数数量差距较大的情况。

Result: 矩阵自由随机化方法可将计算速度提高多个数量级，单边估计器在低样本情况下表现优于Hutch++。

Conclusion: 矩阵自由随机化方法能够显著加速NTK的分析和应用，为大规模神经网络的理论研究提供了实用工具。

Abstract: The Neural Tangent Kernel (NTK) characterizes how a model's state evolves over Gradient Descent. Computing the full NTK matrix is often infeasible, especially for recurrent architectures. Here, we introduce a matrix-free perspective, using trace estimation to rapidly analyze the empirical, finite-width NTK. This enables fast computation of the NTK's trace, Frobenius norm, effective rank, and alignment. We provide numerical recipes based on the Hutch++ trace estimator with provably fast convergence guarantees. In addition, we show that, due to the structure of the NTK, one can compute the trace using only forward- or reverse-mode automatic differentiation, not requiring both modes. We show these so-called one-sided estimators can outperform Hutch++ in the low-sample regime, especially when the gap between the model state and parameter count is large. In total, our results demonstrate that matrix-free randomized approaches can yield speedups of many orders of magnitude, leading to faster analysis and applications of the NTK.

</details>


### [49] [Near-optimal Linear Predictive Clustering in Non-separable Spaces via Mixed Integer Programming and Quadratic Pseudo-Boolean Reductions](https://arxiv.org/abs/2511.10809)
*Jiazhou Liang,Hassan Khurram,Scott Sanner*

Main category: cs.LG

TL;DR: 提出了两种新的线性预测聚类（LPC）全局优化方法，通过利用可分离性的理论特性，推导出具有可证明误差界的近似最优解，显著提高了MIP公式的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有贪婪优化方法缺乏全局最优性，在非可分集群设置中表现不佳；而Bertsimas和Shioda的MIP方法虽然能保证全局最优性，但可扩展性差。

Method: 1. 基于可分离性理论特性推导近似最优解，降低MIP公式复杂度；2. 将LPC近似为二次伪布尔优化（QPBO）问题。

Result: 在合成和真实数据集上的比较分析表明，新方法始终获得近似最优解，回归误差显著低于贪婪优化，同时比现有MIP公式具有更好的可扩展性。

Conclusion: 提出的两种方法在保持近似最优性的同时，显著提高了LPC全局优化的计算效率，解决了现有方法的可扩展性问题。

Abstract: Linear Predictive Clustering (LPC) partitions samples based on shared linear relationships between feature and target variables, with numerous applications including marketing, medicine, and education. Greedy optimization methods, commonly used for LPC, alternate between clustering and linear regression but lack global optimality. While effective for separable clusters, they struggle in non-separable settings where clusters overlap in feature space. In an alternative constrained optimization paradigm, Bertsimas and Shioda (2007) formulated LPC as a Mixed-Integer Program (MIP), ensuring global optimality regardless of separability but suffering from poor scalability. This work builds on the constrained optimization paradigm to introduce two novel approaches that improve the efficiency of global optimization for LPC. By leveraging key theoretical properties of separability, we derive near-optimal approximations with provable error bounds, significantly reducing the MIP formulation's complexity and improving scalability. Additionally, we can further approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving substantial computational improvements in some settings. Comparative analyses on synthetic and real-world datasets demonstrate that our methods consistently achieve near-optimal solutions with substantially lower regression errors than greedy optimization while exhibiting superior scalability over existing MIP formulations.

</details>


### [50] [Transformers know more than they can tell -- Learning the Collatz sequence](https://arxiv.org/abs/2511.10811)
*François Charton,Ashvni Narayanan*

Main category: cs.LG

TL;DR: 本文研究Transformer预测Collatz长步数的能力，发现模型准确率随编码基数变化（最高99.7%，最低25%），所有模型都学习相同的模式：按输入模2^p的余数分类学习，准确率接近完美。错误主要源于循环长度估计错误而非幻觉。


<details>
  <summary>Details</summary>
Motivation: 使用数学问题作为工具来理解、解释和可能改进语言模型，特别是研究Transformer学习复杂算术函数的能力。

Method: 使用不同基数编码输入输出，训练Transformer预测Collatz序列中的长步数，分析模型学习模式和错误类型。

Result: 模型准确率随基数变化显著（24和32基数达99.7%，11和3基数仅37%和25%），所有模型都学习相同的输入分类模式，错误主要源于循环长度估计错误（90%以上），几乎不发生幻觉。

Conclusion: 学习复杂算术函数的难点在于理解计算的控制结构（循环长度），使用数学问题作为分析工具的方法可广泛应用于其他问题并产生丰硕成果。

Abstract: We investigate transformer prediction of long Collatz steps, a complex arithmetic function that maps odd integers to their distant successors in the Collatz sequence ( $u_{n+1}=u_n/2$ if $u_n$ is even, $u_{n+1}=(3u_n+1)/2$ if $u_n$ is odd). Model accuracy varies with the base used to encode input and output. It can be as high as $99.7\%$ for bases $24$ and $32$, and as low as $37$ and $25\%$ for bases $11$ and $3$. Yet, all models, no matter the base, follow a common learning pattern. As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$. Models achieve near-perfect accuracy on these classes, and less than $1\%$ for all other inputs. This maps to a mathematical property of Collatz sequences: the length of the loops involved in the computation of a long Collatz step can be deduced from the binary representation of its input. The learning pattern reflects the model learning to predict inputs associated with increasing loop lengths. An analysis of failure cases reveals that almost all model errors follow predictable patterns. Hallucination, a common feature of large language models, almost never happens. In over $90\%$ of failures, the model performs the correct calculation, but wrongly estimates loop lengths. Our observations give a full account of the algorithms learned by the models. They suggest that the difficulty of learning such complex arithmetic function lies in figuring the control structure of the computation -- the length of the loops. We believe that the approach outlined here, using mathematical problems as tools for understanding, explaining, and perhaps improving language models, can be applied to a broad range of problems and bear fruitful results.

</details>


### [51] [Towards Universal Neural Operators through Multiphysics Pretraining](https://arxiv.org/abs/2511.10829)
*Mikhail Masliaev,Dmitry Gusarov,Ilya Markov,Alexander Hvatov*

Main category: cs.LG

TL;DR: 研究评估了基于Transformer的神经算子在迁移学习中的表现，证明其能够有效在不同PDE问题间传递知识。


<details>
  <summary>Details</summary>
Motivation: 尽管神经算子在数据驱动的物理模拟中广泛应用，但其训练计算成本高昂。下游学习通过预训练简化问题再微调复杂问题来解决此问题。

Method: 在更通用的迁移学习设置中研究基于Transformer的神经算子，评估其在多样化PDE问题中的表现，包括外推未见参数、引入新变量以及从多方程数据集迁移。

Result: 结果表明，先进的神经算子架构能够有效在PDE问题间传递知识。

Conclusion: 基于Transformer的神经算子在PDE问题的迁移学习中表现出色，为解决训练计算成本高的问题提供了有效途径。

Abstract: Although neural operators are widely used in data-driven physical simulations, their training remains computationally expensive. Recent advances address this issue via downstream learning, where a model pretrained on simpler problems is fine-tuned on more complex ones. In this research, we investigate transformer-based neural operators, which have previously been applied only to specific problems, in a more general transfer learning setting. We evaluate their performance across diverse PDE problems, including extrapolation to unseen parameters, incorporation of new variables, and transfer from multi-equation datasets. Our results demonstrate that advanced neural operator architectures can effectively transfer knowledge across PDE problems.

</details>


### [52] [A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates](https://arxiv.org/abs/2511.11211)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: 本文为Tsallis-INF多臂老虎机算法提供了一个简化的最佳世界保证推导，避免了共轭函数的使用，采用在线凸优化工具。


<details>
  <summary>Details</summary>
Motivation: 简化J. Zimmert和Y. Seldin提出的Tsallis-INF算法的证明过程，使其更加简洁易懂。

Method: 使用现代在线凸优化工具，避免共轭函数的使用，不优化边界常数以保持证明简洁。

Result: 成功推导出Tsallis-INF算法在随机和对抗性老虎机问题中的最佳世界保证。

Conclusion: 通过简化证明方法，为Tsallis-INF算法提供了一个更加优雅和易于理解的推导过程。

Abstract: In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.

</details>


### [53] [Benchmarking Quantum Kernels Across Diverse and Complex Data](https://arxiv.org/abs/2511.10831)
*Yuhan Jiang,Matthew Otten*

Main category: cs.LG

TL;DR: 论文提出了一个变分量子核框架，在8个高维真实世界数据集上验证了量子核方法相对于经典RBF核的性能优势。


<details>
  <summary>Details</summary>
Motivation: 当前量子核方法研究主要局限于低维或合成数据集，无法充分评估其在真实高维数据上的实际优势。

Method: 开发了资源高效的变分量子核框架，采用参数缩放技术加速收敛，并在8个挑战性真实数据集上进行全面基准测试。

Result: 经典模拟结果显示，所提出的量子核在性能上明显优于标准经典核（如RBF核）。

Conclusion: 适当设计的量子核可以作为多功能高性能工具，为现实世界机器学习中的量子增强应用奠定基础。

Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.

</details>


### [54] [SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?](https://arxiv.org/abs/2511.10833)
*Sanchit Kabra,Shobhnik Kriplani,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.LG

TL;DR: SurfaceBench是一个用于符号曲面发现的首个综合基准，包含183个任务，涵盖15种符号复杂度类别，支持显式、隐式和参数方程表示形式，并采用几何感知指标评估方程发现质量。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法依赖记忆公式或简化函数形式，现有基准主要关注标量函数、忽略领域背景，且使用脆弱的字符串匹配指标，无法捕捉科学等价性。

Method: 构建包含183个任务的基准数据集，涵盖15种符号复杂度类别，支持三种方程表示形式，包含真实方程、变量语义和合成三维数据，采用几何感知指标如Chamfer和Hausdorff距离进行评估。

Result: 实验表明，最先进的框架虽然在特定函数族上偶尔成功，但在跨表示类型和曲面复杂度方面难以泛化。

Conclusion: SurfaceBench建立了一个具有挑战性和诊断性的测试平台，将符号推理与几何重建相结合，为组合泛化、数据驱动的科学归纳和几何感知推理提供了原则性基准。

Abstract: Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench

</details>


### [55] [EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence](https://arxiv.org/abs/2511.10834)
*Ansel Kaplan Erol,Seungjun Lee,Divya Mahajan*

Main category: cs.LG

TL;DR: EarthSight是一个分布式卫星图像智能框架，通过多任务推理、地面站查询调度和动态过滤器排序，在带宽和功耗限制下实现低延迟图像分析。


<details>
  <summary>Details</summary>
Motivation: 传统卫星图像传输管道需要先下载所有图像再分析，导致数小时到数天的延迟。现有星上机器学习方案将每颗卫星视为独立节点，存在冗余推理和资源浪费问题。

Method: 1) 卫星上使用共享主干网络进行多任务推理；2) 地面站查询调度器聚合用户请求并分配计算预算；3) 动态过滤器排序整合模型选择性、精度和执行成本，早期拒绝低价值图像。

Result: 相比现有最佳基线，EarthSight将每张图像的平均计算时间减少1.9倍，将90%分位端到端延迟从51分钟降低到21分钟。

Conclusion: EarthSight通过结合地面站全局上下文和星上资源感知自适应决策，使卫星星座能够在严格的下行带宽和星上功耗预算内执行可扩展的低延迟图像分析。

Abstract: Low-latency delivery of satellite imagery is essential for time-critical applications such as disaster response, intelligence, and infrastructure monitoring. However, traditional pipelines rely on downlinking all captured images before analysis, introducing delays of hours to days due to restricted communication bandwidth. To address these bottlenecks, emerging systems perform onboard machine learning to prioritize which images to transmit. However, these solutions typically treat each satellite as an isolated compute node, limiting scalability and efficiency. Redundant inference across satellites and tasks further strains onboard power and compute costs, constraining mission scope and responsiveness. We present EarthSight, a distributed runtime framework that redefines satellite image intelligence as a distributed decision problem between orbit and ground. EarthSight introduces three core innovations: (1) multi-task inference on satellites using shared backbones to amortize computation across multiple vision tasks; (2) a ground-station query scheduler that aggregates user requests, predicts priorities, and assigns compute budgets to incoming imagery; and (3) dynamic filter ordering, which integrates model selectivity, accuracy, and execution cost to reject low-value images early and conserve resources. EarthSight leverages global context from ground stations and resource-aware adaptive decisions in orbit to enable constellations to perform scalable, low-latency image analysis within strict downlink bandwidth and onboard power budgets. Evaluations using a prior established satellite simulator show that EarthSight reduces average compute time per image by 1.9x and lowers 90th percentile end-to-end latency from first contact to delivery from 51 to 21 minutes compared to the state-of-the-art baseline.

</details>


### [56] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: 提出一个区分外在和内在幻觉的评估框架，并利用基于注意力的不确定性量化算法改进幻觉检测性能


<details>
  <summary>Details</summary>
Motivation: LLMs在安全关键领域部署时容易产生幻觉，现有检测方法计算成本高且未区分幻觉类型

Method: 使用基于注意力的不确定性量化算法，提出新的注意力聚合策略，建立区分外在和内在幻觉的评估框架

Result: 采样方法对检测外在幻觉有效但内在幻觉失败，注意力聚合方法更适合内在幻觉检测

Conclusion: 注意力是量化模型不确定性的丰富信号，为根据幻觉性质调整检测策略提供了新方向

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [57] [FlowPath: Learning Data-Driven Manifolds with Invertible Flows for Robust Irregularly-sampled Time Series Classification](https://arxiv.org/abs/2511.10841)
*YongKyung Oh,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: FlowPath提出了一种通过可逆神经流学习控制路径几何形状的新方法，用于从不规则采样的时间序列中建模连续时间动态，相比固定插值方案显著提升了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有神经控制微分方程的性能高度依赖于从离散观测构建的控制路径选择，固定插值方案强加了简化的几何假设，往往不能准确表示底层数据流形，特别是在高缺失率情况下。

Method: FlowPath通过可逆神经流学习控制路径的几何形状，构建连续且数据自适应的流形，通过可逆性约束强制信息保持和良好行为的变换，这与先前无约束可学习路径模型不同。

Result: 在18个基准数据集和真实世界案例研究中的实证评估表明，FlowPath在使用固定插值或不可逆架构的基线方法上，分类准确性实现了统计显著的持续改进。

Conclusion: 结果强调了不仅建模沿路径的动态，还要建模路径本身几何形状的重要性，为从不规则时间序列学习提供了稳健且可泛化的解决方案。

Abstract: Modeling continuous-time dynamics from sparse and irregularly-sampled time series remains a fundamental challenge. Neural controlled differential equations provide a principled framework for such tasks, yet their performance is highly sensitive to the choice of control path constructed from discrete observations. Existing methods commonly employ fixed interpolation schemes, which impose simplistic geometric assumptions that often misrepresent the underlying data manifold, particularly under high missingness. We propose FlowPath, a novel approach that learns the geometry of the control path via an invertible neural flow. Rather than merely connecting observations, FlowPath constructs a continuous and data-adaptive manifold, guided by invertibility constraints that enforce information-preserving and well-behaved transformations. This inductive bias distinguishes FlowPath from prior unconstrained learnable path models. Empirical evaluations on 18 benchmark datasets and a real-world case study demonstrate that FlowPath consistently achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures. These results highlight the importance of modeling not only the dynamics along the path but also the geometry of the path itself, offering a robust and generalizable solution for learning from irregular time series.

</details>


### [58] [Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning](https://arxiv.org/abs/2511.10843)
*Alexander W. Goodall,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出一种新的在线强化学习方法，通过精心设计的行为策略收集离线数据，以获得方差更低的回报估计，从而提高样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法依赖回报估计进行策略改进，但高方差的回报估计会导致样本效率低下和训练不稳定。研究发现收集离线数据可以显著降低回报估计的方差。

Method: 将离线策略评估的方差降低方法扩展到在线强化学习设置，在策略评估和改进交替进行时，使用专门设计的行为策略收集数据，以获得方差更低的回报估计。

Result: 在多种环境中，将两种策略梯度方法扩展为此机制后，表现出更好的样本效率和性能。

Conclusion: 在线强化学习中，精心设计的行为策略可以收集方差更低的离线数据，从而显著提高学习效率和性能。

Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.

</details>


### [59] [STAMP: Spatial-Temporal Adapter with Multi-Head Pooling](https://arxiv.org/abs/2511.10848)
*Brad Shook,Abby Turner,Jieshi Chen,Michał Wiliński,Mononito Goswami,Jonathan Elmer,Artur Dubrawski*

Main category: cs.LG

TL;DR: 提出了STAMP适配器，将通用时间序列基础模型应用于EEG数据，性能媲美专用EEG基础模型，且参数轻量、输入灵活。


<details>
  <summary>Details</summary>
Motivation: 缺乏对EEG专用基础模型与通用时间序列基础模型在EEG任务上的对比分析，需要探索通用模型在EEG领域的适用性。

Method: 设计STAMP适配器，利用通用TSFM的单变量嵌入，隐式建模EEG数据的时空特征，通过多头池化处理。

Result: 在8个EEG临床任务基准数据集上表现优异，性能与最先进的EEG专用基础模型相当，且参数更少。

Conclusion: STAMP证明了通用TSFM通过适配器可有效处理EEG数据，为EEG建模提供了轻量灵活的解决方案。

Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.

</details>


### [60] [ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries](https://arxiv.org/abs/2511.10855)
*Tom Yuviler,Dana Drachsler-Cohen*

Main category: cs.LG

TL;DR: ExPairT-LLM是一种用于代码选择的精确学习算法，通过向LLM提出两种新型查询（成对成员资格和成对等价性）来识别正确程序，在代码生成任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有代码选择算法存在两个问题：可能错误识别非等价程序，以及依赖LLM并假设其总能正确确定每个输入的输出。需要一种更稳健的方法来从LLM生成的多个程序中选择正确程序。

Method: 提出ExPairT-LLM算法，通过向LLM提出两种新型查询：成对成员资格查询和成对等价性查询。这些查询对LLM来说更简单，并通过锦标赛方式识别正确程序，对某些LLM错误具有鲁棒性。

Result: 在四个流行的代码数据集上评估，ExPairT-LLM的pass@1（成功率）平均比最先进的代码选择算法高出+13.0%，最高达+27.1%。还将执行复杂推理的LLM的pass@1提高了+24.0%。

Conclusion: ExPairT-LLM通过引入更简单的查询类型和锦标赛方法，有效解决了代码选择中的挑战，显著提升了代码生成的准确性和鲁棒性。

Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.

</details>


### [61] [Private Zeroth-Order Optimization with Public Data](https://arxiv.org/abs/2511.10859)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 提出了PAZO框架，利用公共数据指导私有零阶优化算法，在保持隐私的同时显著提升效用和运行效率


<details>
  <summary>Details</summary>
Motivation: 解决一阶差分隐私机器学习算法（如DP-SGD）的高计算和内存成本问题，同时提升零阶方法在隐私保护下的效用表现

Method: 利用公共数据指导私有零阶梯度近似，开发PAZO框架，在理论和实验上验证公共与私有数据相似性假设下的有效性

Result: 在视觉和文本任务的预训练和微调中，PAZO在隐私/效用权衡上优于最佳一阶基线方法，在高度隐私保护机制下表现尤其突出，同时提供高达16倍的运行加速

Conclusion: PAZO框架通过有效利用公共数据，显著提升了私有零阶优化算法的实用性和效率，为差分隐私机器学习提供了更优的解决方案

Abstract: One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.

</details>


### [62] [Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go](https://arxiv.org/abs/2511.10868)
*Yashshi Pipalani,Hritik Raj,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta*

Main category: cs.LG

TL;DR: GO UT Bench是一个包含5264对代码和单元测试的基准数据集，用于解决代码LLM在训练数据不平衡问题，特别是在Golang等低资源语言中。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLM的训练数据严重不平衡，过度代表开源代码而低估了更广泛的软件工程任务，导致模型擅长代码自动补全但在实际开发工作流（如单元测试生成）中表现不佳。

Method: 引入GO UT Bench基准数据集，包含来自10个许可宽松的Golang仓库的5264对代码和单元测试。在两个LLM家族（专家混合和密集解码器）上评估其作为微调数据集的有效性。

Result: 微调后的模型在超过75%的基准任务上优于其基础对应模型。

Conclusion: GO UT Bench作为微调数据集能有效提升代码LLM在单元测试生成等实际开发任务上的性能。

Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

</details>


### [63] [Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations](https://arxiv.org/abs/2511.10872)
*Shuyuan Zhang,Zihan Wang,Xiao-Wen Chang,Doina Precup*

Main category: cs.LG

TL;DR: 提出了G4RL方法，通过图编码器-解码器评估未见状态，解决图引导分层强化学习中的样本效率低和子目标表示差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图引导分层强化学习方法依赖领域知识构建图，或动态构建图但难以将图中信息传递给新访问状态，存在样本效率低和子目标表示差的问题。

Method: 开发图编码器-解码器评估未见状态，G4RL方法可集成到任何现有GCHRL方法中，在具有对称和可逆转换的环境中提升性能。

Result: 实验结果表明，利用图编码器-解码器产生的高层和低层内在奖励，显著提升了最先进GCHRL方法的性能，且额外计算成本很小。

Conclusion: G4RL方法有效解决了图引导分层强化学习中的关键挑战，在密集和稀疏奖励环境中都能显著提升性能。

Abstract: The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.

</details>


### [64] [Multi-Joint Physics-Informed Deep Learning Framework for Time-Efficient Inverse Dynamics](https://arxiv.org/abs/2511.10878)
*Shuhao Ma,Zeyi Huang,Yu Cao,Wesley Doorsamy,Chaoyang Shi,Jun Li,Zhi-Qiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种物理信息深度学习框架PI-MJCA-BiGRU，直接从运动学数据估计肌肉激活和力量，无需标注数据，实现高效推理和生理一致的预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算成本高且缺乏高质量多关节应用标注数据集，需要开发时间效率高且无需标注数据的肌肉激活和力量估计方法。

Method: 使用新颖的多关节交叉注意力模块和双向门控循环单元层捕捉关节间协调，通过将多关节动力学、关节间耦合和外部力交互嵌入损失函数实现物理信息学习。

Result: 在两个数据集上的实验验证表明，该方法性能与传统监督方法相当但无需真实标签，MJCA模块显著提升了关节间协调建模能力。

Conclusion: PI-MJCA-BiGRU框架能够提供生理一致的肌肉激活和力量估计，无需标注数据，在临床评估和辅助设备控制中具有应用潜力。

Abstract: Time-efficient estimation of muscle activations and forces across multi-joint systems is critical for clinical assessment and assistive device control. However, conventional approaches are computationally expensive and lack a high-quality labeled dataset for multi-joint applications. To address these challenges, we propose a physics-informed deep learning framework that estimates muscle activations and forces directly from kinematics. The framework employs a novel Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Unit (BiGRU) layers to capture inter-joint coordination, enabling each joint to adaptively integrate motion information from others. By embedding multi-joint dynamics, inter-joint coupling, and external force interactions into the loss function, our Physics-Informed MJCA-BiGRU (PI-MJCA-BiGRU) delivers physiologically consistent predictions without labeled data while enabling time-efficient inference. Experimental validation on two datasets demonstrates that PI-MJCA-BiGRU achieves performance comparable to conventional supervised methods without requiring ground-truth labels, while the MJCA module significantly enhances inter-joint coordination modeling compared to other baseline architectures.

</details>


### [65] [Multi-View Polymer Representations for the Open Polymer Prediction](https://arxiv.org/abs/2511.10893)
*Wonjin Jung,Yongseok Choi*

Main category: cs.LG

TL;DR: 提出了一种多视图聚合物性质预测方法，整合了四种互补表征，在NeurIPS 2025 Open Polymer Prediction Challenge中排名第9位。


<details>
  <summary>Details</summary>
Motivation: 利用互补的表征方法来提高聚合物性质预测的准确性，通过整合不同视角的信息来克服单一方法的局限性。

Method: 集成四种表征家族：RDKit/Morgan描述符、图神经网络、3D信息表征和预训练SMILES语言模型，采用均匀集成平均预测结果，使用10折交叉验证和SMILES测试时增强。

Result: 在2241个团队中排名第9位，公开测试集MAE为0.057，私有测试集MAE为0.082。

Conclusion: 多视图集成方法在聚合物性质预测中表现优异，证明了互补表征整合的有效性。

Abstract: We address polymer property prediction with a multi-view design that exploits complementary representations. Our system integrates four families: (i) tabular RDKit/Morgan descriptors, (ii) graph neural networks, (iii) 3D-informed representations, and (iv) pretrained SMILES language models, and averages per-property predictions via a uniform ensemble. Models are trained with 10-fold splits and evaluated with SMILES test-time augmentation. The approach ranks 9th of 2241 teams in the Open Polymer Prediction Challenge at NeurIPS 2025. The submitted ensemble achieves a public MAE of 0.057 and a private MAE of 0.082.

</details>


### [66] [Graph Attention Network for Predicting Duration of Large-Scale Power Outages Induced by Natural Disasters](https://arxiv.org/abs/2511.10898)
*Chenghao Duan,Chuanyi Ji*

Main category: cs.LG

TL;DR: 提出了一种基于图注意力网络（GAT）的新方法来预测恶劣天气导致的停电持续时间，通过无监督预训练和半监督学习，在四个飓风影响的数据集上取得了93%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 自然灾害导致的大规模停电造成了巨大的经济和社会影响，准确预测停电恢复时间对电网韧性至关重要。现有方法面临空间依赖性、空间异质性和事件数据有限三大挑战。

Method: 使用图注意力网络（GAT），采用无监督预训练和半监督学习框架，利用八个东南部州501个县的四次主要飓风现场数据。

Result: 模型表现出优异性能（>93%准确率），在整体性能和类别准确率上比XGBoost、随机森林、GCN和简单GAT方法高出2%-15%。

Conclusion: 提出的GAT方法能有效解决停电持续时间预测中的空间依赖性和异质性挑战，为电网韧性管理提供了可靠工具。

Abstract: Natural disasters such as hurricanes, wildfires, and winter storms have induced large-scale power outages in the U.S., resulting in tremendous economic and societal impacts. Accurately predicting power outage recovery and impact is key to resilience of power grid. Recent advances in machine learning offer viable frameworks for estimating power outage duration from geospatial and weather data. However, three major challenges are inherent to the task in a real world setting: spatial dependency of the data, spatial heterogeneity of the impact, and moderate event data. We propose a novel approach to estimate the duration of severe weather-induced power outages through Graph Attention Networks (GAT). Our network uses a simple structure from unsupervised pre-training, followed by semi-supervised learning. We use field data from four major hurricanes affecting $501$ counties in eight Southeastern U.S. states. The model exhibits an excellent performance ($>93\%$ accuracy) and outperforms the existing methods XGBoost, Random Forest, GCN and simple GAT by $2\% - 15\%$ in both the overall performance and class-wise accuracy.

</details>


### [67] [Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework](https://arxiv.org/abs/2511.10915)
*Guanxiong He,Jie Wang,Liaoyuan Tang,Zheng Wang,Rong Wang,Feiping Nie*

Main category: cs.LG

TL;DR: 提出了SPP-FGC算法，通过本地结构图作为隐私保护知识共享媒介，解决了联邦聚类中性能与隐私的权衡问题，实现了单轮通信的高效聚类。


<details>
  <summary>Details</summary>
Motivation: 解决联邦聚类中传统方法面临的困境：传输嵌入表示会泄露敏感数据，而仅共享抽象聚类原型会导致模型准确性下降。

Method: 基于客户端-服务器架构，客户端构建捕捉数据内在关系的私有结构图，服务器安全聚合和对齐形成全局图，从中推导统一聚类结构。提供SPP-FGC（单轮通信）和SPP-FGC+（迭代优化）两种模式。

Result: 在广泛实验中达到最先进性能，相比联邦基线将聚类准确率（NMI）提升高达10%，同时保持可证明的隐私保证。

Conclusion: SPP-FGC框架通过结构图作为隐私保护共享媒介，成功解决了联邦聚类中性能与隐私的权衡问题，为不同数据场景提供了灵活高效的解决方案。

Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.

</details>


### [68] [GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning](https://arxiv.org/abs/2511.10936)
*Ying Song,Balaji Palanisamy*

Main category: cs.LG

TL;DR: 提出了GraphToxin，首个针对图遗忘的图重构攻击方法，能够恢复被删除的敏感信息和个人链接，暴露了图遗忘技术的严重隐私风险。


<details>
  <summary>Details</summary>
Motivation: 图遗忘技术虽然能响应"被遗忘权"要求删除敏感信息，但存在漏洞：多方参与创造新的攻击面，删除数据的残留痕迹仍可能存在于遗忘后的图神经网络中，攻击者可利用这些漏洞恢复本应被删除的数据。

Method: 提出GraphToxin攻击方法，引入新颖的曲率匹配模块为完整遗忘图恢复提供细粒度指导，支持白盒和黑盒设置下的多节点移除攻击，并建立了包含随机和最坏情况节点移除的综合评估框架。

Result: GraphToxin能够成功恢复被删除个体的信息、个人链接及其连接的敏感内容，现有防御机制对此攻击基本无效，某些情况下甚至会放大攻击效果。

Conclusion: GraphToxin暴露了图遗忘技术的严重隐私风险，强调了开发更有效和鲁棒防御策略的紧迫性，需要进行最坏情况分析来系统评估图遗忘方法对图重构攻击的脆弱性。

Abstract: Graph unlearning has emerged as a promising solution for complying with "the right to be forgotten" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual traces of deleted data can still remain in the unlearned graph neural networks. These vulnerabilities can be exploited by attackers to recover the supposedly erased samples, thereby undermining the inherent functionality of graph unlearning. In this work, we propose GraphToxin, the first graph reconstruction attack against graph unlearning. Specifically, we introduce a novel curvature matching module to provide a fine-grained guidance for full unlearned graph recovery. We demonstrate that GraphToxin can successfully subvert the regulatory guarantees expected from graph unlearning - it can recover not only a deleted individual's information and personal links but also sensitive content from their connections, thereby posing substantially more detrimental threats. Furthermore, we extend GraphToxin to multiple node removals under both white-box and black-box setting. We highlight the necessity of a worst-case analysis and propose a comprehensive evaluation framework to systematically assess the attack performance under both random and worst-case node removals. This provides a more robust and realistic measure of the vulnerability of graph unlearning methods to graph reconstruction attacks. Our extensive experiments demonstrate the effectiveness and flexibility of GraphToxin. Notably, we show that existing defense mechanisms are largely ineffective against this attack and, in some cases, can even amplify its performance. Given the severe privacy risks posed by GraphToxin, our work underscores the urgent need for the development of more effective and robust defense strategies against this attack.

</details>


### [69] [Cascading Bandits With Feedback](https://arxiv.org/abs/2511.10938)
*R Sri Prakash,Nikhil Karamchandani,Sharayu Moharir*

Main category: cs.LG

TL;DR: 该论文研究了边缘推理中的级联赌博机模型，分析了四种决策策略的理论遗憾界限，发现LCB和Thompson Sampling因持续自适应而获得常数遗憾，优于固定排序策略。


<details>
  <summary>Details</summary>
Motivation: 受边缘推理挑战的启发，研究每个臂对应具有相关准确率和错误概率的推理模型的级联赌博机变体。

Method: 分析四种决策策略：Explore-then-Commit、Action Elimination、Lower Confidence Bound (LCB)和Thompson Sampling，并提供各自的理论遗憾保证。

Result: Explore-then-Commit和Action Elimination因探索后固定排序而遭受次优遗憾，而LCB和Thompson Sampling通过持续更新决策实现常数O(1)遗憾。模拟验证了理论发现。

Conclusion: 在不确定性下的高效边缘推理中，自适应能力起着关键作用，LCB和Thompson Sampling等持续自适应策略优于固定排序策略。

Abstract: Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.

</details>


### [70] [Flow matching-based generative models for MIMO channel estimation](https://arxiv.org/abs/2511.10941)
*Wenkai Liu,Nan Ma,Jianqiao Chen,Xiaoxuan Qi,Yuhang Ma*

Main category: cs.LG

TL;DR: 提出了一种基于流匹配的生成模型用于MIMO信道估计，通过构建从噪声信道分布到真实信道分布的直线轨迹条件概率路径，显著降低了采样开销并提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的信道估计方法虽然能获得高精度的信道状态信息，但采样速度慢是其关键挑战。

Method: 在流匹配框架下构建条件概率路径，推导仅依赖于噪声统计的矢量场来指导生成模型训练，在采样阶段使用训练好的矢量场作为先验信息，通过ODE欧拉求解器快速可靠地增强噪声信道。

Result: 数值结果表明，相比其他流行的基于扩散模型的方案，所提出的基于流匹配的信道估计方案能显著降低采样开销，并在不同信道条件下实现优越的信道估计精度。

Conclusion: 基于流匹配的信道估计方案在保持高精度的同时，有效解决了扩散模型采样速度慢的问题，为MIMO系统提供了高效可靠的信道估计方法。

Abstract: Diffusion model (DM)-based channel estimation, which generates channel samples via a posteriori sampling stepwise with denoising process, has shown potential in high-precision channel state information (CSI) acquisition. However, slow sampling speed is an essential challenge for recent developed DM-based schemes. To alleviate this problem, we propose a novel flow matching (FM)-based generative model for multiple-input multiple-output (MIMO) channel estimation. We first formulate the channel estimation problem within FM framework, where the conditional probability path is constructed from the noisy channel distribution to the true channel distribution. In this case, the path evolves along the straight-line trajectory at a constant speed. Then, guided by this, we derive the velocity field that depends solely on the noise statistics to guide generative models training. Furthermore, during the sampling phase, we utilize the trained velocity field as prior information for channel estimation, which allows for quick and reliable noise channel enhancement via ordinary differential equation (ODE) Euler solver. Finally, numerical results demonstrate that the proposed FM-based channel estimation scheme can significantly reduce the sampling overhead compared to other popular DM-based schemes, such as the score matching (SM)-based scheme. Meanwhile, it achieves superior channel estimation accuracy under different channel conditions.

</details>


### [71] [From Parameter to Representation: A Closed-Form Approach for Controllable Model Merging](https://arxiv.org/abs/2511.10943)
*Jialin Wu,Jian Yang,Handing Wang,Jiajun Wen,Zhiyong Yu*

Main category: cs.LG

TL;DR: 提出了一种新的可控模型合并方法，通过直接修正模型最终表示的线性变换来替代复杂的离线多目标优化，实现线性复杂度的实时Pareto最优模型生成。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法面临参数干扰问题，且现有可控合并方法采用编译-查询范式，需要昂贵的离线多目标优化，复杂度随任务数量指数增长。

Method: 将视角从参数空间优化转向直接修正模型最终表示，将修正建模为最优线性变换，得到闭式解，用单步架构无关计算替代整个离线优化过程。

Result: 实验结果显示该方法生成更优的Pareto前沿，具有更精确的偏好对齐和显著降低的计算成本。

Conclusion: 该方法通过直接表示修正实现了高效可控模型合并，复杂度线性增长，支持实时Pareto最优模型生成。

Abstract: Model merging combines expert models for multitask performance but faces challenges from parameter interference. This has sparked recent interest in controllable model merging, giving users the ability to explicitly balance performance trade-offs. Existing approaches employ a compile-then-query paradigm, performing a costly offline multi-objective optimization to enable fast, preference-aware model generation. This offline stage typically involves iterative search or dedicated training, with complexity that grows exponentially with the number of tasks. To overcome these limitations, we shift the perspective from parameter-space optimization to a direct correction of the model's final representation. Our approach models this correction as an optimal linear transformation, yielding a closed-form solution that replaces the entire offline optimization process with a single-step, architecture-agnostic computation. This solution directly incorporates user preferences, allowing a Pareto-optimal model to be generated on-the-fly with complexity that scales linearly with the number of tasks. Experimental results show our method generates a superior Pareto front with more precise preference alignment and drastically reduced computational cost.

</details>


### [72] [How Data Quality Affects Machine Learning Models for Credit Risk Assessment](https://arxiv.org/abs/2511.10964)
*Andrea Maurino*

Main category: cs.LG

TL;DR: 本文研究了数据质量问题（缺失值、噪声属性、异常值、标签错误）对信用风险评估机器学习模型预测准确性的影响，通过引入受控数据损坏评估了10种常用模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在信用风险评估中的应用日益增多，其有效性很大程度上取决于输入数据的质量。需要了解不同类型数据质量问题对模型性能的影响程度。

Method: 使用开源数据集，通过Pucktrick库引入受控数据损坏，评估包括随机森林、SVM和逻辑回归在内的10种常用模型的鲁棒性。

Result: 实验显示，基于数据退化的性质和严重程度，模型的鲁棒性存在显著差异。

Conclusion: 提出的方法和配套工具为从业者增强数据管道鲁棒性提供了实用支持，并为研究人员在数据为中心的AI背景下进行进一步实验提供了灵活框架。

Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.

</details>


### [73] [Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm](https://arxiv.org/abs/2511.11009)
*Fuxiang Huang,Xiaowei Fu,Shiyu Ye,Lina Ma,Wen Li,Xinbo Gao,David Zhang,Lei Zhang*

Main category: cs.LG

TL;DR: 本文提出了无监督鲁棒域自适应(URDA)范式，解决了传统域自适应方法在对抗攻击下缺乏鲁棒性的问题，并开发了DART算法来实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 传统无监督域自适应方法注重迁移能力但忽视对抗鲁棒性，而标准对抗训练在UDA中效果不佳。本文旨在解决UDA中的鲁棒性问题。

Method: 提出URDA范式，开发DART算法：先预训练任意UDA模型，然后通过解纠缠蒸馏进行瞬时鲁棒化后训练步骤。

Result: 在四个基准数据集上的实验表明，DART有效增强了鲁棒性同时保持了域适应性，验证了URDA范式和理论。

Conclusion: 本文首次建立了URDA范式和理论，提出的DART算法简单有效，能够同时保证迁移性和鲁棒性。

Abstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.

</details>


### [74] [Enhancing Graph Representations with Neighborhood-Contextualized Message-Passing](https://arxiv.org/abs/2511.11046)
*Brian Godwin Lim*

Main category: cs.LG

TL;DR: 提出了邻域上下文消息传递（NCMP）框架，通过考虑更广泛的邻域上下文信息来增强传统图神经网络的消息传递能力，并开发了SINC-GCN模型来实际应用这一框架。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递GNN仅考虑中心节点与单个邻居节点的特征对，忽略了更广泛的邻域上下文信息，这限制了模型学习复杂邻域关系的能力。

Method: 基于注意力变体的关键特性形式化邻域上下文概念，提出NCMP框架，并开发了SINC-GCN模型来参数化和操作化这一框架。

Result: 在合成二元节点分类问题上的初步分析表明，所提出的GNN架构具有更强的表达能力和效率。

Conclusion: NCMP框架为增强经典GNN的图表示能力提供了一条实用的路径。

Abstract: Graph neural networks (GNNs) have become an indispensable tool for analyzing relational data. In the literature, classical GNNs may be classified into three variants: convolutional, attentional, and message-passing. While the standard message-passing variant is highly expressive, its typical pair-wise messages nevertheless only consider the features of the center node and each neighboring node individually. This design fails to incorporate the rich contextual information contained within the broader local neighborhood, potentially hindering its ability to learn complex relationships within the entire set of neighboring nodes. To address this limitation, this work first formalizes the concept of neighborhood-contextualization, rooted in a key property of the attentional variant. This then serves as the foundation for generalizing the message-passing variant to the proposed neighborhood-contextualized message-passing (NCMP) framework. To demonstrate its utility, a simple, practical, and efficient method to parametrize and operationalize NCMP is presented, leading to the development of the proposed Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN). A preliminary analysis on a synthetic binary node classification problem then underscores both the expressivity and efficiency of the proposed GNN architecture. Overall, the paper lays the foundation for the novel NCMP framework as a practical path toward further enhancing the graph representational power of classical GNNs.

</details>


### [75] [Echoless Label-Based Pre-computation for Memory-Efficient Heterogeneous Graph Learning](https://arxiv.org/abs/2511.11081)
*Jun Hu,Shangheng Chen,Yufei He,Yuan Li,Bryan Hooi,Bingsheng He*

Main category: cs.LG

TL;DR: 提出Echoless-LP方法解决异质图神经网络中训练标签泄漏问题，通过分区聚焦无回声传播消除回声效应，保持内存效率并与各种消息传递方法兼容。


<details>
  <summary>Details</summary>
Motivation: 现有基于标签预计算的HGNNs存在训练标签泄漏问题（回声效应），即节点自身标签信息在多跳消息传递中传播回自身，现有缓解策略在大型图上内存效率低或与先进消息传递方法不兼容。

Method: 提出Echoless-LP方法，采用分区聚焦无回声传播(PFEP)，将目标节点分区，每个分区中的节点仅从其他分区的邻居收集标签信息；还引入非对称分区方案(APS)和PostAdjust机制处理分区带来的信息损失和分布偏移。

Result: 在公共数据集上的实验表明，Echoless-LP相比基线方法实现了更优越的性能，同时保持了内存效率。

Conclusion: Echoless-LP有效解决了异质图神经网络中的训练标签泄漏问题，提供了一种内存高效且与各种消息传递方法兼容的解决方案。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are widely used for deep learning on heterogeneous graphs. Typical end-to-end HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Pre-computation-based HGNNs address this by performing message passing only once during preprocessing, collecting neighbor information into regular-shaped tensors, which enables efficient mini-batch training. Label-based pre-computation methods collect neighbors' label information but suffer from training label leakage, where a node's own label information propagates back to itself during multi-hop message passing - the echo effect. Existing mitigation strategies are memory-inefficient on large graphs or suffer from compatibility issues with advanced message passing methods. We propose Echoless Label-based Pre-computation (Echoless-LP), which eliminates training label leakage with Partition-Focused Echoless Propagation (PFEP). PFEP partitions target nodes and performs echoless propagation, where nodes in each partition collect label information only from neighbors in other partitions, avoiding echo while remaining memory-efficient and compatible with any message passing method. We also introduce an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism to address information loss from partitioning and distributional shifts across partitions. Experiments on public datasets demonstrate that Echoless-LP achieves superior performance and maintains memory efficiency compared to baselines.

</details>


### [76] [Scalable Population Training for Zero-Shot Coordination](https://arxiv.org/abs/2511.11083)
*Bingyu Hui,Lebin Yu,Quanming Yao,Yunpeng Qu,Xudong Zhang,Jian Wang*

Main category: cs.LG

TL;DR: 提出ScaPT框架，通过元代理和互信息正则化器实现可扩展的群体训练，解决零样本协调中群体规模受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有零样本协调方法受计算资源限制，主要优化小群体多样性，忽视了扩大群体规模带来的性能提升潜力。

Method: ScaPT框架包含两个关键组件：通过选择性共享参数实现群体的元代理，以及保证群体多样性的互信息正则化器。

Result: 在Hanabi游戏中评估ScaPT，验证了其有效性并确认了优于代表性框架的优越性。

Conclusion: ScaPT为大规模群体训练提供了高效解决方案，在零样本协调任务中表现出色。

Abstract: Zero-shot coordination(ZSC) has become a hot topic in reinforcement learning research recently. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators that are not seen before without any fine-tuning. Population-based training has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi and confirms its superiority.

</details>


### [77] [Sheaf Cohomology of Linear Predictive Coding Networks](https://arxiv.org/abs/2511.11092)
*Jeffrey Seely*

Main category: cs.LG

TL;DR: 该论文将线性预测编码网络构建为胞腔层，通过层上同调分析不可消除的误差模式，并研究循环拓扑中反馈环导致的内部矛盾对学习的影响。


<details>
  <summary>Details</summary>
Motivation: 预测编码用局部优化替代全局反向传播，但循环拓扑中的反馈环会产生与监督无关的预测误差，影响学习效果，需要理论工具来分析这些问题。

Method: 将线性PC网络建模为胞腔层，使用层拉普拉斯算子进行扩散推理，通过层上同调和霍奇分解分析误差模式。

Result: 发现循环拓扑中的内部矛盾会导致学习停滞，层形式主义为识别问题网络配置和设计有效权重初始化提供了工具。

Conclusion: 胞腔层理论为分析预测编码网络提供了强大的数学框架，能够诊断问题配置并指导循环PC网络的权重初始化设计。

Abstract: Predictive coding (PC) replaces global backpropagation with local optimization over weights and activations. We show that linear PC networks admit a natural formulation as cellular sheaves: the sheaf coboundary maps activations to edge-wise prediction errors, and PC inference is diffusion under the sheaf Laplacian. Sheaf cohomology then characterizes irreducible error patterns that inference cannot remove. We analyze recurrent topologies where feedback loops create internal contradictions, introducing prediction errors unrelated to supervision. Using a Hodge decomposition, we determine when these contradictions cause learning to stall. The sheaf formalism provides both diagnostic tools for identifying problematic network configurations and design principles for effective weight initialization for recurrent PC networks.

</details>


### [78] [SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems](https://arxiv.org/abs/2511.11111)
*Xin Wang,Pietro Lodi Rizzini,Sourav Medya,Zhiling Lan*

Main category: cs.LG

TL;DR: 提出了一个结合图神经网络和大型语言模型的替代模型，用于预测Dragonfly网络中的应用程序运行时间，支持高效的混合仿真。


<details>
  <summary>Details</summary>
Motivation: Dragonfly网络在高性能计算中面临共享链路工作负载干扰的挑战，传统并行离散事件仿真计算成本高，不适用于大规模或实时场景。

Method: 开发了结合图神经网络和大型语言模型的替代模型，从端口级路由器数据中捕获空间和时间模式。

Result: 该模型在运行时间预测方面优于现有的统计和机器学习基线方法。

Conclusion: 该混合仿真方法能够准确预测运行时间，支持Dragonfly网络的高效仿真。

Abstract: The Dragonfly network, with its high-radix and low-diameter structure, is a leading interconnect in high-performance computing. A major challenge is workload interference on shared network links. Parallel discrete event simulation (PDES) is commonly used to analyze workload interference. However, high-fidelity PDES is computationally expensive, making it impractical for large-scale or real-time scenarios. Hybrid simulation that incorporates data-driven surrogate models offers a promising alternative, especially for forecasting application runtime, a task complicated by the dynamic behavior of network traffic. We present \ourmodel, a surrogate model that combines graph neural networks (GNNs) and large language models (LLMs) to capture both spatial and temporal patterns from port level router data. \ourmodel outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.

</details>


### [79] [Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization](https://arxiv.org/abs/2511.11118)
*Gerard Pons,Besim Bilalli,Anna Queralt*

Main category: cs.LG

TL;DR: 提出一种新的知识图谱嵌入初始化策略，利用图模式和已有嵌入为新实体生成初始表示，提高持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 知识图谱频繁更新，需要持续学习技术来适应变化。现有方法中新实体嵌入初始化对最终准确性和训练时间有重要影响，特别是对于小型频繁更新。

Method: 利用知识图谱模式和先前学习的嵌入，基于实体所属类别为新实体获取初始表示，可无缝集成到现有持续学习方法中。

Result: 实验分析表明该初始化策略提高了预测性能，增强了知识保留，加速了知识获取，减少了训练轮次和时间。

Conclusion: 该方法在不同类型的知识图谱嵌入学习模型中均表现出优势，能有效减少灾难性遗忘，提高学习效率。

Abstract: Many Knowledege Graphs (KGs) are frequently updated, forcing their Knowledge Graph Embeddings (KGEs) to adapt to these changes. To address this problem, continual learning techniques for KGEs incorporate embeddings for new entities while updating the old ones. One necessary step in these methods is the initialization of the embeddings, as an input to the KGE learning process, which can have an important impact in the accuracy of the final embeddings, as well as in the time required to train them. This is especially relevant for relatively small and frequent updates. We propose a novel informed embedding initialization strategy, which can be seamlessly integrated into existing continual learning methods for KGE, that enhances the acquisition of new knowledge while reducing catastrophic forgetting. Specifically, the KG schema and the previously learned embeddings are utilized to obtain initial representations for the new entities, based on the classes the entities belong to. Our extensive experimental analysis shows that the proposed initialization strategy improves the predictive performance of the resulting KGEs, while also enhancing knowledge retention. Furthermore, our approach accelerates knowledge acquisition, reducing the number of epochs, and therefore time, required to incrementally learn new embeddings. Finally, its benefits across various types of KGE learning models are demonstrated.

</details>


### [80] [Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods](https://arxiv.org/abs/2511.11143)
*Federico Maddanu,Tommaso Proietti,Riccardo Crupi*

Main category: cs.LG

TL;DR: 提出并评估了在中等和高维数据集上计算效率高的几种稳健方法，用于检测银行账户余额中的点异常。


<details>
  <summary>Details</summary>
Motivation: 检测银行账户余额中的点异常对金融机构至关重要，可以识别潜在欺诈、操作问题或其他异常情况。稳健统计有助于标记异常值并提供不受污染观测影响的数据分布参数估计，但在高维设置下通常效率较低且计算成本高。

Method: 提出并评估了几种具有高崩溃点和低计算时间的稳健方法，适用于中等和高维数据集。

Result: 应用处理了约260万条匿名用户银行账户余额的每日记录。

Conclusion: 所提出的方法在高维金融数据异常检测中具有计算效率和稳健性优势。

Abstract: Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.

</details>


### [81] [Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI](https://arxiv.org/abs/2511.11152)
*Tanmay Ghosh,Shaurabh Anand,Rakesh Gomaji Nannewar,Nithin Nagaraj*

Main category: cs.LG

TL;DR: 开发了一个可解释的深度学习框架用于印度四个主要城市的短期降水预测，结合CNN-ConvLSTM架构和可解释性分析，在保持准确性的同时提高透明度。


<details>
  <summary>Details</summary>
Motivation: 深度学习降水预测模型通常作为黑盒运行，限制了在实际天气预测中的应用。为了在保持准确性的同时增强透明度，需要开发可解释的深度学习框架。

Method: 采用混合时间分布CNN-ConvLSTM架构，使用多年代ERA5再分析数据进行训练。为每个城市优化卷积滤波器数量：班加罗尔(32)、孟买和德里(64)、加尔各答(128)。使用排列重要性、Grad-CAM、时间遮挡和反事实扰动进行可解释性分析。

Result: 模型在不同城市取得了不同的RMSE值：班加罗尔0.21毫米/天、孟买0.52毫米/天、德里0.48毫米/天、加尔各答1.80毫米/天。预测范围从班加罗尔的1天到加尔各答的5天不等。

Conclusion: 研究表明可解释AI可以在保持准确预测的同时，为不同城市环境中的降水模式提供透明洞察，模型依赖城市特定的变量进行预测。

Abstract: Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.

</details>


### [82] [Adaptive Symmetrization of the KL Divergence](https://arxiv.org/abs/2511.11159)
*Omri Ben-Dov,Luiz F. O. Chamon*

Main category: cs.LG

TL;DR: 提出了一种新的方法来最小化Jeffreys散度，通过使用代理模型来辅助优化主模型的Jeffreys散度，将联合训练任务表述为约束优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统的概率分布学习方法通常使用前向KL散度，但其不对称性可能无法捕捉目标分布的所有特性。对称替代方案如生成对抗网络涉及脆弱的min-max公式，而Jeffreys散度从样本中计算具有挑战性。

Method: 使用代理模型来拟合数据并辅助优化主模型的Jeffreys散度，将联合训练任务表述为约束优化问题，并在训练过程中自适应调整模型优先级。

Result: 开发了一个实用的算法框架，可以结合归一化流和能量基模型的优势，应用于密度估计、图像生成和基于模拟的推理等任务。

Conclusion: 该方法为最小化Jeffreys散度提供了一种新的实用方法，能够有效结合不同类型生成模型的优势。

Abstract: Many tasks in machine learning can be described as or reduced to learning a probability distribution given a finite set of samples. A common approach is to minimize a statistical divergence between the (empirical) data distribution and a parameterized distribution, e.g., a normalizing flow (NF) or an energy-based model (EBM). In this context, the forward KL divergence is a ubiquitous due to its tractability, though its asymmetry may prevent capturing some properties of the target distribution. Symmetric alternatives involve brittle min-max formulations and adversarial training (e.g., generative adversarial networks) or evaluating the reverse KL divergence, as is the case for the symmetric Jeffreys divergence, which is challenging to compute from samples. This work sets out to develop a new approach to minimize the Jeffreys divergence. To do so, it uses a proxy model whose goal is not only to fit the data, but also to assist in optimizing the Jeffreys divergence of the main model. This joint training task is formulated as a constrained optimization problem to obtain a practical algorithm that adapts the models priorities throughout training. We illustrate how this framework can be used to combine the advantages of NFs and EBMs in tasks such as density estimation, image generation, and simulation-based inference.

</details>


### [83] [Training Neural Networks at Any Scale](https://arxiv.org/abs/2511.11163)
*Thomas Pethick,Kimon Antonakopoulos,Antonio Silveti-Falls,Leena Chennuru Vankadara,Volkan Cevher*

Main category: cs.LG

TL;DR: 本文综述了现代神经网络训练优化方法，重点讨论效率和可扩展性，提出了统一的算法框架，并介绍了如何使算法适应不同规模的问题。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络规模的不断扩大，需要开发更高效的优化方法来处理大规模训练问题，同时为从业者和研究人员提供实用的指导。

Method: 提出统一的算法模板，强调适应问题结构的重要性，并介绍使算法对问题规模不敏感的技术。

Result: 系统性地整理了最先进的优化算法，建立了统一的框架来理解不同方法，并提供了适应不同规模问题的解决方案。

Conclusion: 本文为神经网络优化领域提供了全面的综述和统一的视角，有助于推动该领域的发展和应用。

Abstract: This article reviews modern optimization methods for training neural networks with an emphasis on efficiency and scale. We present state-of-the-art optimization algorithms under a unified algorithmic template that highlights the importance of adapting to the structures in the problem. We then cover how to make these algorithms agnostic to the scale of the problem. Our exposition is intended as an introduction for both practitioners and researchers who wish to be involved in these exciting new developments.

</details>


### [84] [Power Ensemble Aggregation for Improved Extreme Event AI Prediction](https://arxiv.org/abs/2511.11170)
*Julien Collard,Pierre Gentine,Tian Zheng*

Main category: cs.LG

TL;DR: 使用机器学习方法改进热浪等气候极端事件预测，通过将集合预测用幂平均聚合，显著提升分类器性能，在预测极端高温事件方面比传统平均预测更准确。


<details>
  <summary>Details</summary>
Motivation: 解决气候极端事件（特别是热浪）预测的关键挑战，提高预测准确性。

Method: 将问题构建为分类问题，预测地表气温是否会在指定时间内超过其局部q分位数；通过使机器学习天气预报模型具有生成性，并应用幂平均非线性聚合方法来聚合集合预测。

Result: 幂平均聚合方法显著提升了分类器性能，在预测极端高温事件方面比传统平均预测更准确，且对更高极端事件的预测效果更好。

Conclusion: 幂聚合方法显示出良好的前景和适应性，其最佳性能随所选分位数阈值而变化，在预测更高极端事件时效果更佳。

Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.

</details>


### [85] [On-line learning of dynamic systems: sparse regression meets Kalman filtering](https://arxiv.org/abs/2511.11178)
*Gianluigi Pillonetto,Akram Yazdani,Aleksandr Aravkin*

Main category: cs.LG

TL;DR: 本文提出了一种结合稀疏识别和卡尔曼滤波的新方法SKF，用于实时学习非线性动力系统的时变参数，并在混沌系统和真实飞机数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 从数据中学习控制方程对于理解物理系统行为至关重要，但现有方法难以实时处理时变非线性系统。

Method: 将Sindy算法与卡尔曼滤波结合，将未知系统参数作为状态变量处理，实现复杂时变非线性模型的实时推断。

Result: 在参数漂移或切换的混沌Lorenz系统和真实飞行数据构建的稀疏非线性飞机模型上验证了SKF的有效性。

Conclusion: SKF统一了稀疏识别和卡尔曼滤波框架，能够实时推断复杂时变非线性模型，并简化了稀疏水平、方差参数和切换时刻的估计。

Abstract: Learning governing equations from data is central to understanding the behavior of physical systems across diverse scientific disciplines, including physics, biology, and engineering. The Sindy algorithm has proven effective in leveraging sparsity to identify concise models of nonlinear dynamical systems. In this paper, we extend sparsity-driven approaches to real-time learning by integrating a cornerstone algorithm from control theory -- the Kalman filter (KF). The resulting Sindy Kalman Filter (SKF) unifies both frameworks by treating unknown system parameters as state variables, enabling real-time inference of complex, time-varying nonlinear models unattainable by either method alone. Furthermore, SKF enhances KF parameter identification strategies, particularly via look-ahead error, significantly simplifying the estimation of sparsity levels, variance parameters, and switching instants. We validate SKF on a chaotic Lorenz system with drifting or switching parameters and demonstrate its effectiveness in the real-time identification of a sparse nonlinear aircraft model built from real flight data.

</details>


### [86] [Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss](https://arxiv.org/abs/2511.11181)
*Zhenghao Zhang,Jun Xie,Xingchen Chen,Tao Yu,Hongzhu Yi,Kaixin Xu,Yuanxiang Wang,Tianyu Zong,Xinming Wang,Jiahuan Chen,Guoqing Chao,Feng Chen,Zhepeng Wang,Jungang Xu*

Main category: cs.LG

TL;DR: 提出了一种动态深度图学习方法DGIMVCM，用于不完整多视图聚类，通过掩码图重构损失解决现有方法的噪声问题


<details>
  <summary>Details</summary>
Motivation: 现实世界中多视图数据普遍存在，但不完整多视图聚类面临挑战：传统KNN构建静态图引入噪声，MSE损失导致梯度噪声

Method: 构建缺失鲁棒全局图，使用图卷积嵌入层提取特征和动态视图特定图结构，通过图结构对比学习识别一致性，引入图自注意力编码器，采用掩码图重构损失优化

Result: 在多个数据集上的广泛实验验证了DGIMVCM的有效性和优越性

Conclusion: DGIMVCM方法能够有效解决不完整多视图聚类中的噪声问题，提升聚类性能

Abstract: The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.

</details>


### [87] [LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag](https://arxiv.org/abs/2511.11190)
*Tianlang He,Zhongming Lin,Tianrui Jiang,S. -H. Gary Chan*

Main category: cs.LG

TL;DR: LoRaCompass是一种强化学习模型，用于在未知环境中高效定位LoRa标签，通过空间感知特征提取和策略蒸馏损失函数来应对信号波动和领域偏移，在80km²的多样化环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的LoRa标签定位方法容易受到领域偏移和信号波动的影响，导致级联决策错误和显著的定位不准确性。

Method: 提出LoRaCompass强化学习模型，包含空间感知特征提取器、策略蒸馏损失函数和基于上置信界的探索函数，从RSSI信号中学习稳健的空间表示。

Result: 在超过80km²的多样化未见环境中验证，定位成功率超过90%（比现有方法提高40%），搜索路径长度与初始距离呈线性关系。

Conclusion: LoRaCompass能够实现稳健高效的LoRa标签搜索，在领域偏移和信号波动下表现出色，为心智障碍人士等高风险人群的定位提供了可靠解决方案。

Abstract: The Long-Range (LoRa) protocol, known for its extensive range and low power, has increasingly been adopted in tags worn by mentally incapacitated persons (MIPs) and others at risk of going missing. We study the sequential decision-making process for a mobile sensor to locate a periodically broadcasting LoRa tag with the fewest moves (hops) in general, unknown environments, guided by the received signal strength indicator (RSSI). While existing methods leverage reinforcement learning for search, they remain vulnerable to domain shift and signal fluctuation, resulting in cascading decision errors that culminate in substantial localization inaccuracies. To bridge this gap, we propose LoRaCompass, a reinforcement learning model designed to achieve robust and efficient search for a LoRa tag. For exploitation under domain shift and signal fluctuation, LoRaCompass learns a robust spatial representation from RSSI to maximize the probability of moving closer to a tag, via a spatially-aware feature extractor and a policy distillation loss function. It further introduces an exploration function inspired by the upper confidence bound (UCB) that guides the sensor toward the tag with increasing confidence. We have validated LoRaCompass in ground-based and drone-assisted scenarios within diverse unseen environments covering an area of over 80km^2. It has demonstrated high success rate (>90%) in locating the tag within 100m proximity (a 40% improvement over existing methods) and high efficiency with a search path length (in hops) that scales linearly with the initial distance.

</details>


### [88] [When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping](https://arxiv.org/abs/2511.11208)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了一种基于零样本合成验证的联邦学习早期停止框架，利用生成式AI监控模型性能，在达到最优性能时自适应停止训练，节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通常预设固定训练轮数，导致在达到最优性能后仍继续不必要的计算，或模型无法取得有意义性能时仍持续训练，造成计算资源浪费。

Method: 使用生成式AI构建零样本合成验证框架，监控模型性能并确定早期停止点，自适应地在接近最优轮数时停止训练。

Result: 在多标签胸部X光分类任务中，该方法将训练轮数减少高达74%，同时准确率保持在最优值的1%以内。

Conclusion: 该方法能有效节省联邦学习的计算资源，并支持快速超参数调整，在保持模型性能的同时显著提高训练效率。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.

</details>


### [89] [Virtual Width Networks](https://arxiv.org/abs/2511.11238)
*Seed,Baisheng Li,Banggu Wu,Bole Ma,Bowen Xiao,Chaoyi Zhang,Cheng Li,Chengyi Wang,Chenyin Xu,Chi Zhang,Chong Hu,Daoguang Zan,Defa Zhu,Dongyu Xu,Du Li,Faming Wu,Fan Xia,Ge Zhang,Guang Shi,Haobin Chen,Hongyu Zhu,Hongzhi Huang,Huan Zhou,Huanzhang Dou,Jianhui Duan,Jianqiao Lu,Jianyu Jiang,Jiayi Xu,Jiecao Chen,Jin Chen,Jin Ma,Jing Su,Jingji Chen,Jun Wang,Jun Yuan,Juncai Liu,Jundong Zhou,Kai Hua,Kai Shen,Kai Xiang,Kaiyuan Chen,Kang Liu,Ke Shen,Liang Xiang,Lin Yan,Lishu Luo,Mengyao Zhang,Ming Ding,Mofan Zhang,Nianning Liang,Peng Li,Penghao Huang,Pengpeng Mu,Qi Huang,Qianli Ma,Qiyang Min,Qiying Yu,Renming Pang,Ru Zhang,Shen Yan,Shen Yan,Shixiong Zhao,Shuaishuai Cao,Shuang Wu,Siyan Chen,Siyu Li,Siyuan Qiao,Tao Sun,Tian Xin,Tiantian Fan,Ting Huang,Ting-Han Fan,Wei Jia,Wenqiang Zhang,Wenxuan Liu,Xiangzhong Wu,Xiaochen Zuo,Xiaoying Jia,Ximing Yang,Xin Liu,Xin Yu,Xingyan Bin,Xintong Hao,Xiongcai Luo,Xujing Li,Xun Zhou,Yanghua Peng,Yangrui Chen,Yi Lin,Yichong Leng,Yinghao Li,Yingshuan Song,Yiyuan Ma,Yong Shan,Yongan Xiang,Yonghui Wu,Yongtao Zhang,Yongzhen Yao,Yu Bao,Yuehang Yang,Yufeng Yuan,Yunshui Li,Yuqiao Xian,Yutao Zeng,Yuxuan Wang,Zehua Hong,Zehua Wang,Zengzhi Wang,Zeyu Yang,Zhengqiang Yin,Zhenyi Lu,Zhexi Zhang,Zhi Chen,Zhi Zhang,Zhiqi Lin,Zihao Huang,Zilin Xu,Ziyun Wei,Zuo Wang*

Main category: cs.LG

TL;DR: Virtual Width Networks (VWN) 通过解耦表示宽度和主干网络宽度，在几乎不增加计算成本的情况下扩展嵌入空间，实现了2-3倍的优化加速和损失降低。


<details>
  <summary>Details</summary>
Motivation: 传统增加隐藏层大小的方法会导致二次计算成本增长，VWN旨在获得更宽表示的好处而不产生这种高昂代价。

Method: VWN框架将表示宽度与主干网络宽度解耦，扩展嵌入空间同时保持主干计算几乎不变。

Result: 8倍扩展使下一个token预测优化加速2倍以上，下两个token预测加速3倍。随着训练进行，损失差距扩大且收敛加速比增加，虚拟宽度与损失减少呈近似对数线性缩放关系。

Conclusion: VWN不仅token高效，而且随着规模扩大效果更显著，虚拟宽度缩放可作为大型模型效率的新维度进行探索。

Abstract: We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.

</details>


### [90] [HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning](https://arxiv.org/abs/2511.11240)
*Yuhan Xie,Chen Lyu*

Main category: cs.LG

TL;DR: HealSplit是首个专为Split Federated Learning设计的统一防御框架，通过拓扑感知检测、生成式恢复和对抗性多教师蒸馏，有效防御五种复杂的数据投毒攻击。


<details>
  <summary>Details</summary>
Motivation: Split Federated Learning虽然是一种新兴的隐私保护分布式学习范式，但仍容易受到针对本地特征、标签、粉碎数据和模型权重的复杂数据投毒攻击。现有防御方法主要从传统联邦学习改编而来，在SFL中效果有限。

Method: HealSplit包含三个关键组件：1）拓扑感知检测模块，通过构建粉碎数据图并使用拓扑异常评分识别中毒样本；2）生成式恢复管道，为检测到的异常合成语义一致的替代品；3）对抗性多教师蒸馏框架，使用Vanilla Teacher的语义监督和Anomaly-Influence Debiasing Teacher的异常感知信号训练学生模型。

Result: 在四个基准数据集上的广泛实验表明，HealSplit在十种最先进防御方法中表现最佳，在各种攻击场景下实现了卓越的鲁棒性和防御效果。

Conclusion: HealSplit为Split Federated Learning提供了一个有效的端到端防御解决方案，能够检测和恢复多种复杂的数据投毒攻击，显著提升了SFL系统的安全性。

Abstract: Split Federated Learning (SFL) is an emerging paradigm for privacy-preserving distributed learning. However, it remains vulnerable to sophisticated data poisoning attacks targeting local features, labels, smashed data, and model weights. Existing defenses, primarily adapted from traditional Federated Learning (FL), are less effective under SFL due to limited access to complete model updates. This paper presents HealSplit, the first unified defense framework tailored for SFL, offering end-to-end detection and recovery against five sophisticated types of poisoning attacks. HealSplit comprises three key components: (1) a topology-aware detection module that constructs graphs over smashed data to identify poisoned samples via topological anomaly scoring (TAS); (2) a generative recovery pipeline that synthesizes semantically consistent substitutes for detected anomalies, validated by a consistency validation student; and (3) an adversarial multi-teacher distillation framework trains the student using semantic supervision from a Vanilla Teacher and anomaly-aware signals from an Anomaly-Influence Debiasing (AD) Teacher, guided by the alignment between topological and gradient-based interaction matrices. Extensive experiments on four benchmark datasets demonstrate that HealSplit consistently outperforms ten state-of-the-art defenses, achieving superior robustness and defense effectiveness across diverse attack scenarios.

</details>


### [91] [Sparse Methods for Vector Embeddings of TPC Data](https://arxiv.org/abs/2511.11221)
*Tyler Wheeler,Michelle P. Kuchera,Raghuram Ramanujan,Ryan Krupp,Chris Wrede,Saiprasad Ravishankar,Connor L. Cross,Hoi Yan Ian Heung,Andrew J. Jones,Benjamin Votaw*

Main category: cs.LG

TL;DR: 探索稀疏卷积网络在时间投影室(TPC)数据表示学习中的应用，发现稀疏ResNet架构能提供有用的事件向量嵌入，预训练可进一步改进嵌入质量，在不同TPC探测器上验证了其通用性。


<details>
  <summary>Details</summary>
Motivation: TPC探测器在核物理实验中广泛应用，需要有效的表示学习方法来处理其原始数据，以揭示丰富的事件结构。

Method: 使用稀疏张量表示原始pad级信号，训练Minkowski Engine ResNet模型，通过预训练和跨探测器测试验证方法有效性。

Result: 稀疏ResNet模型即使在随机权重下也能提供有用的事件嵌入，预训练后嵌入质量进一步提升，在不同TPC探测器上表现出良好通用性。

Conclusion: 稀疏卷积技术有潜力成为TPC实验中通用的表示学习工具，为多样化核物理实验提供有效的数据处理方法。

Abstract: Time Projection Chambers (TPCs) are versatile detectors that reconstruct charged-particle tracks in an ionizing medium, enabling sensitive measurements across a wide range of nuclear physics experiments. We explore sparse convolutional networks for representation learning on TPC data, finding that a sparse ResNet architecture, even with randomly set weights, provides useful structured vector embeddings of events. Pre-training this architecture on a simple physics-motivated binary classification task further improves the embedding quality. Using data from the GAseous Detector with GErmanium Tagging (GADGET) II TPC, a detector optimized for measuring low-energy $β$-delayed particle decays, we represent raw pad-level signals as sparse tensors, train Minkowski Engine ResNet models, and probe the resulting event-level embeddings which reveal rich event structure. As a cross-detector test, we embed data from the Active-Target TPC (AT-TPC) -- a detector designed for nuclear reaction studies in inverse kinematics -- using the same encoder. We find that even an untrained sparse ResNet model provides useful embeddings of AT-TPC data, and we observe improvements when the model is trained on GADGET data. Together, these results highlight the potential of sparse convolutional techniques as a general tool for representation learning in diverse TPC experiments.

</details>


### [92] [Retrofit: Continual Learning with Bounded Forgetting for Security Applications](https://arxiv.org/abs/2511.11439)
*Yiling He,Junchi Lei,Hongyu She,Shuo Shao,Xinran Zheng,Yiping Liu,Zhan Qin,Lorenzo Cavallaro*

Main category: cs.LG

TL;DR: RETROFIT是一种无需历史数据的持续学习方法，通过参数级合并和低秩稀疏更新来缓解灾难性遗忘，在恶意软件检测和二进制摘要任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在安全关键场景下面临两个挑战：在没有旧数据的情况下保持先验知识，以及以最小干扰集成新知识。传统方法依赖完全重训练或数据回放，在数据敏感环境中不可行。

Method: RETROFIT通过合并先前训练和新微调的模型作为新旧知识的教师，采用参数级合并避免历史数据需求。使用低秩和稀疏更新将参数变化限制在独立子空间，并通过基于模型置信度的知识仲裁动态平衡教师贡献。

Result: 在时间漂移下的恶意软件检测中，RETROFIT将保留分数从基线的20.2%提升至38.6%，并超过新数据上的oracle上限。在跨反编译级别的二进制摘要中，RETROFIT的BLEU分数是先前工作中迁移学习的两倍，在跨表示泛化方面超过所有基线。

Conclusion: RETROFIT通过无数据回顾的持续学习方法有效缓解了遗忘问题，同时保持了适应性，为安全分析中的模型持续学习提供了可行解决方案。

Abstract: Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.
  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.

</details>


### [93] [Neural Network-Powered Finger-Drawn Biometric Authentication](https://arxiv.org/abs/2511.11235)
*Maan Al Balkhi,Kordian Gontarska,Marko Harasic,Adrian Paschke*

Main category: cs.LG

TL;DR: 本文研究了基于神经网络的手指绘制数字生物认证方法，在触摸屏设备上使用CNN和自编码器架构进行用户认证，取得了约89%的认证准确率。


<details>
  <summary>Details</summary>
Motivation: 探索在触摸屏设备上使用简单手指绘制数字（0-9）作为生物特征进行用户认证的可行性，提供一种安全且用户友好的认证方案。

Method: 使用20名参与者在个人触摸屏设备上绘制2000个手指数字，比较了两种CNN架构（改进的Inception-V1和轻量级浅层CNN）以及卷积和全连接自编码器的异常检测方法。

Result: 两种CNN架构均达到约89%的认证准确率，其中浅层CNN参数更少；自编码器方法达到约75%的准确率。

Conclusion: 手指绘制符号认证为触摸屏设备提供了一种可行、安全且用户友好的生物认证解决方案，可与现有基于模式的认证方法集成，构建多层安全系统。

Abstract: This paper investigates neural network-based biometric authentication using finger-drawn digits on touchscreen devices. We evaluated CNN and autoencoder architectures for user authentication through simple digit patterns (0-9) traced with finger input. Twenty participants contributed 2,000 finger-drawn digits each on personal touchscreen devices. We compared two CNN architectures: a modified Inception-V1 network and a lightweight shallow CNN for mobile environments. Additionally, we examined Convolutional and Fully Connected autoencoders for anomaly detection. Both CNN architectures achieved ~89% authentication accuracy, with the shallow CNN requiring fewer parameters. Autoencoder approaches achieved ~75% accuracy. The results demonstrate that finger-drawn symbol authentication provides a viable, secure, and user-friendly biometric solution for touchscreen devices. This approach can be integrated with existing pattern-based authentication methods to create multi-layered security systems for mobile applications.

</details>


### [94] [Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies](https://arxiv.org/abs/2511.11461)
*Riku Green,Huw Day,Zahraa S. Abdallah,Telmo M. Silva Filho*

Main category: cs.LG

TL;DR: 论文重新审视了多步预测中递归策略和直接策略的传统偏见-方差权衡观点，通过理论分析和实验证明，对于非线性模型，递归策略可能同时具有更低偏差和更高方差。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为递归策略具有高偏差低方差，直接策略具有低偏差高方差。本文旨在重新检验这一直觉，通过理论分解多步预测误差来理解两种策略的实际表现。

Method: 将多步预测误差分解为不可约噪声、结构近似差距和估计方差三个部分。对于线性预测器证明结构差距为零，对于非线性预测器分析递归组合如何影响模型表达能力。引入基于雅可比矩阵的放大因子来衡量参数误差敏感性。

Result: 理论分析表明递归策略的估计方差可以表示为一步方差乘以雅可比放大因子。在ETTm1数据集上的多层感知机实验验证了这些发现，显示递归策略可能同时具有更低偏差和更高方差。

Conclusion: 选择递归或直接策略应基于模型非线性和噪声特性，而非传统偏见-方差直觉。非线性模型下递归策略可能提供更好的偏差-方差权衡。

Abstract: Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.

</details>


### [95] [Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models](https://arxiv.org/abs/2511.11490)
*Joan Font-Quer Roset,Devina Mohan,Anna Scaife*

Main category: cs.LG

TL;DR: 使用基于分数的扩散模型估计Radio Galaxy Zoo数据集的固有维度，发现分布外源具有更高的固有维度值，RGZ的整体固有维度超过自然图像数据集。


<details>
  <summary>Details</summary>
Motivation: 研究RGZ数据集的固有维度如何随贝叶斯神经网络能量分数变化，能量分数衡量射电源与RGZ数据集中MiraBest子集的相似度。

Method: 使用基于分数的扩散模型估计RGZ数据集的固有维度，分析固有维度与BNN能量分数、Fanaroff-Riley形态分类和信噪比的关系。

Result: 分布外源表现出更高的固有维度值；RGZ整体固有维度高于自然图像数据集；FR I和FR II类之间无关系；信噪比与固有维度呈弱负相关趋势。

Conclusion: RGZ数据集可利用固有维度与能量分数的关系来定量研究和改进各种自监督学习算法学到的表示。

Abstract: In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.

</details>


### [96] [A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication](https://arxiv.org/abs/2511.11560)
*Angelo Rodio,Giovanni Neglia,Zheng Chen,Erik G. Larsson*

Main category: cs.LG

TL;DR: 本文对半去中心化联邦学习中的两种模型分发策略（S2S和S2A）进行了理论分析和实验比较，揭示了在不同数据异质性程度下各自的优势，并提出了实际部署的设计指南。


<details>
  <summary>Details</summary>
Motivation: 在半去中心化联邦学习中，虽然设备主要依赖设备间通信，但偶尔会与中央服务器交互。目前缺乏对两种模型分发策略（仅向采样客户端分发聚合模型 vs 向所有客户端广播）的严格理论和实证比较。

Method: 建立了一个统一的收敛性分析框架，考虑了采样率、服务器聚合频率和网络连接性等关键系统参数，并通过实验验证分析结果。

Result: 分析和实验结果表明，S2S和S2A策略在不同数据异质性程度下各有优势，存在明显的性能差异区域。

Conclusion: 研究结果为实际半去中心化联邦学习部署提供了具体的设计指导，帮助根据数据异质性程度选择合适的模型分发策略。

Abstract: In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.

</details>


### [97] [Heterogeneous Attributed Graph Learning via Neighborhood-Aware Star Kernels](https://arxiv.org/abs/2511.11245)
*Hong Huang,Chengyu Yao,Haiming Chen,Hang Gao*

Main category: cs.LG

TL;DR: 提出了NASK（邻域感知星形核），一种用于属性图学习的新型图核方法，通过Gower相似度系数和Weisfeiler-Lehman迭代来同时捕捉异质属性语义和邻域结构信息。


<details>
  <summary>Details</summary>
Motivation: 现有图核方法难以同时捕捉属性图中的异质属性语义和邻域信息，而属性图在社交网络、生物信息学等领域普遍存在，需要更有效的相似性度量方法。

Method: 使用Gower相似度系数的指数变换来联合建模数值和分类特征，并通过Weisfeiler-Lehman迭代增强的星形子结构来整合多尺度邻域结构信息。

Result: 在11个属性图和4个大规模真实图基准测试中，NASK在16个最先进基线（包括9个图核和7个图神经网络）上始终表现出优越性能。

Conclusion: NASK是一个正定的图核，能够有效处理属性图的异质特征和结构信息，在多种基准测试中优于现有方法。

Abstract: Attributed graphs, typically characterized by irregular topologies and a mix of numerical and categorical attributes, are ubiquitous in diverse domains such as social networks, bioinformatics, and cheminformatics. While graph kernels provide a principled framework for measuring graph similarity, existing kernel methods often struggle to simultaneously capture heterogeneous attribute semantics and neighborhood information in attributed graphs. In this work, we propose the Neighborhood-Aware Star Kernel (NASK), a novel graph kernel designed for attributed graph learning. NASK leverages an exponential transformation of the Gower similarity coefficient to jointly model numerical and categorical features efficiently, and employs star substructures enhanced by Weisfeiler-Lehman iterations to integrate multi-scale neighborhood structural information. We theoretically prove that NASK is positive definite, ensuring compatibility with kernel-based learning frameworks such as SVMs. Extensive experiments are conducted on eleven attributed and four large-scale real-world graph benchmarks. The results demonstrate that NASK consistently achieves superior performance over sixteen state-of-the-art baselines, including nine graph kernels and seven Graph Neural Networks.

</details>


### [98] [Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria](https://arxiv.org/abs/2511.11293)
*Jiheum Park,Chao Pang,Tristan Y. Lee,Jeong Yun Yang,Jacob Berkowitz,Alexander Z. Wei,Nicholas Tatonetti*

Main category: cs.LG

TL;DR: EHR基础模型相比传统风险因素在癌症筛查中显著提高了高风险人群的识别能力，在8种主要癌症中实现了3-6倍的真实病例富集率提升。


<details>
  <summary>Details</summary>
Motivation: 当前癌症筛查指南仅覆盖少数癌症类型，依赖年龄或单一风险因素等狭窄标准来识别高风险个体。基于电子健康记录的预测模型可能通过检测微妙的癌症前诊断信号，提供更有效的工具。

Method: 使用All of Us研究项目中865,000多名参与者的EHR、基因组和调查数据，系统评估EHR预测模型与包括基因突变和癌症家族史在内的传统风险因素在8种主要癌症中的临床效用。

Result: 即使使用基线建模方法，EHR模型在识别为高风险个体中实现了比单独使用传统风险因素高3-6倍的真实癌症病例富集率。EHR基础模型进一步在26种癌症类型中提高了预测性能。

Conclusion: 基于EHR的预测建模具有临床潜力，可以支持更精确和可扩展的早期检测策略。

Abstract: Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.

</details>


### [99] [Fast and Expressive Multi-Token Prediction with Probabilistic Circuits](https://arxiv.org/abs/2511.11346)
*Andreas Grivas,Lorenzo Loconte,Emile van Krieken,Piotr Nawrot,Yu Zhao,Euan Wielewski,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: MTPC框架通过概率电路探索多令牌预测中表达力与延迟的权衡，显著加速字节级LLM生成，同时保持原始验证器性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多令牌预测方法因假设未来令牌独立而牺牲表达力的问题，在字节级LLM中实现更好的速度与性能平衡。

Method: 提出MTPC框架，使用概率电路编码未来令牌的联合分布，支持混合模型、隐马尔可夫模型和张量网络等架构，并与推测解码结合。

Result: 实验显示MTPC相比独立假设的MTP显著加速生成，同时保证原始验证器LLM性能不变。

Conclusion: MTPC在表达力与延迟之间提供了最优权衡，通过探索不同电路架构和参数化配置实现高效生成。

Abstract: Multi-token prediction (MTP) is a prominent strategy to significantly speed up generation in large language models (LLMs), including byte-level LLMs, which are tokeniser-free but prohibitively slow. However, existing MTP methods often sacrifice expressiveness by assuming independence between future tokens. In this work, we investigate the trade-off between expressiveness and latency in MTP within the framework of probabilistic circuits (PCs). Our framework, named MTPC, allows one to explore different ways to encode the joint distributions over future tokens by selecting different circuit architectures, generalising classical models such as (hierarchical) mixture models, hidden Markov models and tensor networks. We show the efficacy of MTPC by retrofitting existing byte-level LLMs, such as EvaByte. Our experiments show that, when combined with speculative decoding, MTPC significantly speeds up generation compared to MTP with independence assumptions, while guaranteeing to retain the performance of the original verifier LLM. We also rigorously study the optimal trade-off between expressiveness and latency when exploring the possible parameterisations of MTPC, such as PC architectures and partial layer sharing between the verifier and draft LLMs.

</details>


### [100] [Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials](https://arxiv.org/abs/2511.11361)
*Guangyi Dong,Zhihui Wang*

Main category: cs.LG

TL;DR: 开发了一个多保真度机器学习力场框架，可同时利用低保真度非磁性和高保真度磁性计算数据集来训练锂离子电池正极材料的力场，提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池正极材料的机器学习力场发展相对有限，主要因为其复杂的电子结构特性和高质量计算数据集的稀缺。

Method: 构建多保真度机器学习力场框架，同时利用低保真度非磁性和高保真度磁性计算数据集进行训练。

Result: 在磷酸锰铁锂正极材料系统上的测试证明了这种多保真度方法的有效性。

Conclusion: 该工作有助于以较低的训练数据集成本实现正极材料的高精度机器学习力场训练，为机器学习力场在正极材料计算模拟中的应用提供了新视角。

Abstract: Machine learning force fields (MLFFs), which employ neural networks to map atomic structures to system energies, effectively combine the high accuracy of first-principles calculation with the computational efficiency of empirical force fields. They are widely used in computational materials simulations. However, the development and application of MLFFs for lithium-ion battery cathode materials remain relatively limited. This is primarily due to the complex electronic structure characteristics of cathode materials and the resulting scarcity of high-quality computational datasets available for force field training. In this work, we develop a multi-fidelity machine learning force field framework to enhance the data efficiency of computational results, which can simultaneously utilize both low-fidelity non-magnetic and high-fidelity magnetic computational datasets of cathode materials for training. Tests conducted on the lithium manganese iron phosphate (LMFP) cathode material system demonstrate the effectiveness of this multi-fidelity approach. This work helps to achieve high-accuracy MLFF training for cathode materials at a lower training dataset cost, and offers new perspectives for applying MLFFs to computational simulations of cathode materials.

</details>


### [101] [On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization](https://arxiv.org/abs/2511.11362)
*Prabodh Katti,Sangwoo Park,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: MeZO（内存高效零阶优化）通过仅使用前向评估来估计梯度，消除了存储中间激活或优化器状态的需求，使得在边缘设备内存约束下能够部署更大的模型，尽管可能需要更长的微调时间。


<details>
  <summary>Details</summary>
Motivation: 边缘AI系统需要在严格的内存约束下适应不同的代理任务，传统的基于反向传播的训练需要存储层激活和优化器状态，这在边缘部署中严重限制了可部署的最大模型大小。

Method: 使用MeZO（内存高效零阶优化）方法，仅通过前向评估来估计梯度，无需存储中间激活或优化器状态，从而显著减少内存占用。

Result: 理论分析和数值验证表明，在设备内存约束下，MeZO能够容纳比传统反向传播方法更大的模型，并且在有足够微调时间的情况下表现出精度优势。

Conclusion: MeZO为边缘AI系统提供了一种有效的内存高效微调方法，能够在内存受限的环境中部署更大的模型，但需要权衡更长的微调时间。

Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.

</details>


### [102] [When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering](https://arxiv.org/abs/2511.11380)
*Jiangkai Long,Yanran Zhu,Chang Tang,Kun Sun,Yuanyuan Liu,Xuesong Yan*

Main category: cs.LG

TL;DR: SemST是一个语义引导的深度学习框架，利用大语言模型将基因符号转化为生物语义嵌入，结合图神经网络捕获的空间关系，实现空间转录组数据的聚类分析。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法将基因视为孤立数值特征，忽略了基因符号中丰富的生物语义信息，阻碍了对组织微环境的深入理解。

Method: 使用LLM将基因符号转化为生物语义嵌入，通过图神经网络融合空间邻域关系，并引入细粒度语义调制模块进行特征校准。

Result: 在公共空间转录组数据集上实现了最先进的聚类性能，FSM模块具有即插即用的通用性，能持续提升其他基线方法的性能。

Conclusion: SemST成功整合了生物功能和空间结构，为空间转录组分析提供了新的语义增强方法。

Abstract: Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to "speak" through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.

</details>


### [103] [Robust inverse material design with physical guarantees using the Voigt-Reuss Net](https://arxiv.org/abs/2511.11388)
*Sanath Keshav,Felix Fritzen*

Main category: cs.LG

TL;DR: 提出了一种具有严格物理保证的谱归一化代理方法，用于机械均质化的正问题和反问题。该方法基于Voigt-Reuss界限，通过Cholesky类算子学习特征值在[0,1]范围内的无量纲对称半正定表示，确保反演预测位于界限之间。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够同时处理正问题和反问题机械均质化的方法，确保物理可接受性，提供严格的物理保证，并实现高效的大批量反演设计。

Method: 利用Voigt-Reuss界限，通过Cholesky类算子分解其差异，学习特征值在[0,1]范围内的对称半正定表示。在3D线性弹性中使用全连接Voigt-Reuss网络，在2D平面应变中结合谱归一化、可微分渲染器和CNN。

Result: 在3D线性弹性中，各向同性投影恢复近乎完美（R²≥0.998），张量级相对Frobenius误差中值约1.7%，均值约3.4%。在2D平面应变中，所有分量R²>0.99，归一化损失低于1%，准确跟踪渗流引起的特征值跳跃，对分布外图像具有鲁棒泛化能力。

Conclusion: Voigt-Reuss网络统一了准确、物理可接受的正向预测与大批量、约束一致的反演设计，适用于椭圆算子和耦合物理场景。

Abstract: We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees. Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the Löwner sense. In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $>\!7.5\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs. Tensor-level relative Frobenius errors have median $\approx 1.7\%$ and mean $\approx 3.4\%$ across splits. For 2D plane strain on thresholded trigonometric microstructures, coupling spectral normalization with a differentiable renderer and a CNN yields $R^2>0.99$ on all components, subpercent normalized losses, accurate tracking of percolation-induced eigenvalue jumps, and robust generalization to out-of-distribution images. Treating the parametric microstructure as design variables, batched first-order optimization with a single surrogate matches target tensors within a few percent and returns diverse near-optimal designs. Overall, the Voigt-Reuss net unifies accurate, physically admissible forward prediction with large-batch, constraint-consistent inverse design, and is generic to elliptic operators and coupled-physics settings.

</details>


### [104] [SPOT: Single-Shot Positioning via Trainable Near-Field Rainbow Beamforming](https://arxiv.org/abs/2511.11391)
*Yeyue Cai,Jianhua Mo,Meixia Tao*

Main category: cs.LG

TL;DR: 提出基于深度学习的端到端方案，同时设计彩虹波束并估计用户位置，通过将移相器和真时延系数作为可训练变量来最大化定位精度，显著降低开销和定位误差。


<details>
  <summary>Details</summary>
Motivation: 相位时间阵列结合移相器和真时延器是宽带感知和定位中生成频率相关彩虹波束的经济有效架构，需要同时优化波束设计和位置估计。

Method: 将移相器和真时延系数作为可训练变量，网络合成面向任务的波束以最大化定位精度；轻量级全连接模块从用户反馈的最大量化接收功率及其对应子载波索引中恢复用户的角距坐标。

Result: 与现有分析和学习方案相比，该方法将开销降低一个数量级，并持续提供更低的二维定位误差。

Conclusion: 所提出的端到端深度学习方案在单次下行传输后即可实现高效的彩虹波束设计和用户位置估计，显著优于现有方法。

Abstract: Phase-time arrays, which integrate phase shifters (PSs) and true-time delays (TTDs), have emerged as a cost-effective architecture for generating frequency-dependent rainbow beams in wideband sensing and localization. This paper proposes an end-to-end deep learning-based scheme that simultaneously designs the rainbow beams and estimates user positions. Treating the PS and TTD coefficients as trainable variables allows the network to synthesize task-oriented beams that maximize localization accuracy. A lightweight fully connected module then recovers the user's angle-range coordinates from its feedback of the maximum quantized received power and its corresponding subcarrier index after a single downlink transmission. Compared with existing analytical and learning-based schemes, the proposed method reduces overhead by an order of magnitude and delivers consistently lower two-dimensional positioning error.

</details>


### [105] [Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning](https://arxiv.org/abs/2511.11402)
*Amit Jain,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的强化学习框架，用于统一多阶段航天器轨迹优化，通过单一策略架构处理不同动态特性的任务阶段，消除了手动阶段切换的需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为不同任务阶段分别训练策略，限制了适应性和增加了操作复杂性。需要能够跨动态不同阶段的自适应策略。

Method: 基于近端策略优化(PPO)，用Transformer编码器-解码器结构替换传统循环网络，集成门控Transformer-XL(GTrXL)架构，保持跨阶段的一致性记忆。

Result: 在单阶段基准测试中达到接近最优性能，在多阶段航点导航变体中有效学习，在复杂多阶段火箭上升问题中成功处理大气飞行、级分离和真空操作。

Conclusion: Transformer框架不仅匹配简单情况下的解析解，还能有效学习跨动态不同阶段的连贯控制策略，为可扩展自主任务规划奠定基础，减少对阶段特定控制器的依赖。

Abstract: Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual astrodynamics tasks, existing approaches often require separate policies for distinct mission phases, limiting adaptability and increasing operational complexity. This work introduces a transformer-based RL framework that unifies multi-phase trajectory optimization through a single policy architecture, leveraging the transformer's inherent capacity to model extended temporal contexts. Building on proximal policy optimization (PPO), our framework replaces conventional recurrent networks with a transformer encoder-decoder structure, enabling the agent to maintain coherent memory across mission phases spanning seconds to minutes during critical operations. By integrating a Gated Transformer-XL (GTrXL) architecture, the framework eliminates manual phase transitions while maintaining stability in control decisions. We validate our approach progressively: first demonstrating near-optimal performance on single-phase benchmarks (double integrator and Van der Pol oscillator), then extending to multiphase waypoint navigation variants, and finally tackling a complex multiphase rocket ascent problem that includes atmospheric flight, stage separation, and vacuum operations. Results demonstrate that the transformer-based framework not only matches analytical solutions in simple cases but also effectively learns coherent control policies across dynamically distinct regimes, establishing a foundation for scalable autonomous mission planning that reduces reliance on phase-specific controllers while maintaining compatibility with safety-critical verification protocols.

</details>


### [106] [Multicalibration yields better matchings](https://arxiv.org/abs/2511.11413)
*Riccardo Colini Baldeschi,Simone Di Gregorio,Simone Fioravanti,Federico Fusco,Ido Guy,Daniel Haimovich,Stefano Leonardi,Fridolin Linder,Lorenzo Perini,Matteo Russo,Niek Tax*

Main category: cs.LG

TL;DR: 本文提出使用多校准方法来处理图匹配问题中权重预测不完美的情况，通过构造多校准预测器来提升匹配算法的性能。


<details>
  <summary>Details</summary>
Motivation: 在加权图匹配问题中，当只能访问基于上下文的权重预测时，如果预测器不是贝叶斯最优的，那么基于预测权重计算最佳匹配可能不是最优的。需要一种方法来补偿预测误差。

Method: 提出使用多校准方法，要求预测器在受保护上下文集合的每个元素上都是无偏的。给定匹配算法类和权重预测器，构造特定的多校准预测器。

Result: 基于多校准预测器输出的最佳匹配与在原始预测器上应用最佳决策规则具有竞争力。

Conclusion: 多校准方法能够有效处理不完美预测器的问题，并提供了样本复杂度界限来支持理论结果。

Abstract: Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.
  In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\hat γ$, with the following property. Picking the best matching based on the output of $\hat γ$ is competitive with the best decision rule in $\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.

</details>


### [107] [Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization](https://arxiv.org/abs/2511.11415)
*Nikolas Borrel-Jensen,Josiah Bjorgaard*

Main category: cs.LG

TL;DR: 提出了一种基于可微分编程的声学逆问题解决方法，应用于导纳估计和共振阻尼的形状优化，通过自动微分和有限差分结合实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 传统声学逆问题求解需要手动推导伴随方程，过程复杂且效率低，需要一种更自动化和高效的优化方法。

Method: 使用JAX-FEM进行自动微分实现导纳估计，结合PyTorch3D进行网格操作，采用随机有限差分进行形状优化，将物理驱动的边界优化与几何驱动的内部网格适应分离。

Result: 导纳估计达到3位精度，形状优化在目标频率上实现48.1%的能量减少，相比标准有限差分方法减少了30倍的FEM求解次数。

Conclusion: 现代可微分软件栈能够快速原型化基于物理的逆问题优化工作流，自动微分适用于参数估计，有限差分与自动微分结合适用于几何设计。

Abstract: We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.

</details>


### [108] [Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching](https://arxiv.org/abs/2511.11418)
*Dara Varam,Diaa A. Abuhani,Imran Zualkernan,Raghad AlDamani,Lujain Khalil*

Main category: cs.LG

TL;DR: 本文提出了一种基于最优传输(OT)的流匹配生成模型后训练量化方法，能够在2-3位精度下保持生成质量，优于传统量化方案。


<details>
  <summary>Details</summary>
Motivation: 流匹配生成模型虽然具有高效的无模拟训练和确定性采样优势，但实际部署面临高精度参数需求挑战，需要有效的量化压缩方法。

Method: 采用基于最优传输的后训练量化方法，最小化量化权重与原始权重之间的2-Wasserstein距离，并与均匀量化、分段量化和对数量化方案进行系统比较。

Result: 在五个不同复杂度的基准数据集上的实验表明，OT量化在2-3位精度下仍能保持视觉生成质量和潜在空间稳定性，而其他方法在此精度下失效。

Conclusion: OT量化是一种原则性、有效的压缩方法，适用于边缘和嵌入式AI应用中的流匹配生成模型部署。

Abstract: Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.

</details>


### [109] [DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference](https://arxiv.org/abs/2511.11446)
*Farhana Amin,Sabiha Afroz,Kanchon Gharami,Mona Moghadampanah,Dimitrios S. Nikolopoulos*

Main category: cs.LG

TL;DR: DiffPro是一个后训练框架，通过联合优化时间步和逐层精度来加速扩散模型推理，无需重新训练即可实现6.25倍模型压缩和2.8倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量图像，但由于需要大量去噪步骤和繁重的矩阵运算，推理成本高昂。

Method: 结合三个组件：基于流形感知的敏感度指标分配权重比特、动态激活量化稳定跨时间步的激活、基于师生漂移的预算时间步选择器。

Result: 在标准基准测试上实现Delta FID <= 10，模型压缩达6.25倍，时间步减少50%，推理速度提升2.8倍。

Conclusion: DiffPro将步数减少和精度规划统一为单个可部署计划，为实时节能扩散推理提供实用效率增益。

Abstract: Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.

</details>


### [110] [FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression](https://arxiv.org/abs/2511.11459)
*Xiaoyin Xi,Zhe Yu*

Main category: cs.LG

TL;DR: 提出了一个基于密度估计的FairReweighing预处理算法，用于解决回归任务中的公平性问题，确保模型满足分离准则，在保持高准确性的同时提升公平性。


<details>
  <summary>Details</summary>
Motivation: AI软件在公共部门和工业应用中日益普及，但缺乏透明度引发了关于这些数据驱动的AI决策是否对所有种族、性别或年龄群体都公平的担忧。目前大多数公平性研究集中在二元分类任务上，回归中的公平性相对未被充分探索。

Method: 采用基于互信息的指标评估分离违规，并将其扩展到可同时处理分类和回归问题以及二元和连续敏感属性。提出了基于密度估计的FairReweighing预处理算法，确保学习模型满足分离准则。

Result: 理论上证明在数据独立性假设下，FairReweighing算法可以保证训练数据中的分离。实证结果表明，在合成和真实世界数据上，FairReweighing在提高分离的同时保持高准确性方面优于现有最先进的回归公平性解决方案。

Conclusion: FairReweighing算法为回归任务中的公平性问题提供了一个有效的预处理解决方案，能够显著改善模型公平性而不牺牲准确性。

Abstract: There has been a prevalence of applying AI software in both high-stakes public-sector and industrial contexts. However, the lack of transparency has raised concerns about whether these data-informed AI software decisions secure fairness against people of all racial, gender, or age groups. Despite extensive research on emerging fairness-aware AI software, up to now most efforts to solve this issue have been dedicated to binary classification tasks. Fairness in regression is relatively underexplored. In this work, we adopted a mutual information-based metric to assess separation violations. The metric is also extended so that it can be directly applied to both classification and regression problems with both binary and continuous sensitive attributes. Inspired by the Reweighing algorithm in fair classification, we proposed a FairReweighing pre-processing algorithm based on density estimation to ensure that the learned model satisfies the separation criterion. Theoretically, we show that the proposed FairReweighing algorithm can guarantee separation in the training data under a data independence assumption. Empirically, on both synthetic and real-world data, we show that FairReweighing outperforms existing state-of-the-art regression fairness solutions in terms of improving separation while maintaining high accuracy.

</details>


### [111] [MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture](https://arxiv.org/abs/2511.11462)
*Kevin Chen,Kenneth W. Parker,Anish Arora*

Main category: cs.LG

TL;DR: 提出了一种基于纯机器学习的从运动捕捉数据合成雷达频谱图的方法，使用基于Transformer的模型将MoCap数据转换为多普勒雷达频谱图。


<details>
  <summary>Details</summary>
Motivation: 解决雷达数据稀缺问题，利用更丰富的运动捕捉数据来增强雷达数据集，同时相比基于物理的方法需要更少的计算资源。

Method: 将MoCap到频谱图的转换建模为窗口序列到序列任务，使用Transformer模型共同捕捉MoCap标记之间的空间关系和帧间的时间动态。

Result: 实验表明该方法能够生成视觉和定量上合理的多普勒雷达频谱图，并具有良好的泛化能力。消融实验显示模型具备将多部位运动转换为多普勒特征的能力。

Conclusion: 这是使用Transformer进行时间序列信号处理的有趣示例，特别适用于边缘计算和物联网雷达，能够有效增强雷达数据集，计算效率优于基于物理的方法。

Abstract: We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.
  The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.

</details>


### [112] [Quantifying and Improving Adaptivity in Conformal Prediction through Input Transformations](https://arxiv.org/abs/2511.11472)
*Sooyong Jang,Insup Lee*

Main category: cs.LG

TL;DR: 提出了一种新的自适应预测集评估方法，通过输入变换和均匀质量分箱来更准确地评估方法的适应性，并基于此开发了新的自适应预测集算法。


<details>
  <summary>Details</summary>
Motivation: 现有适应性评估方法存在分箱不平衡问题，导致覆盖率或集合大小估计不准确，需要更可靠的适应性评估指标。

Method: 使用输入变换按难度排序样本，然后进行均匀质量分箱，提出两个新指标来评估适应性。基于此开发了按估计难度分组并应用组条件保形预测的新算法。

Result: 实验表明，新指标与期望的适应性属性相关性更强，新算法在图像分类和医疗任务上都优于现有方法。

Conclusion: 提出的分箱方法和评估指标能更准确地评估适应性，基于此的新自适应预测集算法在多个任务上表现优异。

Abstract: Conformal prediction constructs a set of labels instead of a single point prediction, while providing a probabilistic coverage guarantee. Beyond the coverage guarantee, adaptiveness to example difficulty is an important property. It means that the method should produce larger prediction sets for more difficult examples, and smaller ones for easier examples. Existing evaluation methods for adaptiveness typically analyze coverage rate violation or average set size across bins of examples grouped by difficulty. However, these approaches often suffer from imbalanced binning, which can lead to inaccurate estimates of coverage or set size. To address this issue, we propose a binning method that leverages input transformations to sort examples by difficulty, followed by uniform-mass binning. Building on this binning, we introduce two metrics to better evaluate adaptiveness. These metrics provide more reliable estimates of coverage rate violation and average set size due to balanced binning, leading to more accurate adaptivity assessment. Through experiments, we demonstrate that our proposed metric correlates more strongly with the desired adaptiveness property compared to existing ones. Furthermore, motivated by our findings, we propose a new adaptive prediction set algorithm that groups examples by estimated difficulty and applies group-conditional conformal prediction. This allows us to determine appropriate thresholds for each group. Experimental results on both (a) an Image Classification (ImageNet) (b) a medical task (visual acuity prediction) show that our method outperforms existing approaches according to the new metrics.

</details>


### [113] [Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys](https://arxiv.org/abs/2511.11485)
*Alinda Ezgi Gerçek,Till Korten,Paul Chekhonin,Maleeha Hassan,Peter Steinbach*

Main category: cs.LG

TL;DR: 提出了一种数据高效的轻量级U-Net分割方法，仅使用10张标注的SEM图像就能准确分割反应堆压力容器钢中的碳化物，Dice系数达0.98，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 反应堆压力容器钢中碳化物的准确分割对预测力学性能至关重要，但由于碳化物与基体的灰度值重叠，简单的阈值分割方法效果不佳。

Method: 使用轻量级U-Net（30.7M参数）构建数据高效的分割流程，仅需10张标注的SEM图像进行训练。

Result: 模型在有限数据下达到0.98的Dice-Sørensen系数，显著优于传统图像分析方法（0.85），并将标注工作量减少了一个数量级。

Conclusion: 该方法实现了快速自动化的碳化物量化，可推广到其他钢种，展示了数据高效深度学习在反应堆压力容器钢分析中的潜力。

Abstract: Understanding reactor-pressure-vessel steel microstructure is crucial for predicting mechanical properties, as carbide precipitates both strengthen the alloy and can initiate cracks. In scanning electron microscopy images, gray-value overlap between carbides and matrix makes simple thresholding ineffective. We present a data-efficient segmentation pipeline using a lightweight U-Net (30.7~M parameters) trained on just \textbf{10 annotated scanning electron microscopy images}. Despite limited data, our model achieves a \textbf{Dice-Sørensen coefficient of 0.98}, significantly outperforming the state-of-the-art in the field of metallurgy (classical image analysis: 0.85), while reducing annotation effort by one order of magnitude compared to the state-of-the-art data efficient segmentation model. This approach enables rapid, automated carbide quantification for alloy design and generalizes to other steel types, demonstrating the potential of data-efficient deep learning in reactor-pressure-vessel steel analysis.

</details>


### [114] [Honesty over Accuracy: Trustworthy Language Models through Reinforced Hesitation](https://arxiv.org/abs/2511.11500)
*Mohamad Amin Mohamadi,Tianhao Wang,Zhiyuan Li*

Main category: cs.LG

TL;DR: 论文提出强化犹豫(RH)方法，通过三元奖励机制(+1正确，0弃权，-λ错误)训练模型学会在不确定时弃权，并引入级联推理策略来利用弃权信号进行协调。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型缺乏可信智能的基本要求：知道何时不回答。即使面临严重后果，这些模型仍会产生自信的幻觉，无法通过提示覆盖训练中"任何回答都比不回答好"的倾向。

Method: 提出强化犹豫(RH)：在可验证奖励强化学习(RLVR)中使用三元奖励而非二元奖励；引入级联推理和自级联推理策略，利用训练出的弃权能力作为协调信号。

Result: 在GSM8K、MedQA和GPQA上的评估显示，前沿模型几乎从不弃权；控制实验表明不同λ值产生帕累托前沿上的不同模型；级联策略以更低计算成本优于多数投票。

Conclusion: 将弃权作为首要训练目标，使"我不知道"从失败转变为协调信号，让模型通过对其局限性的校准诚实来赢得信任。

Abstract: Modern language models fail a fundamental requirement of trustworthy intelligence: knowing when not to answer. Despite achieving impressive accuracy on benchmarks, these models produce confident hallucinations, even when wrong answers carry catastrophic consequences. Our evaluations on GSM8K, MedQA and GPQA show frontier models almost never abstain despite explicit warnings of severe penalties, suggesting that prompts cannot override training that rewards any answer over no answer. As a remedy, we propose Reinforced Hesitation (RH): a modification to Reinforcement Learning from Verifiable Rewards (RLVR) to use ternary rewards (+1 correct, 0 abstention, -$λ$ error) instead of binary. Controlled experiments on logic puzzles reveal that varying $λ$ produces distinct models along a Pareto frontier, where each training penalty yields the optimal model for its corresponding risk regime: low penalties produce aggressive answerers, high penalties conservative abstainers. We then introduce two inference strategies that exploit trained abstention as a coordination signal: cascading routes queries through models with decreasing risk tolerance, while self-cascading re-queries the same model on abstention. Both outperform majority voting with lower computational cost. These results establish abstention as a first-class training objective that transforms ``I don't know'' from failure into a coordination signal, enabling models to earn trust through calibrated honesty about their limits.

</details>


### [115] [FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models](https://arxiv.org/abs/2511.11505)
*Yonatan Dukler,Guihong Li,Deval Shah,Vikram Appia,Emad Barsoum*

Main category: cs.LG

TL;DR: FarSkip-Collective通过修改模型架构，在MoE模型中跳过连接以实现计算与通信的重叠，在16B到109B参数的大模型中保持精度，并加速训练和推理。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在分布式环境中由于阻塞通信导致的效率问题。

Method: 修改模型架构，跳过连接，通过自蒸馏转换模型，实现通信与计算的重叠。

Result: 成功转换16B到109B参数模型，精度与原版相当（如Llama 4 Scout 109B精度损失<1%），并实现训练和推理加速。

Conclusion: FarSkip-Collective方法有效解决了MoE模型的通信瓶颈，在保持精度的同时显著提升效率。

Abstract: Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.

</details>


### [116] [Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications](https://arxiv.org/abs/2511.11539)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 本文推广了最近公平聚类问题的研究，从仅处理两个群体扩展到任意数量群体，证明了多群体情况下的NP难问题，并提出了近线性时间近似算法，同时改进了公平相关聚类和公平共识聚类的近似保证。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法经常无法为受多个保护属性定义的各种边缘化群体提供公平表示，这通常由训练数据中的偏见引起。现有研究仅限于两个群体的情况，而实际数据通常包含多个群体（如年龄、种族、性别等），因此需要扩展最近公平聚类问题以处理任意数量的群体。

Method: 首先证明了多群体最近公平聚类问题即使在所有群体大小相等的情况下也是NP难的。然后提出了能够高效处理任意大小多群体的近线性时间近似算法。利用这些算法进一步改进了公平相关聚类问题的近似保证，并为多群体公平共识聚类问题提供了首个近似算法。

Result: 成功将最近公平聚类问题从两个群体推广到任意数量群体，解决了Chakraborty等人提出的开放性问题。提出的算法在效率和处理能力方面表现优异，同时为其他公平聚类问题提供了改进的解决方案。

Conclusion: 本研究填补了多群体公平聚类研究的空白，为处理具有多个保护属性的公平聚类问题提供了有效的算法解决方案，推动了公平机器学习领域的发展。

Abstract: Clustering is a fundamental task in machine learning and data analysis, but it frequently fails to provide fair representation for various marginalized communities defined by multiple protected attributes -- a shortcoming often caused by biases in the training data. As a result, there is a growing need to enhance the fairness of clustering outcomes, ideally by making minimal modifications, possibly as a post-processing step after conventional clustering. Recently, Chakraborty et al. [COLT'25] initiated the study of \emph{closest fair clustering}, though in a restricted scenario where data points belong to only two groups. In practice, however, data points are typically characterized by many groups, reflecting diverse protected attributes such as age, ethnicity, gender, etc.
  In this work, we generalize the study of the \emph{closest fair clustering} problem to settings with an arbitrary number (more than two) of groups. We begin by showing that the problem is NP-hard even when all groups are of equal size -- a stark contrast with the two-group case, for which an exact algorithm exists. Next, we propose near-linear time approximation algorithms that efficiently handle arbitrary-sized multiple groups, thereby answering an open question posed by Chakraborty et al. [COLT'25].
  Leveraging our closest fair clustering algorithms, we further achieve improved approximation guarantees for the \emph{fair correlation clustering} problem, advancing the state-of-the-art results established by Ahmadian et al. [AISTATS'20] and Ahmadi et al. [2020]. Additionally, we are the first to provide approximation algorithms for the \emph{fair consensus clustering} problem involving multiple (more than two) groups, thus addressing another open direction highlighted by Chakraborty et al. [COLT'25].

</details>


### [117] [Multistability of Self-Attention Dynamics in Transformers](https://arxiv.org/abs/2511.11553)
*Claudio Altafini*

Main category: cs.LG

TL;DR: 本文揭示了自注意力动态与多智能体Oja流的关系，将单头自注意力系统的平衡点分为四类：共识、二分共识、聚类和多边形平衡点，并发现前两类平衡点总是与价值矩阵的特征向量对齐。


<details>
  <summary>Details</summary>
Motivation: 研究自注意力动态的数学特性，探索其与Oja流的关系，并系统分类自注意力系统的平衡点类型。

Method: 将自注意力动态建模为连续时间多智能体系统，分析其与多智能体Oja流的关系，并对单头自注意力系统的平衡点进行数学分类。

Result: 识别出四类平衡点：共识、二分共识、聚类和多边形平衡点，发现前两类总是与价值矩阵的特征向量对齐，且前三类平衡点经常共存。

Conclusion: 自注意力动态具有丰富的平衡点结构，其共识和二分共识平衡点与价值矩阵的特征向量密切相关，这为理解transformer的注意力机制提供了数学基础。

Abstract: In machine learning, a self-attention dynamics is a continuous-time multiagent-like model of the attention mechanisms of transformers. In this paper we show that such dynamics is related to a multiagent version of the Oja flow, a dynamical system that computes the principal eigenvector of a matrix corresponding for transformers to the value matrix. We classify the equilibria of the ``single-head'' self-attention system into four classes: consensus, bipartite consensus, clustering and polygonal equilibria. Multiple asymptotically stable equilibria from the first three classes often coexist in the self-attention dynamics. Interestingly, equilibria from the first two classes are always aligned with the eigenvectors of the value matrix, often but not exclusively with the principal eigenvector.

</details>


### [118] [Optimizing Mixture of Block Attention](https://arxiv.org/abs/2511.11571)
*Guangxuan Xiao,Junxian Guo,Kasra Mazaheri,Song Han*

Main category: cs.LG

TL;DR: MoBA是一种通过稀疏注意力机制高效处理长上下文的LLM构建块，但缺乏理论理解和高效GPU实现。本文开发了统计模型分析MoBA机制，发现性能取决于路由器准确区分相关块的能力，并提出改进方法：使用更小的块大小和在键上应用短卷积。同时开发了FlashMoBA CUDA内核，使理论改进具有实用性。


<details>
  <summary>Details</summary>
Motivation: MoBA虽然能通过稀疏注意力降低计算成本，但其设计原则缺乏理论理解，且没有高效的GPU实现，阻碍了实际应用。

Method: 开发统计模型分析MoBA机制，推导信噪比连接架构参数与检索准确性；提出使用更小块大小和键上短卷积来改进路由准确性；开发FlashMoBA CUDA内核实现高效执行。

Result: 改进的MoBA模型在从头训练LLM时能与密集注意力基线性能匹配；FlashMoBA在小块情况下比FlashAttention-2快达14.7倍。

Conclusion: 通过理论分析和硬件优化，成功提升了MoBA的性能和效率，使其成为实用的长上下文处理解决方案。

Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.

</details>
