## Total Papers Today: 4
**Report Date:** 2025-06-09

<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation](https://arxiv.org/abs/2506.05566)
> *ScaleRTL：通过推理数据和测试时计算扩展大型语言模型以实现准确的RTL代码生成*

*Chenhui Deng, Yun-Da Tsai, Guan-Ting Liu, Zhongzhi Yu, Haoxing Ren* | **Main category: cs.AR**

**Keywords:** RTL代码生成, 大型语言模型, 推理数据, 测试时计算, ScaleRTL

> **TL;DR:** ScaleRTL是一个结合了高质量推理数据和测试时计算的RTL代码生成大型语言模型，显著提高了RTL代码生成的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在软件编码方面表现出色，但在RTL代码生成方面效果有限，主要原因是高质量训练数据稀缺。现有方法未能根本上解决数据瓶颈，且缺乏对测试时扩展的支持。

**Method:** 本文引入了ScaleRTL，它是第一个用于RTL编码的推理大型语言模型。它通过以下方式实现：1) 收集多样化的长链式思考推理痕迹，平均每条56K token，形成一个包含3.5B token的丰富RTL知识数据集。2) 在此语料库上对通用推理模型进行微调。3) 通过新颖的测试时扩展策略进一步增强性能，该策略通过迭代反思和自我纠正先前的推理步骤来扩展推理过程。

**Result:** ScaleRTL在VerilogEval和RTLLM上取得了最先进的性能，在VerilogEval上比18个竞争基线高出18.4%，在RTLLM上高出12.7%。

**Conclusion:** ScaleRTL通过结合高质量推理数据和创新的测试时计算扩展策略，显著提升了大型语言模型在RTL代码生成任务上的准确性和性能，克服了现有方法的局限性。

> **ai_Abstract:** ScaleRTL是首个针对RTL编码的推理大型语言模型，旨在解决现有LLM在RTL代码生成中数据稀缺和测试时扩展不足的问题。通过构建包含3.5B token的高质量推理数据集并进行模型微调，结合新颖的测试时迭代自校正策略，ScaleRTL在VerilogEval和RTLLM基准测试中表现出超越现有SOTA模型的性能。

> **摘要翻译:** 大型语言模型（LLM）的最新进展已使它们在软件编码基准测试上达到接近人类的水平，但由于高质量训练数据的稀缺，它们在RTL代码生成方面的有效性仍然有限。虽然之前的努力已经对LLM进行了RTL任务的微调，但它们并未从根本上克服数据瓶颈，并且由于其非推理性质而缺乏对测试时扩展的支持。在这项工作中，我们引入了ScaleRTL，这是第一个用于RTL编码的推理LLM，它同时扩展了高质量推理数据和测试时计算。具体来说，我们整理了一组多样化的长链式思考推理痕迹，平均每条56K token，从而形成了一个包含3.5B token的数据集，该数据集捕获了丰富的RTL知识。在此语料库上对通用推理模型进行微调产生了ScaleRTL，它能够进行深入的RTL推理。随后，我们通过一种新颖的测试时扩展策略进一步增强了ScaleRTL的性能，该策略通过迭代反思和自我纠正先前的推理步骤来扩展推理过程。实验结果表明，ScaleRTL在VerilogEval和RTLLM上取得了最先进的性能，在VerilogEval上比18个竞争基线高出18.4%，在RTLLM上高出12.7%。

</details>


### [2] [Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy](https://arxiv.org/abs/2506.05682)
> *Lumina：通过利用计算冗余实现实时移动神经渲染*

*Yu Feng, Weikai Lin, Yuge Cheng, Zihan Liu, Jingwen Leng, Minyi Guo, Chen Chen, Shixuan Sun, Yuhao Zhu* | **Main category: cs.AR**

**Keywords:** 神经渲染, 3D Gaussian Splatting, 移动计算, 软硬件协同设计, 计算冗余

> **TL;DR:** Lumina是一个软硬件协同设计的系统，通过S^2算法、辐射缓存和LuminCore加速器，显著提高了移动设备上3DGS神经渲染的效率和能效，同时保持了高质量。

<details>
  <summary>Details</summary>

**Motivation:** 3D Gaussian Splatting (3DGS) 在神经渲染方面取得了巨大进展，但对于当前的移动SoC来说，计算量仍然非常大。

**Method:** 提出Lumina，一个软硬件协同设计的系统。它集成了两种主要优化：一种新颖的S^2算法和一种辐射缓存（RC）机制。S^2算法利用渲染中的时间相干性来减少计算开销，而RC利用3DGS的颜色积分过程来降低密集光栅化计算的频率。此外，还提出了一个加速器架构LuminCore，以进一步加速缓存查找并解决光栅化中的基本效率低下问题。

**Result:** Lumina相对于移动Volta GPU实现了4.5倍的速度提升和5.3倍的能耗降低，在合成和真实世界数据集上只有微不足道的质量损失（< 0.2 dB峰值信噪比降低）。

**Conclusion:** Lumina通过软硬件协同设计，显著提高了移动设备上神经渲染的效率和能效，同时保持了高质量。

> **ai_Abstract:** 本文提出Lumina，一个针对移动设备神经渲染的软硬件协同设计系统，旨在解决3DGS在移动SoC上计算量过大的问题。Lumina包含S^2算法（利用时间相干性）和辐射缓存（RC）机制（减少光栅化频率），并配合LuminCore加速器。实验结果表明，Lumina在保持高质量的同时，实现了显著的速度提升和能耗降低。

> **摘要翻译:** 3D高斯溅射（3DGS）极大地推动了神经渲染的进展，但它在当今的移动SoC上仍然计算量巨大。为了解决这一挑战，我们提出了Lumina，一个软硬件协同设计的系统，它集成了两项主要优化：一种新颖的S^2算法和一种辐射缓存（RC）机制，以提高神经渲染的效率。S^2算法利用渲染中的时间相干性来减少计算开销，而RC则利用3DGS的颜色积分过程来降低密集光栅化计算的频率。结合这些技术，我们提出了一种加速器架构LuminCore，以进一步加速缓存查找并解决光栅化中固有的低效率问题。我们表明，Lumina相对于移动Volta GPU实现了4.5倍的速度提升和5.3倍的能耗降低，在合成和真实世界数据集上只有微不足道的质量损失（< 0.2 dB峰值信噪比降低）。

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [3] [Preprocessing Methods for Memristive Reservoir Computing for Image Recognition](https://arxiv.org/abs/2506.05588)
> *忆阻器储层计算用于图像识别的预处理方法*

*Rishona Daniels, Duna Wattad, Ronny Ronen, David Saad, Shahar Kvatinsky* | **Main category: cs.NE**

**Keywords:** 忆阻器储层计算, 预处理方法, 图像识别, 准确性, 能耗

> **TL;DR:** 本文系统比较了忆阻器储层计算的预处理方法，并提出了一种新的奇偶校验预处理方法，提高了图像识别的准确性，同时增加了适度的设备数量。

<details>
  <summary>Details</summary>

**Motivation:** 尽管忆阻器储层计算在图像识别中具有潜力，但其性能受输入预处理方法和储层大小的严重影响，且目前缺乏对这些因素影响的全面评估。

**Method:** 本文系统地比较了各种用于忆阻器储层计算系统的预处理方法，评估它们对准确性和能耗的影响。此外，还提出了一种基于奇偶校验的预处理方法。

**Result:** 研究发现，预处理方法对忆阻器储层计算系统的准确性和能耗有显著影响。提出的奇偶校验预处理方法将准确性提高了2-6%，同时仅需要适度增加设备数量。

**Conclusion:** 研究结果强调了明智的预处理策略对于提高忆阻器储层计算系统效率和可扩展性的重要性。

> **ai_Abstract:** 本文针对忆阻器储层计算在图像识别中面临的挑战，系统地比较了多种预处理方法对准确性和能耗的影响。研究提出了一种新的基于奇偶校验的预处理方法，该方法在仅需适度增加设备数量的情况下，将图像识别准确率提高了2-6%。研究强调了选择合适的预处理策略对于提升忆阻器储层计算系统性能的关键作用。

> **摘要翻译:** 储层计算（RC）作为一种高效的循环神经网络架构，因其简化的训练（仅需训练其最后一个感知器读出层）而备受关注。当用忆阻器实现时，RC系统受益于其动态特性，这使得它们非常适合储层构建。然而，在基于忆阻器的RC中实现高性能仍然具有挑战性，因为它关键地取决于输入预处理方法和储层大小。尽管兴趣日益增长，但仍然缺乏量化这些因素影响的全面评估。本文系统地比较了忆阻器RC系统的各种预处理方法，评估它们对准确性和能耗的影响。我们还提出了一种基于奇偶校验的预处理方法，该方法将准确性提高了2-6%，同时与其他方法相比，仅需要适度增加设备数量。我们的研究结果强调了明智的预处理策略对于提高忆阻器RC系统效率和可扩展性的重要性。

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory](https://arxiv.org/abs/2506.05994)
> *RETENTION：基于内容可寻址存储器的资源高效树集成模型加速*

*Yi-Chun Liao, Chieh-Lin Tsai, Yuan-Hao Chang, Camélia Slimani, Jalil Boukhobza, Tei-Wei Kuo* | **Main category: cs.LG**

**Keywords:** 树集成模型, 内容可寻址存储器, 模型加速, 资源效率, 迭代剪枝

> **TL;DR:** RETENTION框架通过迭代剪枝和树映射方案，显著降低了树集成模型在CAM上的内存需求，实现了高效加速。

<details>
  <summary>Details</summary>

**Motivation:** 树集成模型在结构化数据上表现优异，但传统加速器面临挑战。虽然内容可寻址存储器（CAM）为加速树集成模型提供了前景方案，但现有设计存在内存消耗过大和利用率低的问题。

**Method:** 本文提出了RETENTION端到端框架，旨在显著降低树集成模型推理所需的CAM容量。该框架包含两个核心部分：1. 迭代剪枝算法：针对基于bagging的模型（如随机森林）量身定制，具有新型剪枝准则，旨在最小化模型复杂性并确保可控的精度下降。2. 树映射方案：结合两种创新的数据放置策略，以减轻CAM中广泛使用“don't care”状态导致的内存冗余。

**Result:** 实验结果表明，仅实施树映射方案即可实现1.46倍至21.30倍的空间效率提升。完整的RETENTION框架则实现了4.35倍至207.12倍的改进，且精度损失小于3%。

**Conclusion:** RETENTION在降低内容可寻址存储器（CAM）容量需求方面非常有效，为树集成模型加速提供了资源高效的方向。

> **ai_Abstract:** 本文提出了RETENTION框架，旨在解决基于内容可寻址存储器（CAM）的树集成模型加速中存在的内存消耗过大和利用率低的问题。该框架通过迭代剪枝算法和创新的树映射方案，显著降低了CAM容量需求，实现了树集成模型的高效、资源节约型推理加速，并在实验中展现出显著的空间效率提升和整体性能改进。

> **摘要翻译:** 尽管深度学习在从非结构化数据中学习方面展示了卓越的能力，但现代的树集成模型在从结构化数据集中提取相关信息和学习方面仍然表现更优。虽然已经做出了一些努力来加速树集成模型，但模型的固有特性给传统加速器带来了重大挑战。最近利用内容可寻址存储器（CAM）的研究为加速树集成模型提供了一个有前景的解决方案，然而现有设计存在内存消耗过大和利用率低的问题。这项工作通过引入RETENTION解决了这些挑战，RETENTION是一个端到端框架，显著降低了树集成模型推理所需的CAM容量。我们提出了一种迭代剪枝算法，其具有针对基于bagging的模型（例如随机森林）量身定制的新型剪枝准则，该算法在确保可控精度下降的同时最小化模型复杂性。此外，我们提出了一种树映射方案，该方案结合了两种创新的数据放置策略，以减轻CAM中广泛使用“don't care”状态导致的内存冗余。实验结果表明，仅实施树映射方案即可实现1.46倍至21.30倍的空间效率提升，而完整的RETENTION框架则实现了4.35倍至207.12倍的改进，且精度损失小于3%。这些结果表明RETENTION在降低CAM容量需求方面非常有效，为树集成模型加速提供了资源高效的方向。

</details>
