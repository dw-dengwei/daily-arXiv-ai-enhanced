## Total Papers Today: 4
**Report Date:** 2025-06-09

<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation](https://arxiv.org/abs/2506.05566)
> *ScaleRTL: 利用推理数据和测试时计算扩展LLM以实现精确的RTL代码生成*

*Chenhui Deng, Yun-Da Tsai, Guan-Ting Liu, Zhongzhi Yu, Haoxing Ren* | **Main category: cs.AR**

**Keywords:** RTL代码生成, LLM, 推理数据, 测试时计算, ScaleRTL

> **TL;DR:** ScaleRTL通过高质量推理数据和测试时计算，显著提升LLM在RTL代码生成上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有LLM在RTL代码生成方面受限于高质量训练数据稀缺，且缺乏测试时扩展能力，无法进行推理。

**Method:** 引入ScaleRTL，首个用于RTL编码的推理LLM。方法包括：1. 整理包含长链式思维推理轨迹的高质量推理数据集（3.5B tokens）。2. 在此数据集上微调通用推理模型。3. 通过迭代反思和自我纠正的测试时扩展策略进一步提升性能。

**Result:** ScaleRTL在VerilogEval和RTLLM上实现了最先进的性能，在VerilogEval上超越18个基线模型高达18.4%，在RTLLM上超越12.7%。

**Conclusion:** ScaleRTL通过结合高质量推理数据和创新的测试时计算策略，显著提高了LLM在RTL代码生成任务上的准确性和性能，克服了数据瓶颈和推理能力不足的问题。

> **ai_Abstract:** 本文介绍了ScaleRTL，一种通过整合大规模高质量推理数据和创新的测试时计算策略来增强LLM在RTL代码生成方面性能的新方法。ScaleRTL通过微调通用推理模型并在测试阶段进行迭代自我纠正，在VerilogEval和RTLLM基准测试中取得了最先进的成果，显著优于现有方法。

> **摘要翻译:** 大型语言模型（LLM）的最新进展已使软件编码基准测试达到接近人类的水平，但由于高质量训练数据的稀缺性，其在RTL代码生成方面的有效性仍然有限。虽然之前的努力已经针对RTL任务对LLM进行了微调，但它们并未从根本上克服数据瓶颈，并且由于其非推理性质而缺乏对测试时扩展的支持。在这项工作中，我们引入了ScaleRTL，这是第一个用于RTL编码的推理LLM，它同时扩展了高质量推理数据和测试时计算。具体来说，我们整理了一组多样化的长链式思维推理轨迹，平均每个56K token，从而形成了一个包含3.5B token的数据集，捕获了丰富的RTL知识。在此语料库上对通用推理模型进行微调，产生了能够进行深度RTL推理的ScaleRTL。随后，我们通过一种新颖的测试时扩展策略进一步提升了ScaleRTL的性能，该策略通过迭代反思和自我纠正之前的推理步骤来扩展推理过程。实验结果表明，ScaleRTL在VerilogEval和RTLLM上取得了最先进的性能，在VerilogEval上超越18个竞争基线高达18.4%，在RTLLM上超越12.7%。

</details>


### [2] [Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy](https://arxiv.org/abs/2506.05682)
> *Lumina: 通过利用计算冗余实现实时移动神经渲染*

*Yu Feng, Weikai Lin, Yuge Cheng, Zihan Liu, Jingwen Leng, Minyi Guo, Chen Chen, Shixuan Sun, Yuhao Zhu* | **Main category: cs.AR**

**Keywords:** 神经渲染, 3D高斯泼溅, 移动计算, 硬件加速, 计算冗余

> **TL;DR:** Lumina是一个软硬件协同设计的系统，通过算法优化和专用加速器，显著提高了移动设备上3D高斯泼溅神经渲染的实时性和能效。

<details>
  <summary>Details</summary>

**Motivation:** 3D高斯泼溅（3DGS）在神经渲染方面取得了显著进展，但其在移动SoC上的计算需求仍然非常高，限制了其在移动设备上的应用。

**Method:** 本文提出了Lumina系统，该系统采用软硬件协同设计方法，包含两项主要优化：1. S^2算法：利用渲染中的时间相干性来减少计算开销。2. 辐射缓存（RC）机制：利用3DGS的颜色积分过程来降低密集光栅化计算的频率。此外，还提出了LuminCore加速器架构，以进一步加速缓存查找并解决光栅化中的基本低效率问题。

**Result:** Lumina在合成和真实世界数据集上，相比移动Volta GPU，实现了4.5倍的速度提升和5.3倍的能耗降低。同时，图像质量损失极小（峰值信噪比降低< 0.2 dB）。

**Conclusion:** Lumina通过软硬件协同设计，成功解决了3DGS在移动SoC上的计算瓶颈，显著提高了移动神经渲染的效率和实时性，同时保持了高质量的渲染效果。

> **ai_Abstract:** 本文介绍了Lumina，一个针对移动设备上3D高斯泼溅（3DGS）神经渲染的软硬件协同设计系统。为解决3DGS在移动SoC上计算量大的挑战，Lumina集成了S^2算法以利用时间相干性减少计算开销，以及辐射缓存（RC）机制以降低光栅化频率。此外，还设计了LuminCore加速器架构来优化缓存查找和光栅化效率。实验证明，Lumina在保持极低质量损失的前提下，实现了4.5倍的速度提升和5.3倍的能耗降低。

> **摘要翻译:** 3D高斯泼溅（3DGS）极大地推动了神经渲染的进步，但其在当今移动SoC上仍然计算量巨大。为了解决这一挑战，我们提出了Lumina，一个软硬件协同设计的系统，它集成了两项主要优化：一种新颖的S^2算法和一种辐射缓存（RC）机制，以提高神经渲染的效率。S^2算法利用渲染中的时间相干性来减少计算开销，而RC机制则利用3DGS的颜色积分过程来降低密集光栅化计算的频率。结合这些技术，我们提出了一种加速器架构LuminCore，以进一步加速缓存查找并解决光栅化中的根本性低效率问题。我们表明，Lumina在合成和真实世界数据集上，相比移动Volta GPU，实现了4.5倍的速度提升和5.3倍的能耗降低，同时质量损失微乎其微（峰值信噪比降低< 0.2 dB）。

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [3] [Preprocessing Methods for Memristive Reservoir Computing for Image Recognition](https://arxiv.org/abs/2506.05588)
> *忆阻器储层计算用于图像识别的预处理方法*

*Rishona Daniels, Duna Wattad, Ronny Ronen, David Saad, Shahar Kvatinsky* | **Main category: cs.NE**

**Keywords:** 忆阻器，储层计算，图像识别，预处理，奇偶校验

> **TL;DR:** 本文系统比较了忆阻器储层计算（RC）中不同的预处理方法，评估其对准确性和能耗的影响，并提出一种奇偶校验预处理方法，可提高准确性2-6%。

<details>
  <summary>Details</summary>

**Motivation:** 忆阻器储层计算（RC）在图像识别中表现出潜力，但其性能受到输入预处理方法和储层大小的严重影响。目前缺乏对这些因素影响的全面评估。

**Method:** 本文系统比较了忆阻器储层计算系统中各种预处理方法，评估了它们对准确性和能耗的影响。此外，还提出了一种基于奇偶校验的预处理方法。

**Result:** 研究发现，预处理策略对忆阻器RC系统的效率和可扩展性至关重要。提出的奇偶校验预处理方法在设备数量仅适度增加的情况下，将准确性提高了2-6%。

**Conclusion:**  informado的预处理策略对于提高忆阻器储层计算系统的效率和可扩展性至关重要。

> **ai_Abstract:** 本研究系统比较了忆阻器储层计算（RC）中不同的预处理方法，以评估其对图像识别准确性和能耗的影响。尽管忆阻器RC具有简化训练的优势，但其性能高度依赖于输入预处理和储层大小。本文提出了一种基于奇偶校验的预处理方法，在仅适度增加设备数量的情况下，将准确性提高了2-6%。研究结果强调了选择合适的预处理策略对于提高忆阻器RC系统效率和可扩展性的重要性。

> **摘要翻译:** 储层计算（RC）作为一种高效的循环神经网络架构，因其简化的训练（仅需训练其最后一个感知器读出层）而备受关注。当用忆阻器实现时，RC系统受益于其动态特性，这使得它们非常适合储层构建。然而，在基于忆阻器的RC中实现高性能仍然具有挑战性，因为它关键地取决于输入预处理方法和储层大小。尽管兴趣日益增长，但仍缺乏量化这些因素影响的全面评估。本文系统比较了忆阻器RC系统中的各种预处理方法，评估了它们对准确性和能耗的影响。我们还提出了一种基于奇偶校验的预处理方法，该方法将准确性提高了2-6%，同时与其他方法相比，所需的设备数量仅适度增加。我们的研究结果强调了知情预处理策略对于提高忆阻器RC系统效率和可扩展性的重要性。

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory](https://arxiv.org/abs/2506.05994)
> *RETENTION：基于内容可寻址存储器的资源高效树集成模型加速*

*Yi-Chun Liao, Chieh-Lin Tsai, Yuan-Hao Chang, Camélia Slimani, Jalil Boukhobza, Tei-Wei Kuo* | **Main category: cs.LG**

**Keywords:** 树集成模型加速, 内容可寻址存储器, 资源高效, 模型剪枝, 内存优化

> **TL;DR:** RETENTION通过优化CAM使用，显著降低了树集成模型加速的内存需求。

<details>
  <summary>Details</summary>

**Motivation:** 尽管深度学习在非结构化数据方面表现出色，但树集成模型在结构化数据上仍有优势。现有树模型加速方法面临挑战，特别是利用内容可寻址存储器（CAM）的方案存在内存消耗过大和利用率低的问题。

**Method:** 本文提出了RETENTION端到端框架，旨在显著降低树模型推理的CAM容量需求。该框架包含：1) 针对bagging模型（如随机森林）的迭代剪枝算法，该算法具有新颖的剪枝标准，可在确保可控精度下降的同时最小化模型复杂度；2) 树映射方案，该方案结合了两种创新的数据放置策略，以减轻CAM中广泛使用“不关心”状态引起的内存冗余。

**Result:** 实验结果表明，单独实施树映射方案可实现1.46倍至21.30倍的空间效率提升。完整的RETENTION框架可实现4.35倍至207.12倍的改进，且精度损失小于3%。

**Conclusion:** RETENTION在降低内容可寻址存储器（CAM）容量需求方面非常有效，为树集成模型加速提供了一个资源高效的方向。

> **ai_Abstract:** 本文介绍了RETENTION，一个旨在通过优化内容可寻址存储器（CAM）的使用来加速树集成模型的端到端框架。它通过提出迭代剪枝算法和创新的树映射方案，显著减少了CAM的容量需求。实验证明，RETENTION在保持低精度损失的同时，实现了显著的空间效率和整体性能提升，为资源高效的树模型加速提供了有效途径。

> **摘要翻译:** 尽管深度学习在从非结构化数据中学习方面展现了卓越的能力，但现代树集成模型在提取相关信息和从结构化数据集中学习方面仍然更胜一筹。虽然已经为加速树模型做出了多项努力，但模型的内在特性给传统加速器带来了巨大挑战。最近利用内容可寻址存储器（CAM）的研究为加速树模型提供了一个有前景的解决方案，但现有设计存在内存消耗过大和利用率低的问题。这项工作通过引入RETENTION来应对这些挑战，RETENTION是一个端到端框架，显著降低了树模型推理的CAM容量需求。我们提出了一种针对bagging模型（例如随机森林）的迭代剪枝算法，该算法具有新颖的剪枝标准，可在确保可控精度下降的同时最小化模型复杂度。此外，我们提出了一种树映射方案，该方案结合了两种创新的数据放置策略，以减轻CAM中广泛使用“不关心”状态引起的内存冗余。实验结果表明，单独实施树映射方案可实现1.46倍至21.30倍的空间效率提升，而完整的RETENTION框架可实现4.35倍至207.12倍的改进，且精度损失小于3%。这些结果表明，RETENTION在降低CAM容量需求方面非常有效，为树集成模型加速提供了一个资源高效的方向。

</details>
