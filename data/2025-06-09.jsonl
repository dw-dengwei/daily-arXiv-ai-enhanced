{"id": "2506.05566", "pdf": "https://arxiv.org/pdf/2506.05566", "abs": "https://arxiv.org/abs/2506.05566", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM."}
{"id": "2506.05682", "pdf": "https://arxiv.org/pdf/2506.05682", "abs": "https://arxiv.org/abs/2506.05682", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "categories": ["cs.AR"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets."}
{"id": "2506.05588", "pdf": "https://arxiv.org/pdf/2506.05588", "abs": "https://arxiv.org/abs/2506.05588", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "categories": ["cs.NE", "cs.AR", "cs.ET"], "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems."}
{"id": "2506.05994", "pdf": "https://arxiv.org/pdf/2506.05994", "abs": "https://arxiv.org/abs/2506.05994", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "categories": ["cs.LG", "cs.AR", "cs.ET"], "comment": null, "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration."}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05566v1"}
{"id": "2506.05682", "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/pdf/2506.05682v1"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "cate": "cs.NE", "url": "http://arxiv.org/pdf/2506.05588v1"}
{"id": "2506.05994", "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "CamÃ©lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.05994v1"}
