<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.PF](#cs.PF) [Total: 3]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation](https://arxiv.org/abs/2509.18337)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: CoRaCMG是一个基于检索增强的提交消息生成框架，通过检索相似的diff-message对来指导LLM生成更精确、信息量更大的提交消息，显著提升了多个评估指标的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的提交消息往往质量低下、模糊或不完整，限制了其实用性。虽然LLM在自动生成提交消息方面显示出潜力，但其性能仍有局限，需要提升生成消息的精确性和信息量。

Method: 提出了CoRaCMG框架，包含三个阶段：(1)检索相似的diff-message对；(2)将检索结果与查询diff结合成结构化提示；(3)通过LLM生成对应的提交消息。该方法使LLM能够从检索到的示例对中学习项目特定术语和写作风格。

Result: 实验表明CoRaCMG显著提升了多个LLM模型在BLEU、Rouge-L、METEOR和CIDEr四个指标上的性能。DeepSeek-R1在使用单个检索示例对时，BLEU和CIDEr分别提升了76%和71%；GPT-4o的BLEU提升了89%。性能提升在超过三个示例后趋于平稳。

Conclusion: 检索增强方法通过让模型从检索到的示例对中学习人类编写的提交消息的术语和写作风格，有效提升了提交消息生成的质量和准确性。

Abstract: Commit messages play a key role in documenting the intent behind code
changes. However, they are often low-quality, vague, or incomplete, limiting
their usefulness. Commit Message Generation (CMG) aims to automatically
generate descriptive commit messages from code diffs to reduce developers'
effort and improve message quality. Although recent advances in LLMs have shown
promise in automating CMG, their performance remains limited. This paper aims
to enhance CMG performance by retrieving similar diff-message pairs to guide
LLMs to generate commit messages that are more precise and informative. We
proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message
Generation, structured in three phases: (1) Retrieve: retrieving the similar
diff-message pairs; (2) Augment: combining them with the query diff into a
structured prompt; and (3) Generate: generating commit messages corresponding
to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific
terminologies and writing styles from the retrieved diff-message pairs, thereby
producing high-quality commit messages. We evaluated our method on various
LLMs, including closed-source GPT models and open-source DeepSeek models.
Experimental results show that CoRaCMG significantly boosts LLM performance
across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,
DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when
augmented with a single retrieved example pair. After incorporating the single
example pair, GPT-4o achieves the highest improvement rate, with BLEU
increasing by 89%. Moreover, performance gains plateau after more than three
examples are used, indicating diminishing returns. Further analysis shows that
the improvements are attributed to the model's ability to capture the
terminologies and writing styles of human-written commit messages from the
retrieved example pairs.

</details>


### [2] [Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts](https://arxiv.org/abs/2509.18361)
*Daye Nam,Malgorzata Salawa,Satish Chandra*

Main category: cs.SE

TL;DR: 使用开发者提示的情感分析来评估开发者对AI助手的满意度，该方法比显式用户反馈率高13倍以上


<details>
  <summary>Details</summary>
Motivation: 评估开发者对对话式AI助手的满意度很重要但具有挑战性，用户研究不可扩展，而大规模定量信号往往太浅或稀疏不可靠

Method: 分析372名专业开发者的工业使用日志，通过情感分析识别开发者提示中的隐式满意度信号

Result: 该方法能在约8%的交互中识别信号，比显式用户反馈率高13倍以上，即使使用现成的情感分析方法也具有合理准确性

Conclusion: 这种新方法可以补充现有反馈渠道，为大规模理解开发者体验开辟新方向

Abstract: Evaluating developer satisfaction with conversational AI assistants at scale
is critical but challenging. User studies provide rich insights, but are
unscalable, while large-scale quantitative signals from logs or in-product
ratings are often too shallow or sparse to be reliable. To address this gap, we
propose and evaluate a new approach: using sentiment analysis of developer
prompts to identify implicit signals of user satisfaction. With an analysis of
industrial usage logs of 372 professional developers, we show that this
approach can identify a signal in ~8% of all interactions, a rate more than 13
times higher than explicit user feedback, with reasonable accuracy even with an
off-the-shelf sentiment analysis approach. This new practical approach to
complement existing feedback channels would open up new directions for building
a more comprehensive understanding of the developer experience at scale.

</details>


### [3] [SC2Tools: StarCraft II Toolset and Dataset API](https://arxiv.org/abs/2509.18454)
*Andrzej Białecki,Piotr Białecki,Piotr Sowiński,Mateusz Budziak,Jan Gajewski*

Main category: cs.SE

TL;DR: SC2Tools是一个用于星际争霸2数据分析的工具集，旨在简化数据收集和处理工作，为游戏和电竞研究提供便利。


<details>
  <summary>Details</summary>
Motivation: 计算机游戏作为完全受控的模拟环境，在强化学习研究中发挥重要作用。游戏和电竞领域需要大规模AI和机器学习解决方案，但数据收集和预处理工作繁重，阻碍了技术能力较弱的研究人员参与。

Method: 开发SC2Tools工具集，包含多个子模块，采用模块化结构设计，支持星际争霸2及其他类型数据的处理。提供PyTorch和PyTorch Lightning API接口便于数据访问。

Result: 创建了迄今为止最大的星际争霸2比赛数据集，工具集不仅限于星际争霸2，还可用于其他类型数据的数据集创建。

Conclusion: 减轻数据收集、预处理和自定义代码开发的负担对于技术能力较弱的研究人员参与游戏和电竞研究至关重要。该工具为星际争霸2实验工作流程的标准化提供了基础工作。

Abstract: Computer games, as fully controlled simulated environments, have been
utilized in significant scientific studies demonstrating the application of
Reinforcement Learning (RL). Gaming and esports are key areas influenced by the
application of Artificial Intelligence (AI) and Machine Learning (ML) solutions
at scale. Tooling simplifies scientific workloads and is essential for
developing the gaming and esports research area.
  In this work, we present ``SC2Tools'', a toolset containing multiple
submodules responsible for working with, and producing larger datasets. We
provide a modular structure of the implemented tooling, leaving room for future
extensions where needed. Additionally, some of the tools are not StarCraft~2
exclusive and can be used with other types of data for dataset creation.
  The tools we present were leveraged in creating one of the largest
StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch
Lightning application programming interface (API) for easy access to the data.
  We conclude that alleviating the burden of data collection, preprocessing,
and custom code development is essential for less technically proficient
researchers to engage in the growing gaming and esports research area. Finally,
our solution provides some foundational work toward normalizing experiment
workflow in StarCraft~2

</details>


### [4] [Locking Down Science Gateways](https://arxiv.org/abs/2509.18548)
*Steven R Brandt,Max Morris,Patrick Diehl,Christopher Bowen,Jacob Tucker,Lauren Bristol,Golden G. Richard III*

Main category: cs.SE

TL;DR: 本文探讨了Linux内核新安全特性Landlock在科学网关应用中的实用性，通过修改三个成熟科学代码并实现基于Landlock的安全防护机制


<details>
  <summary>Details</summary>
Motivation: 科学网关应用在启动MPI时需要网络访问，但为了安全考虑，在读取用户提供的参数文件前应该移除这些权限。Landlock提供了比Seccomp更精细的资源访问控制能力

Method: 修改Einstein Toolkit、Octo-Tiger和FUKA三个科学代码，使用Landlock进行安全锁定，并实现一个完全功能的FUKA科学网关，依赖Landlock而非用户认证来确保安全

Result: 成功验证了Landlock在科学计算应用中的实用性，实现了安全的资源访问控制机制

Conclusion: Landlock是科学网关应用中有效的安全解决方案，能够提供细粒度的资源访问控制，替代传统的用户认证方式

Abstract: The most recent Linux kernels have a new feature for securing applications:
Landlock. Like Seccomp before it, Landlock makes it possible for a running
process to give up access to resources. For applications running as Science
Gateways, network access is required while starting up MPI, but for the sake of
security, it should be taken away prior to the reading of user-supplied
parameter files. We explore the usefulness of Landlock by modifying and locking
down three mature scientific codes: The Einstein Toolkit (a code that studies
the dynamics of relativistic astrophysics, e.g. neutron star collisions),
Octo-Tiger (a code for studying the dynamics of non-relativistic astrophysics,
e.g. white dwarfs), and FUKA (an initial data solver for relativistic codes).
Finally, we implement a fully-functioning FUKA science gateway that relies on
Landlock (instead of user authentication) for security.

</details>


### [5] [SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement](https://arxiv.org/abs/2509.18808)
*Zexun Zhan,Shuzheng Gao,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TL;DR: SR-Eval是一个专门评估大语言模型在逐步需求细化下迭代代码生成能力的基准测试，涵盖Python和Java的函数级和仓库级任务，结果显示当前模型在此任务上表现仍不理想。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要将代码生成视为静态单轮任务，忽略了真实软件开发中的逐步需求变化和迭代工作流程，这限制了对大语言模型支持真实开发工作流程能力的理解。

Method: 采用多智能体需求生成方法模拟开发过程，从最终需求恢复多轮交互过程，并使用语义感知的判别性测试用例生成组件确保每轮评估的一致性和判别性。

Result: 评估11个代表性大语言模型后发现，在逐步需求细化下的迭代代码生成仍然极具挑战性：最佳模型在函数级任务上仅达到22.67%完成率，在仓库级任务上为20.00%。

Conclusion: 迭代代码生成在逐步需求细化下仍然是一个高度挑战性的任务，提示策略对性能有显著影响，需要开发更先进的方法来提升模型在此类任务上的表现。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation. However, existing benchmarks mainly formalize the task as a static,
single-turn problem, overlooking the stepwise requirement changes and iterative
workflows in real-world software development. This mismatch limits the
understanding of how well LLMs can support real-world development workflows.
Constructing such iterative benchmarks is challenging due to the lack of public
interaction traces and the difficulty of creating discriminative, turn-specific
test cases.
  To bridge this gap, we present SR-Eval, a benchmark specifically designed to
assess LLMs on iterative code generation under Stepwise requirements
Refinement. SR-Eval spans both function-level and repository-level tasks in
Python and Java, enabling fine-grained and progressive evaluation across
evolving requirements. The construction of SR-Eval follows a carefully designed
pipeline that first leverages a multi-agent-based requirement generation method
to simulate the development process and recover the multi-round interaction
process from final requirements, then employs a semantic-aware discriminative
test case generation component to ensure discriminative and consistent
evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857
questions at both function and repository levels. Using SR-Eval, we evaluate 11
representative LLMs with three prompting strategies that simulate different
usage patterns. Results show that iterative code generation under stepwise
requirement refinement remains highly challenging: the best-performing model
achieves only 22.67% completion rate on function-level tasks and 20.00% on
repository-level tasks. We further observe that prompting strategies
substantially influence performance, highlighting the need for the development
of advanced methods.

</details>


### [6] [On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language](https://arxiv.org/abs/2509.19136)
*Sébastien Salva,Redha Taguelmimt*

Main category: cs.SE

TL;DR: 本文研究了使用大型语言模型（LLM）代理直接执行自然语言测试用例的可行性，重点关注测试用例的不健全性和执行一致性问题，并提出了一种带有防护机制的算法来提高GUI测试的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的手写可执行测试脚本成本高且难以维护，而自然语言测试用例的执行存在不健全性和执行不一致的问题，需要解决这些挑战来提升GUI测试的效率和可靠性。

Method: 提出了一种带有防护机制的算法，使用专门的代理动态验证每个测试步骤的正确执行，并引入了评估LLM测试执行能力的指标和执行一致性的量化方法。

Result: 实验评估显示，Meta Llama 3.1 70B模型在自然语言测试用例执行方面表现出可接受的能力，执行一致性高于3-sigma水平。

Conclusion: 当前LLM代理在GUI测试中具有潜力但仍存在局限性，提出的方法和指标为自然语言测试用例的执行提供了可行的解决方案和评估标准。

Abstract: The use of natural language (NL) test cases for validating graphical user
interface (GUI) applications is emerging as a promising direction to manually
written executable test scripts, which are costly to develop and difficult to
maintain. Recent advances in large language models (LLMs) have opened the
possibility of the direct execution of NL test cases by LLM agents. This paper
investigates this direction, focusing on the impact on NL test case unsoundness
and on test case execution consistency. NL test cases are inherently unsound,
as they may yield false failures due to ambiguous instructions or unpredictable
agent behaviour. Furthermore, repeated executions of the same NL test case may
lead to inconsistent outcomes, undermining test reliability. To address these
challenges, we propose an algorithm for executing NL test cases with guardrail
mechanisms and specialised agents that dynamically verify the correct execution
of each test step. We introduce measures to evaluate the capabilities of LLMs
in test execution and one measure to quantify execution consistency. We propose
a definition of weak unsoundness to characterise contexts in which NL test case
execution remains acceptable, with respect to the industrial quality levels Six
Sigma. Our experimental evaluation with eight publicly available LLMs, ranging
from 3B to 70B parameters, demonstrates both the potential and current
limitations of current LLM agents for GUI testing. Our experiments show that
Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case
execution with high execution consistency (above the level 3-sigma). We provide
prototype tools, test suites, and results.

</details>


### [7] [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](https://arxiv.org/abs/2509.19185)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对39个开源AI代理框架和439个代理应用的大规模实证研究发现，开发者主要使用传统测试方法（如负面测试和成员测试）来管理基础模型的不确定性，而非确定性组件消耗了超过70%的测试工作量，而关键的触发组件（提示词）测试仅占约1%，存在严重盲区。


<details>
  <summary>Details</summary>
Motivation: 基础模型AI代理的非确定性和不可重现性给测试和质量保证带来挑战，现有研究主要关注任务级评估，缺乏对开发过程中内部正确性验证的理解。

Method: 通过对39个开源代理框架和439个代理应用进行大规模实证研究，识别出十种不同的测试模式，并将这些模式映射到代理框架和应用的典型架构组件。

Result: 研究发现传统测试模式被广泛采用（如负面测试和成员测试），而新型代理特定方法（如DeepEval）使用率很低（约1%）。测试工作量存在根本性倒置：确定性组件消耗70%以上测试工作量，而基于基础模型的计划主体测试不到5%，触发组件测试仅占1%。

Conclusion: 研究揭示了在非确定性适应方面的理性但不完整的调整，建议框架开发者改进新型测试方法的支持，应用开发者应采用提示回归测试，研究人员应探索采用障碍，以构建更健壮可靠的AI代理。

Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across
diverse domains, but their inherent non-determinism and non-reproducibility
pose testing and quality assurance challenges. While recent benchmarks provide
task-level evaluations, there is limited understanding of how developers verify
the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of
testing practices in the AI agent ecosystem, analyzing 39 open-source agent
frameworks and 439 agentic applications. We identify ten distinct testing
patterns and find that novel, agent-specific methods like DeepEval are seldom
used (around 1%), while traditional patterns like negative and membership
testing are widely adapted to manage FM uncertainty. By mapping these patterns
to canonical architectural components of agent frameworks and agentic
applications, we uncover a fundamental inversion of testing effort:
deterministic components like Resource Artifacts (tools) and Coordination
Artifacts (workflows) consume over 70% of testing effort, while the FM-based
Plan Body receives less than 5%. Crucially, this reveals a critical blind spot,
as the Trigger component (prompts) remains neglected, appearing in around 1% of
all tests.
  Our findings offer the first empirical testing baseline in FM-based agent
frameworks and agentic applications, revealing a rational but incomplete
adaptation to non-determinism. To address it, framework developers should
improve support for novel testing methods, application developers must adopt
prompt regression testing, and researchers should explore barriers to adoption.
Strengthening these practices is vital for building more robust and dependable
AI agents.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [8] [Static Estimation of Reuse Profiles for Arrays in Nested Loops](https://arxiv.org/abs/2509.18684)
*Abdur Razzak,Atanu Barai,Nandakishore Santhi,Abdel-Hameed A. Badawy*

Main category: cs.PF

TL;DR: 提出了一种静态分析框架，用于预测嵌套循环程序中数组引用的重用模式，无需运行时信息即可估算重用距离和缓存命中率


<details>
  <summary>Details</summary>
Motivation: 传统动态内存追踪方法耗时耗资源，不适合早期优化和大规模应用；现有静态预测方法准确性不足，特别是在处理嵌套循环中的数组访问时

Method: 通过分析循环边界、小规模问题访问模式和预测方程，在编译时预测数组访问模式、重用距离和缓存命中率

Result: 与主流动态分析工具PARDA相比，静态预测器达到相当精度，同时分析速度提升数个数量级

Conclusion: 该工作为动态重用分析提供了实用替代方案，为集成到编译器和静态性能建模工具铺平了道路

Abstract: Efficient memory access patterns play a crucial role in determining the
overall performance of applications by exploiting temporal and spatial
locality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is
a widely used metric to quantify temporal locality, measuring the distance
between consecutive accesses to the same memory location. Traditionally,
calculating RDH requires program execution and memory trace collection to
obtain dynamic memory access behavior. This trace collection is often
time-consuming, resource-intensive, and unsuitable for early-stage optimization
or large-scale applications. Static prediction, on the other hand, offers a
significant speedup in estimating RDH and cache hit rates. However, these
approaches lack accuracy, since the predictions come without running the
program and knowing the complete memory access pattern, more specifically when
arrays are used inside nested loops. This paper presents a novel static
analysis framework for predicting the reuse profiles of array references in
programs with nested loop structures, without requiring any runtime
information. By analyzing loop bounds, access patterns in smaller problem
sizes, and predictive equations, our method predicts access patterns of arrays
and estimates reuse distances and cache hit rate at compile time. This paper
extends our previous study by incorporating more analysis and improving
prediction by addressing previously unhandled reuse patterns. We evaluate our
technique against a widely accepted traditional trace-driven profiling tool,
Parallel Reuse Distance Analysis (PARDA). The results demonstrate that our
static predictor achieves comparable accuracy while offering
orders-of-magnitude improvement in the analysis speed. This work offers a
practical alternative to dynamic reuse profiling and paves the way for
integration into compilers and static performance modeling tools.

</details>


### [9] [Confidential LLM Inference: Performance and Cost Across CPU and GPU TEEs](https://arxiv.org/abs/2509.18886)
*Marcin Chrapek,Marcin Copik,Etienne Mettaz,Torsten Hoefler*

Main category: cs.PF

TL;DR: 本文研究了在CPU和GPU可信执行环境(TEE)中运行大型语言模型推理的可行性，发现现代TEE技术能够以较低的性能开销(CPU<10%，GPU4-8%)实现端到端的机密LLM推理


<details>
  <summary>Details</summary>
Motivation: 随着LLM处理机密输入并在昂贵的专有数据集上进行微调，其安全需求阻碍了在医疗和金融等隐私敏感行业的应用，需要寻找解决方案来弥合这一差距

Method: 使用Intel TDX和SGX CPU TEE以及NVIDIA H100机密计算GPU TEE，评估完整的Llama2推理管道(7B、13B、70B)，并通过高级矩阵扩展(AMX)进行加速

Result: CPU TEE在各种数据类型、批大小和输入长度下施加低于10%的吞吐量和20%的延迟开销，AMX进一步减少了这些开销；GPU TEE的吞吐量惩罚为4-8%，随着批处理和输入大小的增长而减少

Conclusion: 通过比较性能、成本和安全性权衡，CPU TEE可以比GPU TEE更具成本效益或更安全，这是首个全面展示现代TEE在CPU和GPU上实现机密LLM的性能和实用性的工作

Abstract: Large Language Models (LLMs) are increasingly deployed on converged Cloud and
High-Performance Computing (HPC) infrastructure. However, as LLMs handle
confidential inputs and are fine-tuned on costly, proprietary datasets, their
heightened security requirements slow adoption in privacy-sensitive sectors
such as healthcare and finance. We investigate methods to address this gap and
propose Trusted Execution Environments (TEEs) as a solution for securing
end-to-end LLM inference. We validate their practicality by evaluating these
compute-intensive workloads entirely within CPU and GPU TEEs. On the CPU side,
we conduct an in-depth study running full Llama2 inference pipelines (7B, 13B,
70B) inside Intel's TDX and SGX, accelerated by Advanced Matrix Extensions
(AMX). We derive 12 insights, including that across various data types, batch
sizes, and input lengths, CPU TEEs impose under 10% throughput and 20% latency
overheads, further reduced by AMX. We run LLM inference on NVIDIA H100
Confidential Compute GPUs, contextualizing our CPU findings and observing
throughput penalties of 4-8% that diminish as batch and input sizes grow. By
comparing performance, cost, and security trade-offs, we show how CPU TEEs can
be more cost-effective or secure than their GPU counterparts. To our knowledge,
our work is the first to comprehensively demonstrate the performance and
practicality of modern TEEs across both CPUs and GPUs for enabling confidential
LLMs (cLLMs).

</details>


### [10] [Glass-Box Analysis for Computer Systems: Transparency Index, Shapley Attribution, and Markov Models of Branch Prediction](https://arxiv.org/abs/2509.19027)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.PF

TL;DR: 本文提出了三种玻璃盒分析工具：GTI量化性能方差可解释比例，ETD使用Shapley值进行吞吐量归因，以及分支预测器的精确马尔可夫分析框架。


<details>
  <summary>Details</summary>
Motivation: 为了对计算机系统进行透明化分析，需要开发能够量化内部特征对性能影响的方法论和工具。

Method: 1) 开发Glass-Box Transparency Index (GTI)量化性能方差解释比例；2) 使用Shapley值的Explainable Throughput Decomposition (ETD)进行吞吐量归因；3) 建立分支预测器的精确马尔可夫分析框架。

Result: 提出了三种具有理论保证的分析工具：GTI带有边界和置信区间，ETD提供误差保证和凸性间隙边界，分支预测分析框架包含闭式解和可识别性定理。

Conclusion: 这些工具为计算机系统提供了系统化的玻璃盒分析方法，具有理论严谨性和实际应用价值。

Abstract: We formalize glass-box analysis for computer systems and introduce three
principled tools. First, the Glass-Box Transparency Index (GTI) quantifies the
fraction of performance variance explainable by internal features and comes
equipped with bounds, invariances, cross-validated estimation, and bootstrap
confidence intervals. Second, Explainable Throughput Decomposition (ETD) uses
Shapley values to provide an efficiency-preserving attribution of throughput,
together with non-asymptotic Monte Carlo error guarantees and convexity
(Jensen) gap bounds. Third, we develop an exact Markov analytic framework for
branch predictors, including a closed-form misprediction rate for a two-bit
saturating counter under a two-state Markov branch process and its i.i.d.
corollary. Additionally, we establish an identifiability theorem for recovering
event rates from aggregated hardware counters and provide stability bounds
under noise.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [11] [Bridging Simulation and Silicon: A Study of RISC-V Hardware and FireSim Simulation](https://arxiv.org/abs/2509.18472)
*Atanu Barai,Kamalavasan Kamalakkannan,Patrick Diehl,Maxim Moraru,Jered Dominguez-Trujillo,Howard Pritchard,Nandakishore Santhi,Farzad Fatollahi-Fard,Galen Shipman*

Main category: cs.DC

TL;DR: 本研究评估了FireSim模拟器对RISC-V处理器的性能预测准确性，通过对比模拟结果与真实硬件测试发现存在显著差异


<details>
  <summary>Details</summary>
Motivation: RISC-V处理器在高性能计算领域展现出潜力，但缺乏对FireSim模拟器性能预测准确性的系统评估

Method: 使用FireSim框架模拟商用RISC-V单板计算机和桌面级CPU，通过基准测试比较单核和四核配置下的运行时行为，并使用代表性应用和LAMMPS分子动力学代码进行评估

Result: FireSim能够提供有价值的架构性能趋势洞察，但模拟运行时间与实测结果之间存在差异

Conclusion: 模拟环境的固有局限性和CPU制造商提供的详细性能规格有限，阻碍了精确的配置匹配，导致性能预测存在偏差

Abstract: RISC-V ISA-based processors have recently emerged as both powerful and
energy-efficient computing platforms. The release of the MILK-V Pioneer marked
a significant milestone as the first desktop-grade RISC-V system. With
increasing engagement from both academia and industry, such platforms exhibit
strong potential for adoption in high-performance computing (HPC) environments.
  The open-source, FPGA-accelerated FireSim framework has emerged as a flexible
and scalable tool for architectural exploration, enabling simulation of various
system configurations using RISC-V cores. Despite its capabilities, there
remains a lack of systematic evaluation regarding the feasibility and
performance prediction accuracy of FireSim when compared to physical hardware.
  In this study, we address this gap by modeling a commercially available
single-board computer and a desktop-grade RISC-V CPU within FireSim. To ensure
fidelity between simulation and real hardware, we first measure the performance
of a series of benchmarks to compare runtime behavior under single-core and
four-core configurations. Based on the closest matching simulation parameters,
we subsequently evaluate performance using a representative mini-application
and the LAMMPS molecular dynamics code.
  Our findings indicate that while FireSim provides valuable insights into
architectural performance trends, discrepancies remain between simulated and
measured runtimes. These deviations stem from both inherent limitations of the
simulation environment and the restricted availability of detailed performance
specifications from CPU manufacturers, which hinder precise configuration
matching.

</details>


### [12] [6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and Non-Linear Precoder Design for Radio Access Networks](https://arxiv.org/abs/2509.18735)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 6G Twin是首个端到端AI原生无线接入网络设计，通过神经高斯无线电场压缩CSI获取、持续信道预测和能量最优非线性预编码器，实现100倍导频开销削减、毫秒级闭环操作和4-10倍能量效率提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统RAN系统中高导频开销、移动性管理困难和能量效率低下的问题，需要开发一个统一的AI原生框架来实现实时CSI获取、鲁棒跟踪和最优能量效率。

Method: 采用神经高斯无线电场(GRF)压缩CSI获取，持续学习器处理移动性和小区切换，以及minPMAC非线性预编码器进行能量最优预编码设计。

Result: 实现100倍导频开销削减、1.1ms推理时间、小于2分钟现场训练、10dB以上NMSE改进、4-10倍能量降低和5倍数据率提升。

Conclusion: 6G Twin提供了一个实用的GPU就绪框架，在3GPP标准设置下实现了实时CSI、动态网络中的鲁棒跟踪以及最先进的吞吐量-能量权衡。

Abstract: This work introduces 6G Twin, the first end-to-end artificial intelligence
(AI)-native radio access network (RAN) design that unifies (i) neural Gaussian
Radio Fields (GRF) for compressed channel state information (CSI) acquisition,
(ii) continual channel prediction with handover persistence, and (iii) an
energy-optimal nonlinear precoder (minPMAC). GRF replaces dense pilots with a
sparse Gaussian field, cutting pilot overhead by about 100x while delivering
1.1 ms inference and less than 2 minutes on-site training, thus enabling
millisecond-scale closed-loop operation. A replay-driven continual learner
sustains accuracy under mobility and cell transitions, improving channel
normalized mean square error (NMSE) by more than 10 dB over frozen predictors
and an additional 2-5 dB over uniform replay, thereby stabilizing performance
across UMi/UMa handovers. Finally, minPMAC solves a convex, order-free MAC
precoder design that recovers the globally optimal order from Broadcast Channel
(BC) duals and minimizes transmit energy subject to minimum-rate guarantees,
achieving 4-10 times lower energy (scenario dependent) with monotonically
increasing bits per joule as SNR grows. This translates to up to 5 times higher
data rate at comparable power or the same rates at substantially lower power.
Together, these components form a practical, GPU-ready framework that attains
real-time CSI, robust tracking in dynamic networks with efficient handovers,
and state-of-the-art throughput-energy tradeoffs under 3GPP-style settings.

</details>


### [13] [On The Reproducibility Limitations of RAG Systems](https://arxiv.org/abs/2509.18869)
*Baiqiang Wang,Dongfang Zhao,Nathan R Tallent,Luanzheng Guo*

Main category: cs.DC

TL;DR: ReproRAG是一个用于系统评估向量检索系统可重现性的基准测试框架，通过分析嵌入模型、精度、检索算法等多个因素对RAG系统不确定性的影响


<details>
  <summary>Details</summary>
Motivation: RAG在科学工作流中应用日益广泛，但其检索组件的非确定性经常影响可靠性，需要系统方法来量化和改进可重现性

Method: 提出ReproRAG框架，使用精确匹配率、Jaccard相似度、Kendall's Tau等指标，全面分析嵌入模型、精度、检索算法、硬件配置和分布式环境等因素

Result: 大规模实证研究显示不同嵌入模型对RAG可重现性有显著影响，框架有效揭示了可重现性与性能之间的权衡关系

Conclusion: 开源的ReproRAG框架为研究人员和工程师提供了验证部署、基准测试可重现性和做出明智设计决策的工具，有助于构建更可信的科学AI

Abstract: Retrieval-Augmented Generation (RAG) is increasingly employed in generative
AI-driven scientific workflows to integrate rapidly evolving scientific
knowledge bases, yet its reliability is frequently compromised by
non-determinism in their retrieval components. This paper introduces ReproRAG,
a comprehensive benchmarking framework designed to systematically measure and
quantify the reproducibility of vector-based retrieval systems. ReproRAG
investigates sources of uncertainty across the entire pipeline, including
different embedding models, precision, retrieval algorithms, hardware
configurations, and distributed execution environments. Utilizing a suite of
metrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the
proposed framework effectively characterizes the trade-offs between
reproducibility and performance. Our large-scale empirical study reveals
critical insights; for instance, we observe that different embedding models
have remarkable impact on RAG reproducibility. The open-sourced ReproRAG
framework provides researchers and engineers productive tools to validate
deployments, benchmark reproducibility, and make informed design decisions,
thereby fostering more trustworthy AI for science.

</details>


### [14] [TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources via Distributed Reinforcement Learning](https://arxiv.org/abs/2509.18957)
*Shengye Song,Minxian Xu,Kan Hu,Wenxia Guo,Kejiang Ye*

Main category: cs.DC

TL;DR: TD3-Sched是基于TD3算法的分布式强化学习调度器，在云边环境中实现CPU和内存资源的连续控制，相比基线方法显著降低延迟并提高SLO合规性


<details>
  <summary>Details</summary>
Motivation: 解决云边系统中边缘节点运行延迟敏感工作负载时的资源调度挑战，避免集中式调度器的性能瓶颈和用户体验下降问题

Method: 基于Twin Delayed Deep Deterministic Policy Gradient (TD3)算法的分布式强化学习方法，用于CPU和内存分配的连续控制

Result: 在真实云边测试平台上，相比其他强化学习和基于规则的基线方法，相同负载下延迟降低17.9%-38.6%，高负载下降低16%-31.6%，SLO违规率仅为0.47%

Conclusion: TD3-Sched在容器化云边环境中展现出更快的收敛速度、更低的延迟和更稳定的性能，同时保持了服务质量

Abstract: Resource scheduling in cloud-edge systems is challenging as edge nodes run
latency-sensitive workloads under tight resource constraints, while existing
centralized schedulers can suffer from performance bottlenecks and user
experience degradation. To address the issues of distributed decisions in
cloud-edge environments, we present TD3-Sched, a distributed reinforcement
learning (DRL) scheduler based on Twin Delayed Deep Deterministic Policy
Gradient (TD3) for continuous control of CPU and memory allocation, which can
achieve optimized decisions for resource provisioning under dynamic workloads.
On a realistic cloud-edge testbed with SockShop application and Alibaba traces,
TD3-Sched achieves reductions of 17.9% to 38.6% in latency under same loads
compared with other reinforcement-learning and rule-based baselines, and 16% to
31.6% under high loads. TD3-Sched also shows superior Service Level Objective
(SLO) compliance with only 0.47% violations. These results indicate faster
convergence, lower latency, and more stable performance while preserving
service quality in container-based cloud-edge environment compared with the
baselines.

</details>


### [15] [Scheduler-Driven Job Atomization](https://arxiv.org/abs/2509.19086)
*Michal Konopa,Jan Fesl,Ladislav Beránek*

Main category: cs.DC

TL;DR: 提出SJA调度器驱动作业原子化新范式，通过调度器与作业双向交互，将作业分解为适合空闲时间窗口的子作业，提高GPU集群利用率


<details>
  <summary>Details</summary>
Motivation: 现代GPU集群（特别是基于NVIDIA MIG架构）存在效率低下问题，作业被视为刚性不可分割块，静态峰值内存估计导致碎片化、利用率低和作业拒绝

Method: SJA建立调度器与作业的双向交互：调度器公布可用执行间隙，作业响应表示兴趣，调度器基于分配策略选择作业生成定制子作业

Result: 该方法主动在执行前塑造工作负载，避免昂贵的状态转移和不可预测中断，旨在提高GPU利用率、减少等待时间和最小化迁移开销

Conclusion: SJA是一种新的调度范式，通过实时将作业与机会对齐来确保每个子作业的正确性，本文作为概念论文介绍了该范式的构建模块和未来研究方向

Abstract: Modern GPU clusters, particularly those built on NVIDIA's Multi-Instance GPU
(MIG) architecture, often suffer from inefficiencies because jobs are treated
as rigid, indivisible blocks that occupy a fixed slice until completion. The
reliance on static peak memory estimates exacerbates fragmentation,
underutilization, and job rejections. We propose Scheduler-Driven Job
Atomization (SJA), a new paradigm that establishes a bidirectional interaction
between scheduler and jobs. In SJA, the scheduler advertises available
execution gaps, and jobs respond by signaling interest if they can potentially
generate a subjob that fits the offered time-capacity window. The scheduler may
collect multiple signals for the same slot and, based on its allocation policy
(e.g., fairness, efficiency, or SLA priorities), selects which job is granted
the slot. Only then does the chosen job materialize a safe, self-contained
subjob tailored to that opportunity. Unlike migration or preemption, SJA
proactively shapes workloads before execution, thereby avoiding costly state
transfers and unpredictable interruptions. It aims to increase GPU utilization,
reduce wait times, and minimize migration overhead by aligning jobs with
opportunities in real time, ensuring that each admitted subjob is correct by
construction. This paper is presented as a concept paper: it introduces the
paradigm, defines its building blocks, and outlines future research directions,
rather than offering a full experimental evaluation.

</details>


### [16] [In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow Patterns](https://arxiv.org/abs/2509.19150)
*Harikrishna Tummalapalli,Riccardo Balin,Christine M. Simpson,Andrew Park,Aymen Alsaadi,Andrew E. Shao,Wesley Brewer,Shantenu Jha*

Main category: cs.DC

TL;DR: SimAI-Bench工具用于评估AI-模拟耦合工作流的数据传输性能，在Aurora超算上测试了两种常见模式：一对一和一对多工作流，发现不同模式下最优的数据传输策略不同


<details>
  <summary>Details</summary>
Motivation: 随着AI-模拟耦合工作流成为HPC主要负载且复杂度不断增加，需要新工具来进行性能分析和原型开发

Method: 使用SimAI-Bench工具在Aurora超算上对两种常见耦合模式（一对一和一对多）进行数据传输性能基准测试，比较了多种数据传输策略

Result: 一对一模式下，节点本地和DragonHPC数据暂存策略性能优于Redis和Lustre文件系统；一对多模式下，文件系统是测试策略中的最优解，但数据传输随集成规模增长成为主要瓶颈

Conclusion: 不同耦合工作流模式需要不同的数据传输策略优化，SimAI-Bench是评估和原型开发这些工作流的有用工具

Abstract: Coupled AI-Simulation workflows are becoming the major workloads for HPC
facilities, and their increasing complexity necessitates new tools for
performance analysis and prototyping of new in-situ workflows. We present
SimAI-Bench, a tool designed to both prototype and evaluate these coupled
workflows. In this paper, we use SimAI-Bench to benchmark the data transport
performance of two common patterns on the Aurora supercomputer: a one-to-one
workflow with co-located simulation and AI training instances, and a
many-to-one workflow where a single AI model is trained from an ensemble of
simulations. For the one-to-one pattern, our analysis shows that node-local and
DragonHPC data staging strategies provide excellent performance compared Redis
and Lustre file system. For the many-to-one pattern, we find that data
transport becomes a dominant bottleneck as the ensemble size grows. Our
evaluation reveals that file system is the optimal solution among the tested
strategies for the many-to-one pattern.

</details>


### [17] [Non-Uniform Content-Oblivious Leader Election on Oriented Asynchronous Rings](https://arxiv.org/abs/2509.19187)
*Jérémie Chalopin,Yi-Jun Chang,Lyuting Chen,Giuseppe A. Di Luna,Haoran Zhou*

Main category: cs.DC

TL;DR: 本文研究了面向环网络中在内容不可知异步消息传递系统下的领导者选举问题，证明了在均匀算法中如果每个进程只能发送常数数量的消息则无法解决问题，并提出了多个非均匀算法和随机算法来优化消息复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究在消息内容可能被恶意篡改的内容不可知异步消息传递系统中，面向环网络的领导者选举问题的消息复杂度，特别是探索在进程发送消息数量受限情况下的可行性。

Method: 通过理论证明和算法设计：1) 证明均匀算法在常数消息限制下的不可能性；2) 设计非均匀算法利用环大小上界U和最小标识符ID_min；3) 开发随机算法用于匿名设置。

Result: 1) 证明了均匀算法在常数消息限制下无法解决领导者选举问题；2) 提出了消息复杂度为O(n·U·ID_min)的非均匀算法；3) 设计了消息复杂度为O(U·logID_min)的优化算法；4) 提出了匿名设置下的随机算法，消息复杂度为O(log²U)。

Conclusion: 本文揭示了均匀性假设对领导者选举消息复杂度的重要影响，提出了多个高效的非均匀和随机算法，显著改善了现有结果的消息复杂度依赖关系，特别是在标识符较小的情况下具有优势。

Abstract: We study the leader election problem in oriented ring networks under
content-oblivious asynchronous message-passing systems, where an adversary may
arbitrarily corrupt message contents.
  Frei et al. (DISC 2024) presented a uniform terminating leader election
algorithm for oriented rings in this setting, with message complexity $O(n
\cdot \mathsf{ID}_{\max})$ on a ring of size $n$, where $\mathsf{ID}_{\max}$ is
the largest identifier in the system, this result has been recently extended by
Chalopin et al. (DISC 2025) to unoriented rings.
  In this paper, we investigate the message complexity of leader election on
ring networks in the content-oblivious model, showing that no uniform algorithm
can solve the problem if each process is limited to sending a constant number
of messages in one direction.
  Interestingly, this limitation hinges on the uniformity assumption. In the
non-uniform setting, where processes know an upper bound $U \geq n$ on the ring
size, we present an algorithm with message complexity $O(n \cdot U \cdot
\mathsf{ID}_{\min})$, in which each process sends $O(U \cdot
\mathsf{ID}_{\min})$ messages clockwise and only three messages
counter-clockwise. Here, $\mathsf{ID}_{\min}$ is the smallest identifier in the
system. This dependence on the identifiers compares favorably with the
dependence on $\mathsf{ID}_{\max}$ of Frei et al.
  We also show a non-uniform algorithm where each process sends $O(U \cdot
\log\mathsf{ID}_{\min})$ messages in one direction and
$O(\log\mathsf{ID}_{\min})$ in the other. The factor $\log \mathsf{ID}_{\min}$
is optimal, matching the lower bound of Frei et al.
  Finally, in the anonymous setting, where processes do not have identifiers,
we propose a randomized algorithm where each process sends only $O(\log^2 U)$
messages, with a success probability of $1 - U^{-c}$.

</details>


### [18] [Accelerating Gravitational $N$-Body Simulations Using the RISC-V-Based Tenstorrent Wormhole](https://arxiv.org/abs/2509.19294)
*Jenny Lynn Almerol,Elisabetta Boella,Mario Spera,Daniele Gregori*

Main category: cs.DC

TL;DR: RISC-V加速器在科学计算中表现出色，天体物理N体代码在Wormhole n300卡上实现2倍速度提升和2倍节能


<details>
  <summary>Details</summary>
Motivation: 虽然RISC-V最初为AI工作负载设计，但其在科学计算领域也显示出潜力，需要验证其在天体物理模拟中的性能表现

Method: 将天体物理N体代码移植到Tenstorrent开发的RISC-V架构Wormhole n300加速卡上，并与高度优化的CPU实现进行对比

Result: 相比优化的CPU实现，RISC-V平台提供超过2倍的速度提升和约2倍的能源节省

Conclusion: RISC-V加速器平台在天体物理模拟算法中具有高度竞争力，是高性能科学计算的有力选择

Abstract: Although originally developed primarily for artificial intelligence
workloads, RISC-V-based accelerators are also emerging as attractive platforms
for high-performance scientific computing. In this work, we present our
approach to accelerating an astrophysical $N$-body code on the RISC-V-based
Wormhole n300 card developed by Tenstorrent. Our results show that this
platform can be highly competitive for astrophysical simulations employing this
class of algorithms, delivering more than a $2 \times$ speedup and
approximately $2 \times$ energy savings compared to a highly optimized CPU
implementation of the same code.

</details>
