<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Automated Insertion of Flushes and Fences for Persistency](https://arxiv.org/abs/2509.19459)
*Yutong Guo,Weiyu Luo,Brian Demsky*

Main category: cs.SE

TL;DR: PMRobust是一个编译器，能自动插入flush和fence操作，确保持久内存代码没有缺失flush和fence的bug，平均开销仅0.26%。


<details>
  <summary>Details</summary>
Motivation: 持久内存和CXL共享内存需要手动flush缓存行来确保数据持久化，正确使用flush和fence操作很困难，现有工具无法确保没有缺失flush的bug。

Method: 采用新颖的静态分析和针对新分配对象的优化，自动插入必要的flush和fence操作。

Result: 在持久内存库和数据结构上评估，几何平均开销仅为0.26%，相比手动放置flush和fence操作的基准测试。

Conclusion: PMRobust能有效自动确保持久内存代码的正确性，且性能开销极小。

Abstract: CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.

</details>


### [2] [Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation](https://arxiv.org/abs/2509.19533)
*Mengdi Lu,Steven Ding,Furkan Alaca,Philippe Charland*

Main category: cs.SE

TL;DR: 提出了一个将推理能力大型语言模型与AFL++结合的微服务框架，通过few-shot提示工程提升模糊测试的突变质量，发现Deepseek模型表现最佳，提示复杂度和模型选择比shot数量更重要


<details>
  <summary>Details</summary>
Motivation: 传统基于突变的模糊测试工具缺乏语义推理能力，无法处理复杂的协议逻辑和字段间依赖关系，而大型语言模型具有理解输入格式和约束的能力，但缺乏监督训练的ground truth，需要探索基于提示的few-shot学习方法

Method: 开发开源微服务框架，将推理LLMs与AFL++集成在Google FuzzBench上，解决异步执行和硬件需求差异问题，评估few-shot提示与zero-shot的效果差异

Result: 实验显示Deepseek模型表现最佳，突变有效性更多取决于提示复杂度和模型选择而非shot数量，响应延迟和吞吐量瓶颈是主要挑战

Conclusion: 推理LLMs可以通过提示工程有效提升模糊测试质量，Deepseek是最有前景的模型，未来工作需要解决延迟和吞吐量问题

Abstract: Security vulnerabilities in Internet-of-Things devices, mobile platforms, and
autonomous systems remain critical. Traditional mutation-based fuzzers -- while
effectively explore code paths -- primarily perform byte- or bit-level edits
without semantic reasoning. Coverage-guided tools such as AFL++ use
dictionaries, grammars, and splicing heuristics to impose shallow structural
constraints, leaving deeper protocol logic, inter-field dependencies, and
domain-specific semantics unaddressed. Conversely, reasoning-capable large
language models (LLMs) can leverage pretraining knowledge to understand input
formats, respect complex constraints, and propose targeted mutations, much like
an experienced reverse engineer or testing expert. However, lacking ground
truth for "correct" mutation reasoning makes supervised fine-tuning
impractical, motivating explorations of off-the-shelf LLMs via prompt-based
few-shot learning. To bridge this gap, we present an open-source microservices
framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,
tackling asynchronous execution and divergent hardware demands (GPU- vs.
CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)
How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do
few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt
engineering with off-the-shelf models improve fuzzing directly? and (R4) Which
open-source reasoning LLMs perform best under prompt-only conditions?
Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3
highlight Deepseek as the most promising. Mutation effectiveness depends more
on prompt complexity and model choice than shot count. Response latency and
throughput bottlenecks remain key obstacles, offering directions for future
work.

</details>


### [3] [Reverse Engineering User Stories from Code using Large Language Models](https://arxiv.org/abs/2509.19587)
*Mohamed Ouf,Haoyu Li,Michael Zhang,Mariam Guizani*

Main category: cs.SE

TL;DR: LLMs能够从源代码中自动恢复用户故事，在200行代码内平均F1分数达到0.8，小模型通过单示例提示可媲美大模型性能。


<details>
  <summary>Details</summary>
Motivation: 敏捷开发中用户故事至关重要，但在遗留系统和文档不完善的系统中常常缺失或过时，需要自动化方法来恢复这些用户故事。

Method: 使用1,750个标注的C++代码片段，评估5个最先进的LLM在6种提示策略下的表现，分析代码复杂度和提示设计对输出质量的影响。

Result: 所有模型在200NLOC以内的代码上平均F1分数达到0.8；小模型（8B）通过单示例提示就能达到与大模型（70B）相当的性能；Chain-of-Thought推理仅对大型模型有边际改善。

Conclusion: LLMs能够有效从源代码中恢复用户故事，提示设计比模型规模更重要，简单的示例提示就能让小模型达到优秀性能，为自动化文档生成提供了实用方案。

Abstract: User stories are essential in agile development, yet often missing or
outdated in legacy and poorly documented systems. We investigate whether large
language models (LLMs) can automatically recover user stories directly from
source code and how prompt design impacts output quality. Using 1,750 annotated
C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs
across six prompting strategies. Results show that all models achieve, on
average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a
single illustrative example enables the smallest model (8B) to match the
performance of a much larger 70B model. In contrast, structured reasoning via
Chain-of-Thought offers only marginal gains, primarily for larger models.

</details>


### [4] [Assertion Messages with Large Language Models (LLMs) for Code](https://arxiv.org/abs/2509.19673)
*Ahmed Aljohani,Anamul Haque Mollah,Hyunsook Do*

Main category: cs.SE

TL;DR: 评估四种先进LLM在生成Java测试断言消息方面的能力，Codestral-22B表现最佳但仍有提升空间，上下文信息对生成质量至关重要


<details>
  <summary>Details</summary>
Motivation: 断言消息能显著提升单元测试的可理解性，但开发者和自动化工具经常忽略编写。LLM在此任务上的能力尚未得到系统评估

Method: 使用包含216个Java测试方法的数据集，评估四种FIM LLM（Qwen2.5-Coder-32B、Codestral-22B、CodeLlama-13B、StarCoder）生成断言消息的能力，采用类人评估方法

Result: Codestral-22B获得最高质量分数2.76/5（人工编写为3.24），包含描述性测试注释后性能提升至2.97。所有模型都倾向于复制开发者的语言模式

Conclusion: LLM在生成断言消息方面有潜力但仍有局限，上下文信息对生成质量至关重要，为自动化、上下文感知的断言消息生成提供了重要基础

Abstract: Assertion messages significantly enhance unit tests by clearly explaining the
reasons behind test failures, yet they are frequently omitted by developers and
automated test-generation tools. Despite recent advancements, Large Language
Models (LLMs) have not been systematically evaluated for their ability to
generate informative assertion messages. In this paper, we introduce an
evaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -
Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset
of 216 Java test methods containing developer-written assertion messages. We
find that Codestral-22B achieves the highest quality score of 2.76 out of 5
using a human-like evaluation approach, compared to 3.24 for manually written
messages. Our ablation study shows that including descriptive test comments
further improves Codestral's performance to 2.97, highlighting the critical
role of context in generating clear assertion messages. Structural analysis
demonstrates that all models frequently replicate developers' preferred
linguistic patterns. We discuss the limitations of the selected models and
conventional text evaluation metrics in capturing diverse assertion message
structures. Our benchmark, evaluation results, and discussions provide an
essential foundation for advancing automated, context-aware generation of
assertion messages in test code. A replication package is available at
https://doi.org/10.5281/zenodo.15293133

</details>


### [5] [Intuition to Evidence: Measuring AI's True Impact on Developer Productivity](https://arxiv.org/abs/2509.19708)
*Anand Kumar,Vishal Khare,Deepak Sharma,Satyam Kumar,Vijay Saini,Anshul Yadav,Sachendra Jain,Ankit Rana,Pratham Verma,Vaibhav Meena,Avinash Edubilli*

Main category: cs.SE

TL;DR: 企业级AI辅助软件开发工具的一年期真实评估显示：代码审查周期减少31.8%，开发者满意度达85%，活跃使用率稳定在60%，代码产出量增加28%


<details>
  <summary>Details</summary>
Motivation: 评估AI辅助开发工具在企业规模部署中的实际效果，填补实验室基准测试与真实生产环境之间的研究空白

Method: 对300名工程师进行为期一年的队列分析，使用内部AI平台DeputyDev（集代码生成和自动审查功能），通过纵向数据分析生产环境使用情况

Result: PR审查周期减少31.8%；85%开发者满意代码审查功能；93%希望继续使用；使用率从4%增长到83%峰值后稳定在60%；高产用户代码产出增加61%；约30-40%生产代码通过该工具生成；整体代码交付量增加28%

Conclusion: AI辅助开发工具在企业环境中具有变革潜力，但实际部署面临挑战，需要从实验室基准测试转向生产环境实证研究

Abstract: We present a comprehensive real-world evaluation of AI-assisted software
development tools deployed at enterprise scale. Over one year, 300 engineers
across multiple teams integrated an in-house AI platform (DeputyDev) that
combines code generation and automated review capabilities into their daily
workflows. Through rigorous cohort analysis, our study demonstrates
statistically significant productivity improvements, including an overall 31.8%
reduction in PR review cycle time.
  Developer adoption was strong, with 85% satisfaction for code review features
and 93% expressing a desire to continue using the platform. Adoption patterns
showed systematic scaling from 4% engagement in month 1 to 83% peak usage by
month 6, stabilizing at 60% active engagement. Top adopters achieved a 61%
increase in code volume pushed to production, contributing to approximately 30
to 40% of code shipped to production through this tool, accounting for an
overall 28% increase in code shipment volume.
  Unlike controlled benchmark evaluations, our longitudinal analysis provides
empirical evidence from production environments, revealing both the
transformative potential and practical deployment challenges of integrating AI
into enterprise software development workflows.

</details>


### [6] [Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation](https://arxiv.org/abs/2509.19918)
*Micheline Bénédicte Moumoula,Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: XL-CoGen是一个多智能体代码生成系统，通过数据驱动的桥接语言选择和跨语言知识转移，显著提升了多编程语言的代码生成质量


<details>
  <summary>Details</summary>
Motivation: 当前LLM在不同编程语言间的代码生成能力差异很大，特别是对训练数据较少的语言如Rust、Perl等。现有方法往往孤立处理每种语言，未能充分利用跨语言模式和知识共享的机会

Method: 采用协调的多智能体架构，整合中间表示、代码生成、翻译和自动修复。核心特点是数据驱动的桥接语言选择机制，基于经验推导的转移矩阵识别最佳中间语言

Result: 实验显示XL-CoGen相比最强微调基线提升13个百分点，比现有单语言多智能体方法提升高达30个百分点。消融研究证实兼容性引导的桥接显著优于基于LLM的启发式方法

Conclusion: 该系统证明了累积跨语言知识转移的价值，通过协调多智能体架构和数据驱动的桥接语言选择，有效解决了多语言代码生成的质量差异问题

Abstract: Producing high-quality code across multiple programming languages is
increasingly important as today's software systems are built on heterogeneous
stacks. Large language models (LLMs) have advanced the state of automated
programming, yet their proficiency varies sharply between languages, especially
those with limited training data such as Rust, Perl, OCaml, and Erlang. Many
current solutions including language-specific fine-tuning, multi-agent
orchestration, transfer learning, and intermediate-representation pipelines
still approach each target language in isolation, missing opportunities to
share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture
that integrates intermediate representation, code generation, translation, and
automated repair. Its distinguishing feature is a data-driven mechanism for
selecting bridging languages: empirically derived transfer matrices identify
the best intermediate languages based on demonstrated translation success
rather than raw generation accuracy. The system performs early output
validation, iteratively corrects errors, and reuses intermediate artifacts as
contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13
percentage-point gains over the strongest fine-tuned baseline and as much as 30
percentage points over existing single-language multi-agent methods. Ablation
studies further demonstrate that compatibility-guided bridging significantly
outperforms LLM-based heuristics, confirming the value of cumulative
cross-language knowledge transfer.

</details>


### [7] [Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories](https://arxiv.org/abs/2509.20010)
*Xiaoning Ren,Yuhang Ye,Xiongfei Wu,Yueming Wu,Yinxing Xue*

Main category: cs.SE

TL;DR: 提出神经网络物料清单(NNBOM)概念，构建大规模数据集分析神经网络软件演化趋势


<details>
  <summary>Details</summary>
Motivation: 传统SBOM不适用于神经网络软件，现有AIBOM缺乏大规模演化分析实践，需要专门工具分析NN软件演化

Method: 从55,997个PyTorch GitHub仓库创建NNBOM数据库，分析第三方库、预训练模型和模块的使用模式

Result: 建立了大规模NNBOM数据库，进行了跨软件规模、组件重用和跨领域依赖的全面实证研究

Conclusion: NNBOM为神经网络软件演化分析提供了有效框架，开发的原型应用展示了其实用价值

Abstract: Neural networks have become integral to many fields due to their exceptional
performance. The open-source community has witnessed a rapid influx of neural
network (NN) repositories with fast-paced iterations, making it crucial for
practitioners to analyze their evolution to guide development and stay ahead of
trends. While extensive research has explored traditional software evolution
using Software Bill of Materials (SBOMs), these are ill-suited for NN software,
which relies on pre-defined modules and pre-trained models (PTMs) with distinct
component structures and reuse patterns. Conceptual AI Bills of Materials
(AIBOMs) also lack practical implementations for large-scale evolutionary
analysis. To fill this gap, we introduce the Neural Network Bill of Material
(NNBOM), a comprehensive dataset construct tailored for NN software. We create
a large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,
cataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct
a comprehensive empirical study of neural network software evolution across
software scale, component reuse, and inter-domain dependency, providing
maintainers and developers with a holistic view of its long-term trends.
Building on these findings, we develop two prototype applications,
\textit{Multi repository Evolution Analyzer} and \textit{Single repository
Component Assessor and Recommender}, to demonstrate the practical value of our
analysis.

</details>


### [8] [V-GameGym: Visual Game Generation for Code Large Language Models](https://arxiv.org/abs/2509.20136)
*Wei Zhang,Jack Yang,Renshuai Tao,Lingzheng Chai,Shawn Guo,Jiajun Wu,Xiaoming Chen,Ganqu Cui,Ning Ding,Xander Xu,Hu Wei,Bowen Zhou*

Main category: cs.SE

TL;DR: V-GameGym是一个针对视觉游戏开发的多模态基准测试，包含2,219个高质量样本，填补了现有代码LLM基准在游戏开发评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有代码基准主要关注语法正确性和执行准确性，忽视了游戏开发中关键的玩性、视觉美学和用户参与度等实际部署所需的指标。

Method: 采用基于聚类的筛选方法从真实仓库中提取100个主题集群的样本，构建多模态评估框架和自动化LLM驱动的视觉代码合成流水线。

Result: V-GameGym有效连接了代码生成准确性与实际游戏开发工作流程，为视觉编程和交互元素生成提供了可量化的质量指标。

Conclusion: 该基准测试填补了代码LLM在算法问题解决与完整游戏开发需求之间的差距，为多模态代码生成评估提供了新标准。

Abstract: Code large language models have demonstrated remarkable capabilities in
programming tasks, yet current benchmarks primarily focus on single modality
rather than visual game development. Most existing code-related benchmarks
evaluate syntax correctness and execution accuracy, overlooking critical
game-specific metrics such as playability, visual aesthetics, and user
engagement that are essential for real-world deployment. To address the gap
between current LLM capabilities in algorithmic problem-solving and competitive
programming versus the comprehensive requirements of practical game
development, we present V-GameGym, a comprehensive benchmark comprising 2,219
high-quality samples across 100 thematic clusters derived from real-world
repositories, adopting a novel clustering-based curation methodology to ensure
both diversity and structural completeness. Further, we introduce a multimodal
evaluation framework with an automated LLM-driven pipeline for visual code
synthesis using complete UI sandbox environments. Our extensive analysis
reveals that V-GameGym effectively bridges the gap between code generation
accuracy and practical game development workflows, providing quantifiable
quality metrics for visual programming and interactive element generation.

</details>


### [9] [Enhancing Requirement Traceability through Data Augmentation Using Large Language Models](https://arxiv.org/abs/2509.20149)
*Jianzhang Zhang,Jialong Zhou,Nan Niu,Chuang Liu*

Main category: cs.SE

TL;DR: 使用大型语言模型进行数据增强，通过提示工程技术生成需求到代码的追踪链接，显著提升需求追踪模型的性能，F1分数最高提升28.59%。


<details>
  <summary>Details</summary>
Motivation: 解决需求追踪中训练数据稀缺和人工制品间语义鸿沟的问题，现有自动化方法受限于数据不足。

Method: 提出基于提示工程的LLM数据增强方法，使用4种LLM（Gemini 1.5 Pro、Claude 3、GPT-3.5、GPT-4）和零样本/少样本模板生成追踪链接，并优化追踪模型的编码器组件。

Result: 实验结果显示方法显著提升模型性能，F1分数最高提升28.59%，证明了方法的有效性和实际应用潜力。

Conclusion: LLM数据增强是解决需求追踪数据稀缺问题的有效方法，通过提示工程和模型优化可以显著提升追踪性能，具有重要实践价值。

Abstract: Requirements traceability is crucial in software engineering to ensure
consistency between requirements and code. However, existing automated
traceability methods are constrained by the scarcity of training data and
challenges in bridging the semantic gap between artifacts. This study aims to
address the data scarcity problem in requirements traceability by employing
large language models (LLMs) for data augmentation. We propose a novel approach
that utilizes prompt-based techniques with LLMs to generate augmented
requirement-to-code trace links, thereby enhancing the training dataset. Four
LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both
zero-shot and few-shot templates. Moreover, we optimized the encoder component
of the tracing model to improve its efficiency and adaptability to augmented
data. The key contributions of this paper are: (1) proposing and evaluating
four prompt templates for data augmentation; (2) providing a comparative
analysis of four LLMs for generating trace links; (3) enhancing the model's
encoder for improved adaptability to augmented datasets. Experimental results
show that our approach significantly enhances model performance, achieving an
F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and
potential for practical application.

</details>


### [10] [Benchmarking Web API Integration Code Generation](https://arxiv.org/abs/2509.20172)
*Daniel Maninger,Leon Chemnitz,Amir Molzam Sharifloo,Jannis Brugger,Mira Mezini*

Main category: cs.SE

TL;DR: LLM在生成Web API调用代码方面表现不佳，开源模型在API集成任务上的成功率低于40%，存在端点幻觉和参数错误等问题


<details>
  <summary>Details</summary>
Motivation: API集成是数字基础设施的核心，但编写正确的API调用代码具有挑战性。虽然LLM在软件开发中越来越流行，但其在自动化生成Web API集成代码方面的有效性尚未被探索

Method: 提出了一个数据集和评估流程，用于评估LLM生成Web API调用代码的能力，并对多个开源LLM进行了实验

Result: 实验显示生成API调用存在显著挑战，导致幻觉端点、错误参数使用等问题。所有评估的开源模型都无法解决超过40%的任务

Conclusion: 当前的开源LLM在生成Web API集成代码方面能力有限，需要进一步改进模型在该领域的表现

Abstract: API integration is a cornerstone of our digital infrastructure, enabling
software systems to connect and interact. However, as shown by many studies,
writing or generating correct code to invoke APIs, particularly web APIs, is
challenging. Although large language models~(LLMs) have become popular in
software development, their effectiveness in automating the generation of web
API integration code remains unexplored. In order to address this, we present a
dataset and evaluation pipeline designed to assess the ability of LLMs to
generate web API invocation code. Our experiments with several open-source LLMs
reveal that generating API invocations poses a significant challenge, resulting
in hallucinated endpoints, incorrect argument usage, and other errors. None of
the evaluated open-source models were able to solve more than 40% of the tasks.

</details>


### [11] [The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation](https://arxiv.org/abs/2509.20215)
*Guang Yang,Wei Zheng,Xiang Chen,Yifan Sun,Fengji Zhang,Terry Yue Zhuo*

Main category: cs.SE

TL;DR: VCD-RNK是一个专门用于Verilog代码重排的判别器模型，通过模拟专家在代码语义分析、测试用例生成和功能正确性评估三个维度的推理过程，无需执行计算密集的测试就能有效提升Verilog代码生成质量


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在Verilog生成中由于领域知识有限导致的不可靠问题，硬件工程师需要一个可信赖的解决方案而非多个不确定的候选

Method: 将Verilog生成问题建模为需求与实现之间的语义对齐问题，提出VCD-RNK判别器模型，通过知识蒸馏整合Verilog专家在三个维度的推理能力

Result: VCD-RNK能够有效避免现有方法中计算密集的测试执行过程，提供高效的Verilog代码重排

Conclusion: VCD-RNK为解决Verilog代码生成中的语义对齐问题提供了有效的判别器方法，通过模拟专家推理过程显著提升了生成代码的可信度

Abstract: LLMs face significant challenges in Verilog generation due to limited
domain-specific knowledge. While sampling techniques improve pass@k metrics,
hardware engineers need one trustworthy solution rather than uncertain
candidates. To bridge this gap, we formulate it as a semantic alignment problem
between requirements and Verilog implementations, and propose VCD-RNK, a
discriminator model tailored for efficient Verilog code reranking.
Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling
expert knowledge across three dimensions: code semantic analysis, test case
generation, and functional correctness assessment. By explicitly simulating the
above reasoning processes during inference, VCD-RNK effectively avoids
computationally intensive test execution in existing methods.

</details>


### [12] [Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs](https://arxiv.org/abs/2509.20300)
*Jannis Kiesel,Jonathan Heiss*

Main category: cs.SE

TL;DR: 基于零知识证明的业务流程验证方法，在保护商业机密的同时确保流程执行完整性，通过zkVM集成和原型实现验证了碳足迹计算等场景的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决跨组织业务流程中既要确保流程完整性又不能泄露商业机密的双重挑战，为可信业务流程执行提供隐私保护解决方案。

Method: 将零知识证明虚拟机(zkVM)集成到业务流程管理引擎中，采用系统架构和原型实现，支持通过证明组合实现链式可验证计算。

Result: 实验评估表明该方法能够在给定保密约束下自动化流程验证，评估了不同ZKP证明变体在流程模型中的效率和验证性能。

Conclusion: 提出的ZKP方法成功实现了业务流程的可验证执行同时保护机密信息，为BPM生命周期中零知识证明的实际集成提供了可行方案。

Abstract: Ensuring the integrity of business processes without disclosing confidential
business information is a major challenge in inter-organizational processes.
This paper introduces a zero-knowledge proof (ZKP)-based approach for the
verifiable execution of business processes while preserving confidentiality. We
integrate ZK virtual machines (zkVMs) into business process management engines
through a comprehensive system architecture and a prototypical implementation.
Our approach supports chained verifiable computations through proof
compositions. On the example of product carbon footprinting, we model
sequential footprinting activities and demonstrate how organizations can prove
and verify the integrity of verifiable processes without exposing sensitive
information. We assess different ZKP proving variants within process models for
their efficiency in proving and verifying, and discuss the practical
integration of ZKPs throughout the Business Process Management (BPM) lifecycle.
Our experiment-driven evaluation demonstrates the automation of process
verification under given confidentiality constraints.

</details>


### [13] [Protocol Testing with I/O Grammars](https://arxiv.org/abs/2509.20308)
*Alexander Liggesmeyer,José Antonio Zamudio Amaya,Andreas Zeller*

Main category: cs.SE

TL;DR: 提出了一种基于I/O语法的协议测试新方法，将输入生成和输出检查统一在单一框架中，能够完整指定协议语法语义并实现多功能测试。


<details>
  <summary>Details</summary>
Motivation: 协议测试面临两个基本问题：需要生成语法语义正确且多样化的输入，以及需要验证输出正确性的预言机制。现有工具无法同时解决这些问题。

Method: 引入I/O语法作为完整指定协议语法语义的形式化方法，基于FANDANGO框架实现，支持测试生成、模拟对象和预言功能，并采用k路径引导系统化覆盖协议状态和交互。

Result: 在DNS、FTP、SMTP等协议上的评估表明，I/O语法能正确完整地指定高级协议特性，系统化覆盖相比随机方法能更快覆盖输入响应空间和功能。

Conclusion: I/O语法提供了一个统一的协议测试框架，解决了输入生成和输出验证的双重挑战，在协议测试中表现出优于现有随机方法的效率和效果。

Abstract: Generating software tests faces two fundamental problems. First, one needs to
_generate inputs_ that are syntactically and semantically correct, yet
sufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check
outputs_ whether a test case is correct or not. Both problems become apparent
in _protocol testing_, where inputs are messages exchanged between parties, and
outputs are the responses of these parties.
  In this paper, we propose a novel approach to protocol testing that combines
input generation and output checking in a single framework. We introduce _I/O
grammars_ as the first means to _completely_ specify the syntax and semantics
of protocols, including messages, states, and interactions. Our implementation,
based on the FANDANGO framework, takes a single I/O grammar, and can act as a
_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a
_server_, or both (or actually any number of parties), a versatility not found
in any existing tool or formalism. User-defined _constraints}_can have the
generator focus on arbitrary protocol features; $k$-path guidance
systematically covers states, messages, responses, and value alternatives in a
unified fashion.
  We evaluate the effectiveness of our approach by applying it to several
protocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can
specify advanced protocol features correctly and completely, while also
enabling output validation of the programs under test. In its evaluation, we
find that systematic coverage of the I/O grammar results in much quicker
coverage of the input and response spaces (and thus functionality) compared to
the random-based state-of-the-art approaches.

</details>


### [14] [Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study](https://arxiv.org/abs/2509.20353)
*Viktoria Stray,Elias Goldmann Brandtzæg,Viggo Tellefsen Wivestad,Astri Barbala,Nils Brede Moe*

Main category: cs.SE

TL;DR: GitHub Copilot对开发者活动的影响研究：Copilot用户比非用户更活跃，但采用工具后提交活动无显著变化，与主观生产力感知存在差异


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI工具GitHub Copilot在真实工作环境中对开发者活动和感知生产力的实际影响

Method: 混合方法案例研究：分析26,317个非合并提交数据（来自703个GitHub仓库），调查问卷和13次访谈，比较25名Copilot用户和14名非用户

Result: Copilot用户始终比非用户更活跃（即使在采用Copilot之前），采用Copilot后提交活动指标无统计学显著变化，但观察到轻微增长

Conclusion: 基于提交的指标变化与主观生产力体验之间存在差异，Copilot可能通过其他方式影响生产力而非直接增加提交活动

Abstract: This study investigates the real-world impact of the generative AI (GenAI)
tool GitHub Copilot on developer activity and perceived productivity. We
conducted a mixed-methods case study in NAV IT, a large public sector agile
organization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's
GitHub repositories over a two-year period, focusing on commit-based activity
metrics from 25 Copilot users and 14 non-users. The analysis was complemented
by survey responses on their roles and perceived productivity, as well as 13
interviews. Our analysis of activity metrics revealed that individuals who used
Copilot were consistently more active than non-users, even prior to Copilot's
introduction. We did not find any statistically significant changes in
commit-based activity for Copilot users after they adopted the tool, although
minor increases were observed. This suggests a discrepancy between changes in
commit-based metrics and the subjective experience of productivity.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [15] [Are We Scaling the Right Thing? A System Perspective on Test-Time Scaling](https://arxiv.org/abs/2509.19645)
*Youpeng Zhao,Jinpeng LV,Di Wu,Jun Wang,Christopher Gooley*

Main category: cs.PF

TL;DR: 本文提出系统驱动的测试时缩放视角，分析推理模型在实际指标（如延迟和每令牌成本）上的扩展效果，发现现有方法局限并呼吁转向整体系统感知评估


<details>
  <summary>Details</summary>
Motivation: 现有缩放方法仅关注计算最优Pareto边界，忽略了计算最优并不总是系统最优的简单事实，需要从系统角度评估推理时的扩展规律

Method: 通过评估张量并行和推测解码等流行优化技术的影响，进行初步分析

Result: 揭示了当前方法的局限性

Conclusion: 需要范式转变，采用整体系统感知的评估方法来捕捉推理时扩展规律的真实本质

Abstract: Test-time scaling (TTS) has recently emerged as a promising direction to
exploit the hidden reasoning capabilities of pre-trained large language models
(LLMs). However, existing scaling methods narrowly focus on the compute-optimal
Pareto-frontier, ignoring the simple fact that compute-optimal is not always
system-optimal. In this work, we propose a system-driven perspective on TTS,
analyzing how reasoning models scale against practical metrics, such as latency
and cost-per-token. By evaluating the impact of popular optimizations such as
tensor parallelism and speculative decoding, our preliminary analysis reveals
the limitations of current methods and calls for a paradigm shift toward
holistic, system-aware evaluations that capture the true essence of scaling
laws at inference time.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [16] [The Heterogeneous Multi-Agent Challenge](https://arxiv.org/abs/2509.19512)
*Charles Dansereau,Junior-Samuel Lopez-Yepez,Karthik Soma,Antoine Fagette*

Main category: cs.MA

TL;DR: 该论文指出异构多智能体强化学习(HeMARL)领域缺乏标准化测试环境，导致研究进展难以衡量，需要建立类似ALE和SMAC的基准测试平台。


<details>
  <summary>Details</summary>
Motivation: 现实世界中存在大量异构智能体需要协作的场景，但当前MARL研究主要关注同构智能体，异构多智能体强化学习领域缺乏标准化的测试环境来评估算法性能。

Method: 通过分析当前HeMARL研究现状，指出存在的问题：使用过于简单的环境或弱异构环境，导致算法性能难以区分优劣。

Result: 识别出HeMARL领域标准化测试环境的缺失问题，强调了建立公认基准的重要性。

Conclusion: 需要为异构多智能体强化学习开发标准化的测试环境，以促进该领域的研究进展和算法评估。

Abstract: Multi-Agent Reinforcement Learning (MARL) is a growing research area which
gained significant traction in recent years, extending Deep RL applications to
a much wider range of problems. A particularly challenging class of problems in
this domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where
agents with different sensors, resources, or capabilities must cooperate based
on local information. The large number of real-world situations involving
heterogeneous agents makes it an attractive research area, yet underexplored,
as most MARL research focuses on homogeneous agents (e.g., a swarm of identical
robots). In MARL and single-agent RL, standardized environments such as ALE and
SMAC have allowed to establish recognized benchmarks to measure progress.
However, there is a clear lack of such standardized testbed for cooperative
HeMARL. As a result, new research in this field often uses simple environments,
where most algorithms perform near optimally, or uses weakly heterogeneous MARL
environments.

</details>


### [17] [Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method for Multi-Agent Systems](https://arxiv.org/abs/2509.19599)
*Danilo Trombino,Vincenzo Pecorella,Alessandro de Giulii,Davide Tresoldi*

Main category: cs.MA

TL;DR: KBA Orchestration是一种新颖的多智能体系统协调方法，通过结合静态描述和从智能体知识库提取的动态隐私保护相关性信号，显著提高了任务路由的准确性和系统效率。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体系统协调方法依赖静态智能体描述，这些描述往往过时或不完整，导致在动态环境中任务路由效率低下，特别是在智能体能力持续演化的场景中。

Method: 提出知识库感知协调框架，当静态描述不足以做出明确路由决策时，协调器并行提示子智能体，每个智能体根据其私有知识库评估任务相关性，返回轻量级ACK信号而不暴露底层数据，收集的信号填充共享语义缓存。

Result: 基准测试显示，KBA Orchestration在路由精度和整体系统效率方面显著优于静态描述驱动的方法，适用于需要比标准描述驱动路由更高精度的大规模系统。

Conclusion: 该方法通过结合静态描述和动态隐私保护相关性信号，实现了更准确和自适应的任务路由，同时保持了智能体自主性和数据机密性，为大规模多智能体系统提供了有效的协调解决方案。

Abstract: Multi-agent systems (MAS) are increasingly tasked with solving complex,
knowledge-intensive problems where effective agent orchestration is critical.
Conventional orchestration methods rely on static agent descriptions, which
often become outdated or incomplete. This limitation leads to inefficient task
routing, particularly in dynamic environments where agent capabilities
continuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a
novel approach that augments static descriptions with dynamic,
privacy-preserving relevance signals derived from each agent's internal
knowledge base (KB). In the proposed framework, when static descriptions are
insufficient for a clear routing decision, the orchestrator prompts the
subagents in parallel. Each agent then assesses the task's relevance against
its private KB, returning a lightweight ACK signal without exposing the
underlying data. These collected signals populate a shared semantic cache,
providing dynamic indicators of agent suitability for future queries. By
combining this novel mechanism with static descriptions, our method achieves
more accurate and adaptive task routing preserving agent autonomy and data
confidentiality. Benchmarks show that our KBA Orchestration significantly
outperforms static description-driven methods in routing precision and overall
system efficiency, making it suitable for large-scale systems that require
higher accuracy than standard description-driven routing.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera](https://arxiv.org/abs/2509.19478)
*Ziwei Wang,Cong Wu,Paolo Tasca*

Main category: cs.DC

TL;DR: 本文提出了一种用于Hedera区块链的混合分片解决方案，通过本地和全局委员会的网络分区实现高效跨分片交易，显著降低了存储和通信开销，提高了可扩展性和容错能力。


<details>
  <summary>Details</summary>
Motivation: 解决区块链网络面临的可扩展性挑战，通过分片技术实现更高的交易吞吐量、降低延迟和优化资源使用，特别是在Hedera这种采用Gossip about Gossip协议和异步拜占庭容错技术的分布式账本中。

Method: 提出混合分片解决方案，将网络划分为本地和全局委员会，支持高效的跨分片交易，并通过动态重新配置确保强大的安全性。

Result: 分析显示存储和通信开销显著降低，可扩展性得到改善，容错能力增强，证明了在Hedera架构中集成分片技术的可行性和优势。

Conclusion: 分片技术是解决区块链可扩展性问题的关键方案，本文提出的混合分片方法为Hedera网络提供了有效的扩展路径，具有实际应用潜力。

Abstract: Sharding has emerged as a critical solution to address the scalability
challenges faced by blockchain networks, enabling them to achieve higher
transaction throughput, reduced latency, and optimized resource usage. This
paper investigates the advancements, methodologies, and adoption potential of
sharding in the context of Hedera, a distributed ledger technology known for
its unique Gossip about Gossip protocol and asynchronous Byzantine Fault
Tolerance (ABFT). We explore various academic and industrial sharding
techniques, emphasizing their benefits and trade-offs. Building on these
insights, we propose a hybrid sharding solution for Hedera that partitions the
network into local and global committees, facilitating efficient cross-shard
transactions and ensuring robust security through dynamic reconfiguration. Our
analysis highlights significant reductions in storage and communication
overhead, improved scalability, and enhanced fault tolerance, demonstrating the
feasibility and advantages of integrating sharding into Hedera's architecture.

</details>


### [19] [To Stream or Not to Stream: Towards A Quantitative Model for Remote HPC Processing Decisions](https://arxiv.org/abs/2509.19532)
*Flavio Castro,Weijian Zheng,Joaquin Chung,Ian Foster,Rajkumar Kettimuthu*

Main category: cs.DC

TL;DR: 提出了一个量化框架和流式速度评分来评估远程HPC资源是否能比本地替代方案提供及时的数据处理，通过考虑数据生成率、传输效率、远程处理能力和文件I/O开销等关键参数来计算总处理完成时间。


<details>
  <summary>Details</summary>
Motivation: 现代科学仪器生成数据的速度越来越超过本地计算能力，文件式传输的暂存和I/O开销使得基于文件的远程HPC资源使用对时间敏感的分析和实验引导变得不切实际。

Method: 引入定量框架和流式速度评分，整合数据生成率、传输效率、远程处理能力和文件I/O开销等关键参数，计算总处理完成时间并识别流式处理有益的操作区间。

Result: 测量显示在高数据速率下，流式处理可以实现比基于文件的方法低97%的端到端完成时间，但最坏情况下的拥塞可能使传输时间增加一个数量级以上。

Conclusion: 流式处理在高数据速率下显著优于文件式方法，但尾部延迟对流式处理的可行性决策至关重要，需要系统性地评估流式处理的可行性。

Abstract: Modern scientific instruments generate data at rates that increasingly exceed
local compute capabilities and, when paired with the staging and I/O overheads
of file-based transfers, also render file-based use of remote HPC resources
impractical for time-sensitive analysis and experimental steering. Real-time
streaming frameworks promise to reduce latency and improve system efficiency,
but lack a principled way to assess their feasibility. In this work, we
introduce a quantitative framework and an accompanying Streaming Speed Score to
evaluate whether remote high-performance computing (HPC) resources can provide
timely data processing compared to local alternatives. Our model incorporates
key parameters including data generation rate, transfer efficiency, remote
processing power, and file input/output overhead to compute total processing
completion time and identify operational regimes where streaming is beneficial.
We motivate our methodology with use cases from facilities such as APS, FRIB,
LCLS-II, and the LHC, and validate our approach through an illustrative case
study based on LCLS-II data. Our measurements show that streaming can achieve
up to 97% lower end-to-end completion time than file-based methods under high
data rates, while worst-case congestion can increase transfer times by over an
order of magnitude, underscoring the importance of tail latency in streaming
feasibility decisions.

</details>


### [20] [Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE](https://arxiv.org/abs/2509.19701)
*Akash Poptani,Alireza Khadem,Scott Mahlke,Jonah Miller,Joshua Dolence,Reetuparna Das*

Main category: cs.DC

TL;DR: Parthenon AMR基准测试在CPU-GPU系统上性能分析显示，较小的网格块和较深的AMR层级会因通信增加、串行开销和GPU利用率低下而降低GPU性能


<details>
  <summary>Details</summary>
Motivation: 分析高性能计算中自适应网格细化(AMR)在CPU-GPU异构系统上的性能表现，为能源部即将部署的异构超级计算机提供优化指导

Method: 通过详细性能分析识别Parthenon块结构AMR基准测试在CPU-GPU系统中的效率低下、低占用率和内存访问瓶颈问题

Result: 发现较小网格块和较深AMR层级会显著降低GPU性能，识别了通信开销、串行瓶颈和GPU利用率不足等问题

Conclusion: 提出了优化建议以提高GPU吞吐量和减少内存占用，这些见解可为未来在异构超级计算机上部署AMR提供指导

Abstract: Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce
compute and memory demands while maintaining accuracy. This work analyzes the
performance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems.
We show that smaller mesh blocks and deeper AMR levels degrade GPU performance
due to increased communication, serial overheads, and inefficient GPU
utilization. Through detailed profiling, we identify inefficiencies, low
occupancy, and memory access bottlenecks. We further analyze rank scalability
and memory constraints, and propose optimizations to improve GPU throughput and
reduce memory footprint. Our insights can inform future AMR deployments on
Department of Energy's upcoming heterogeneous supercomputers.

</details>


### [21] [A Survey of Recent Advancements in Secure Peer-to-Peer Networks](https://arxiv.org/abs/2509.19539)
*Raj Patel,Umesh Biswas,Surya Kodipaka,Will Carroll,Preston Peranich,Maxwell Young*

Main category: cs.DC

TL;DR: 这是一篇关于P2P网络安全最新理论进展的综述论文，更新了十多年前的旧调查，重点关注经典威胁和新兴趋势带来的新挑战与解决方案。


<details>
  <summary>Details</summary>
Motivation: P2P网络是现代计算的基石，其安全性是活跃的研究领域。虽然已有许多具有强安全保证的防御方案被提出，但最近的调查已超过十年，需要更新的综述来反映最新进展。

Method: 通过文献综述方法，系统回顾和分析近年来针对P2P网络安全的理论进展，包括对经典威胁（如Sybil攻击和路由攻击）的解决方案，以及新兴趋势（如机器学习、社交网络和动态系统）带来的新挑战和解决方案。

Result: 论文评估了各种解决方案的优势和劣势，提供了对P2P网络安全现状的全面分析，识别了现有方法的强项和局限性。

Conclusion: 论文总结了当前P2P网络安全研究的最新进展，指出了未来研究的方向，为学术界和工业界提供了有价值的参考和指导。

Abstract: Peer-to-peer (P2P) networks are a cornerstone of modern computing, and their
security is an active area of research. Many defenses with strong security
guarantees have been proposed; however, the most-recent survey is over a decade
old. This paper delivers an updated review of recent theoretical advances that
address classic threats, such as the Sybil and routing attacks, while
highlighting how emerging trends -- such as machine learning, social networks,
and dynamic systems -- pose new challenges and drive novel solutions. We
evaluate the strengths and weaknesses of these solutions and suggest directions
for future research.

</details>


### [22] [Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient LLM Inference](https://arxiv.org/abs/2509.19729)
*Haoyu Chen,Xue Li,Kun Qian,Yu Guan,Jin Zhao,Xin Wang*

Main category: cs.DC

TL;DR: Gyges通过自适应调整并行策略来处理LLM服务中的上下文长度变化，在保持高吞吐量的同时支持更大上下文长度


<details>
  <summary>Details</summary>
Motivation: LLM服务中请求的动态性（特别是上下文长度变化）需要高效处理，但现有并行策略（如Tensor Parallelism）在支持更大上下文时会降低整体吞吐量

Method: 提出Cross-Instance Parallelism Transformation (Gyges)，包括：(1)页面友好的头部中心布局加速KV缓存转换；(2)专用权重填充加速模型权重转换；(3)转换感知调度器协同调度请求和并行转换

Result: 使用真实世界trace评估显示，Gyges相比最先进解决方案将吞吐量提高了1.75倍到6.57倍

Conclusion: Gyges通过自适应并行策略转换有效解决了LLM服务中上下文长度动态变化与吞吐量之间的权衡问题

Abstract: Efficiently processing the dynamics of requests, especially the context
length variance, is important in Large Language Model (LLM) serving scenarios.
However, there is an intrinsic trade-off: while leveraging parallelism
strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to
accommodate larger context lengths, it inevitably results in degraded overall
throughput. In this paper, we propose Cross-Instance Parallelism Transformation
(Gyges), which adaptively adjusts the parallelism strategies of running
instances to align with the dynamics of incoming requests. We design (1) a
page-friendly, header-centric layout to accelerate KV cache transformations;
(2) dedicated weight padding to accelerate model weight transformations; and
(3) a transformation-aware scheduler to cooperatively schedule requests and
parallelism transformations, optimizing the overall performance. Evaluations
using real-world traces show that Gyges improves throughput by 1.75x-6.57x
compared to state-of-the-art solutions.

</details>


### [23] [BurstEngine: an Efficient Distributed Framework for Training Transformers on Extremely Long Sequences of over 1M Tokens](https://arxiv.org/abs/2509.19836)
*Ao Sun,Weilin Zhao,Xu Han,Cheng Yang,Zhiyuan Liu,Chuan Shi,Maosong sun*

Main category: cs.DC

TL;DR: BurstEngine是一个高效训练LLM长序列数据的框架，通过BurstAttention降低通信成本，引入序列级选择性检查点和负载平衡优化，在超过100万token的超长序列训练中比现有方法快1.2倍且内存开销更低。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Tensor Parallelism和Context Parallelism在处理超长序列（超过100万token）时，随着序列长度和GPU数量增加，模型FLOPs利用率较低。

Method: 提出BurstEngine框架，包含：1) BurstAttention - 优化的分布式注意力机制，使用拓扑感知环形通信充分利用网络带宽；2) 细粒度通信计算重叠；3) 序列级选择性检查点；4) 语言建模头与损失函数融合；5) 各种注意力掩码类型的负载平衡优化。

Result: 在超过100万token的超长序列训练中，BurstEngine相比最先进的基线方法实现了1.2倍的加速，并且内存开销显著降低。

Conclusion: BurstEngine通过多项优化技术有效解决了长序列训练中的效率和内存问题，为训练超长序列LLM提供了高效的解决方案。

Abstract: Existing methods for training LLMs on long-sequence data, such as Tensor
Parallelism and Context Parallelism, exhibit low Model FLOPs Utilization as
sequence lengths and number of GPUs increase, especially when sequence lengths
exceed 1M tokens. To address these challenges, we propose BurstEngine, an
efficient framework designed to train LLMs on long-sequence data. BurstEngine
introduces BurstAttention, an optimized distributed attention with lower
communication cost than RingAttention. BurstAttention leverages topology-aware
ring communication to fully utilize network bandwidth and incorporates
fine-grained communication-computation overlap. Furthermore, BurstEngine
introduces sequence-level selective checkpointing and fuses the language
modeling head with the loss function to reduce memory cost. Additionally,
BurstEngine introduces workload balance optimization for various types of
attention masking. By integrating these optimizations, BurstEngine achieves a
$1.2\times$ speedup with much lower memory overhead than the state-of-the-art
baselines when training LLMs on extremely long sequences of over 1M tokens. We
have made our code publicly available on GitHub:
https://github.com/thunlp/BurstEngine.

</details>


### [24] [Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models](https://arxiv.org/abs/2509.20160)
*Prashanthi S. K.,Sai Anuroop Kesanapalli,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 对三种NVIDIA Jetson边缘设备进行深度神经网络训练的全面性能分析，研究不同参数对训练时间、能耗和资源利用率的影响


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的DNN训练研究不足，而联邦学习和GPU加速边缘设备的发展需要系统性地了解边缘训练的特性

Method: 在三种Jetson设备上对三种DNN模型-数据集组合进行训练，调整I/O流水线、存储介质、批大小和功率模式等参数，分析CPU/GPU利用率、训练时间、能耗等指标

Result: 揭示了资源间的相互依赖关系和反直觉的发现，建立了预测训练时间和能耗的简单模型

Conclusion: 研究有助于优化边缘训练性能，在受限设备上权衡时间和能耗，并为DNN工作负载选择合适的边缘硬件

Abstract: Deep Neural Networks (DNNs) have had a significant impact on domains like
autonomous vehicles and smart cities through low-latency inferencing on edge
computing devices close to the data source. However, DNN training on the edge
is poorly explored. Techniques like federated learning and the growing capacity
of GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a
holistic characterization of DNN training on the edge. Training DNNs is
resource-intensive and can stress an edge's GPU, CPU, memory and storage
capacities. Edge devices also have different resources compared to workstations
and servers, such as slower shared memory and diverse storage media. Here, we
perform a principled study of DNN training on individual devices of three
contemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three
diverse DNN model--dataset combinations. We vary device and training parameters
such as I/O pipelining and parallelism, storage media, mini-batch sizes and
power modes, and examine their effect on CPU and GPU utilization, fetch stalls,
training time, energy usage, and variability. Our analysis exposes several
resource inter-dependencies and counter-intuitive insights, while also helping
quantify known wisdom. Our rigorous study can help tune the training
performance on the edge, trade-off time and energy usage on constrained
devices, and even select an ideal edge hardware for a DNN workload, and, in
future, extend to federated learning too. As an illustration, we use these
results to build a simple model to predict the training time and energy per
epoch for any given DNN across different power modes, with minimal additional
profiling.

</details>


### [25] [Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge Accelerators](https://arxiv.org/abs/2509.20189)
*Prashanthi S. K.,Kunal Kumar Sahoo,Amartya Ranjan Saikia,Pranav Gupta,Atharva Vinay Joshi,Priyanshu Pansari,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文开发了时间屋顶线和能量屋顶线模型来分析Nvidia Jetson边缘加速器在不同功耗模式下的DNN推理性能，发现默认MAXN模式并非最节能，并提出优化方法可降低15%能耗且最小化推理时间损失。


<details>
  <summary>Details</summary>
Motivation: 边缘加速器如Nvidia Jetson在计算连续体中日益重要，但缺乏对其功耗模式性能行为的原理性研究，需要从第一性原理分析DNN工作负载的性能表现。

Method: 开发时间屋顶线和能量屋顶线模型，结合DNN推理工作负载的计算(FLOP)和内存访问(字节)分析模型，对Jetson Orin AGX的不同功耗模式进行原理性分析。

Result: 揭示了DNN工作负载在边缘加速器上的独特见解：默认MAXN模式并非最节能，时间效率在所有功耗模式下都意味着能量效率。通过优化功耗模式可实现15%的能耗降低且推理时间损失最小。

Conclusion: 提出的屋顶线模型和分析方法能够有效理解和优化边缘加速器的DNN推理性能，为能量和功耗受限的部署提供了重要指导。

Abstract: Edge accelerators such as Nvidia Jetsons are becoming an integral part of the
computing continuum, and are often used for DNN inferencing and training.
Nvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power
envelope and offer $1000$s of power modes to customize CPU, GPU and memory
frequencies. Their widely varying power--performance trade-offs can be
exploited for energy and power-constrained deployments. While data-driven
methods to predict the power and latency of DNN workloads for edge devices
exist, there is a lack of principled study to understand why edge accelerators
and their power modes perform the way they do. We develop a time roofline and a
novel energy roofline model for the Jetson Orin AGX for diverse power modes,
and couple it with an analytical model of the compute (FLOP) and memory access
(bytes) for DNN inference workloads to analyze them from first principles.
These reveal unique, sometimes counter-intuitive, insights into the power and
performance behavior of DNN workloads on edge accelerators, e.g., the default
power mode MAXN is not the most energy efficient and time efficiency implies
energy efficiency for all power modes. We also extend our analytical roofline
models to DNN training. Finally, we apply these methods to tune the power mode
(and hence the roofline) of the edge device to optimize the latency and energy
for DNN inference, with up to $15\%$ lower energy and minimal degradation in
inference time.

</details>


### [26] [Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge Accelerators](https://arxiv.org/abs/2509.20205)
*Prashanthi S. K.,Saisamarth Taluri,Pranav Gupta,Amartya Ranjan Saikia,Kunal Kumar Sahoo,Atharva Vinay Joshi,Lakshya Karwa,Kedar Dhule,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文提出了一种智能时间切片方法GMD和ALS，用于在Jetson边缘设备上同时进行DNN训练和推理，通过优化功率模式和推理批次大小来最大化训练吞吐量，同时满足延迟和功耗预算。


<details>
  <summary>Details</summary>
Motivation: 随着GPU加速边缘设备的普及和隐私问题的增加，需要在边缘设备上同时进行DNN训练和推理。但Jetson等设备不支持原生GPU共享且具有数千种功率模式，需要仔细的时间共享来满足功耗性能目标。

Method: 设计了Fulcrum调度器，包含GMD（高效多维梯度下降搜索，仅分析15种功率模式）和ALS（主动学习技术，识别可重用的帕累托最优功率模式，分析50-150种功率模式）。

Result: 在15个DNN工作负载的273,000+配置中，ALS和GMD优于需要更大规模分析的基准方法。97%以上的运行满足延迟和功耗预算，平均吞吐量接近最优值的7%以内。

Conclusion: 提出的智能时间切片方法能够有效管理边缘设备上并发DNN训练和推理任务，以较小的分析成本实现接近最优的性能表现。

Abstract: The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the
rise in privacy concerns are placing an emphasis on concurrent DNN training and
inferencing on edge devices. Inference and training have different computing
and QoS goals. But edge accelerators like Jetson do not support native GPU
sharing and expose 1000s of power modes. This requires careful time-sharing of
concurrent workloads to meet power--performance goals, while limiting costly
profiling. In this paper, we design an intelligent time-slicing approach for
concurrent DNN training and inferencing on Jetsons. We formulate an
optimization problem to interleave training and inferencing minibatches, and
decide the device power mode and inference minibatch size, while maximizing the
training throughput and staying within latency and power budgets, with modest
profiling costs. We propose GMD, an efficient multi-dimensional gradient
descent search which profiles just $15$ power modes; and ALS, an Active
Learning technique which identifies reusable Pareto-optimal power modes, but
profiles $50$--$150$ power modes. We evaluate these within our Fulcrum
scheduler for $273,000+$ configurations across $15$ DNN workloads. We also
evaluate our strategies on dynamic arrival inference and concurrent inferences.
ALS and GMD outperform simpler and more complex baselines with larger-scale
profiling. Their solutions satisfy the latency and power budget for $>97\%$ of
our runs, and on average are within $7\%$ of the optimal throughput.

</details>


### [27] [An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications](https://arxiv.org/abs/2509.20223)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.DC

TL;DR: 本文对联邦学习在自动驾驶车辆应用中的安全性进行实证分析，重点研究安全聚合技术和多方计算在应对网络攻击时的有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私并提升模型性能，但仍易受投毒攻击和推理攻击等网络威胁，需要更强大的安全措施来保护自动驾驶系统。

Method: 使用交通图像数据集（如LISA交通灯数据集），采用多种安全聚合技术和多方计算协议，在存在不同类型网络攻击的环境下进行实证分析。

Result: 研究评估了各种安全联邦学习聚合技术和多方计算在训练和推理阶段保护自动驾驶应用免受网络威胁的韧性。

Conclusion: 多方计算作为先进的安全机制，能够为标准化的安全聚合提供隐私保护，有助于防止自动驾驶学习模型被误导而导致交通灯错误分类等严重后果。

Abstract: Federated Learning lends itself as a promising paradigm in enabling
distributed learning for autonomous vehicles applications and ensuring data
privacy while enhancing and refining predictive model performance through
collaborative training on edge client vehicles. However, it remains vulnerable
to various categories of cyber-attacks, necessitating more robust security
measures to effectively mitigate potential threats. Poisoning attacks and
inference attacks are commonly initiated within the federated learning
environment to compromise secure system performance. Secure aggregation can
limit the disclosure of sensitive information from outsider and insider
attackers of the federated learning environment. In this study, our aim is to
conduct an empirical analysis on the transportation image dataset (e.g., LISA
traffic light) using various secure aggregation techniques and multiparty
computation in the presence of diverse categories of cyber-attacks. Multiparty
computation serves as a state-of-the-art security mechanism, offering standard
privacy for secure aggregation of edge autonomous vehicles local model updates
through various security protocols. The presence of adversaries can mislead the
autonomous vehicle learning model, leading to the misclassification of traffic
lights, and resulting in detrimental impacts. This empirical study explores the
resilience of various secure federated learning aggregation techniques and
multiparty computation in safeguarding autonomous vehicle applications against
various cyber threats during both training and inference times.

</details>


### [28] [xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G Wireless Networks for Real-Time Digital Agriculture](https://arxiv.org/abs/2509.20340)
*Liubov Kurafeeva,Alan Subedi,Ryan Hartung,Michael Fay,Avhishek Biswas,Shantenu Jha,Ozgur O. Kilic,Chandra Krintz,Andre Merzky,Douglas Thain,Mehmet C. Vuran,Rich Wolski*

Main category: cs.DC

TL;DR: xGFabric系统通过私有5G网络将传感器网络与高性能计算设施耦合，实现实时数字农业模拟


<details>
  <summary>Details</summary>
Motivation: 数字农业中的柑橘保护屏研究需要将分布式传感器网络与集中式高性能计算设施耦合，以进行环境条件建模和实时干预

Method: 开发xGFabric端到端系统，通过5G网络切片将远程传感器连接到HPC系统

Result: 原型系统成功实现了传感器网络与HPC设施的实时耦合

Conclusion: 私有5G网络为解决边缘传感器网络与HPC模拟的近实时耦合挑战提供了新颖能力

Abstract: Advanced scientific applications require coupling distributed sensor networks
with centralized high-performance computing facilities. Citrus Under Protective
Screening (CUPS) exemplifies this need in digital agriculture, where citrus
research facilities are instrumented with numerous sensors monitoring
environmental conditions and detecting protective screening damage. CUPS
demands access to computational fluid dynamics codes for modeling environmental
conditions and guiding real-time interventions like water application or
robotic repairs. These computing domains have contrasting properties: sensor
networks provide low-performance, limited-capacity, unreliable data access,
while high-performance facilities offer enormous computing power through
high-latency batch processing. Private 5G networks present novel capabilities
addressing this challenge by providing low latency, high throughput, and
reliability necessary for near-real-time coupling of edge sensor networks with
HPC simulations. This work presents xGFabric, an end-to-end system coupling
sensor networks with HPC facilities through Private 5G networks. The prototype
connects remote sensors via 5G network slicing to HPC systems, enabling
real-time digital agriculture simulation.

</details>
