<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity](https://arxiv.org/abs/2509.14276)
*Yuxiang Mai,Qiyue Yin,Wancheng Ni,Pei Xu,Kaiqi Huang*

Main category: cs.MA

TL;DR: CoDiCon通过引入竞争激励促进合作智能体间的策略多样性，提升多智能体强化学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视智能体间互动影响，缺乏策略多样性。

Method: 设计基于排名特征的内在奖励机制，平衡竞争与合作，优化双层约束问题。

Result: 在SMAC和GRF环境中性能优于现有方法，策略更富多样性与适应性。

Conclusion: 适度竞争可有效提升合作多智能体系统的策略表现。

Abstract: In recent years, diversity has emerged as a useful mechanism to enhance the
efficiency of multi-agent reinforcement learning (MARL). However, existing
methods predominantly focus on designing policies based on individual agent
characteristics, often neglecting the interplay and mutual influence among
agents during policy formation. To address this gap, we propose Competitive
Diversity through Constructive Conflict (CoDiCon), a novel approach that
incorporates competitive incentives into cooperative scenarios to encourage
policy exchange and foster strategic diversity among agents. Drawing
inspiration from sociological research, which highlights the benefits of
moderate competition and constructive conflict in group decision-making, we
design an intrinsic reward mechanism using ranking features to introduce
competitive motivations. A centralized intrinsic reward module generates and
distributes varying reward values to agents, ensuring an effective balance
between competition and cooperation. By optimizing the parameterized
centralized reward module to maximize environmental rewards, we reformulate the
constrained bilevel optimization problem to align with the original task
objectives. We evaluate our algorithm against state-of-the-art methods in the
SMAC and GRF environments. Experimental results demonstrate that CoDiCon
achieves superior performance, with competitive intrinsic rewards effectively
promoting diverse and adaptive strategies among cooperative agents.

</details>


### [2] [LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.14680)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Dong Huang,Yuanye Zhao,Zheng Lin,Zihan Fang,Dianxin Luan,Heming Cui,Yong Cui*

Main category: cs.MA

TL;DR: LEED框架结合大语言模型生成专家示范，提升多智能体强化学习的样本效率、时间效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习在智能体数量增加时的协调与可扩展性瓶颈问题。

Method: 提出LEED框架，包含示范生成模块（利用大语言模型生成环境交互指令）和策略优化模块（结合专家策略损失与个体策略损失进行去中心化训练）。

Result: 实验表明LEED在样本效率、时间效率和鲁棒可扩展性方面优于现有基线方法。

Conclusion: LEED有效融合专家知识与个体经验，显著提升多智能体强化学习性能。

Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for
intelligent decision-making in complex environments. However, it suffers from a
coordination and scalability bottleneck as the number of agents increases. To
address these issues, we propose the LLM-empowered expert demonstrations
framework for multi-agent reinforcement learning (LEED). LEED consists of two
components: a demonstration generation (DG) module and a policy optimization
(PO) module. Specifically, the DG module leverages large language models to
generate instructions for interacting with the environment, thereby producing
high-quality demonstrations. The PO module adopts a decentralized training
paradigm, where each agent utilizes the generated demonstrations to construct
an expert policy loss, which is then integrated with its own policy loss. This
enables each agent to effectively personalize and optimize its local policy
based on both expert knowledge and individual experience. Experimental results
show that LEED achieves superior sample efficiency, time efficiency, and robust
scalability compared to state-of-the-art baselines.

</details>


### [3] [Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.15103)
*Simin Li,Zheng Yuwei,Zihao Mao,Linhao Wang,Ruixiao Xu,Chengdong Ma,Xin Yu,Yuqing Ma,Qi Dou,Xin Wang,Jie Luo,Bo An,Yaodong Yang,Weifeng Lv,Xianglong Liu*

Main category: cs.MA

TL;DR: 本文提出HAD-MFC框架，通过解耦分层对抗控制，高效识别大规模多智能体系统中最脆弱的智能体。


<details>
  <summary>Details</summary>
Motivation: 大规模系统中部分智能体失效不可避免，需定位最易导致系统性能崩溃的关键智能体。

Method: 构建分层对抗均值场控制框架，利用Fenchel-Rockafellar变换解耦上下层问题，将组合优化转为带密集奖励的MDP，结合贪心与强化学习求解。

Result: 实验证明方法能有效识别脆弱智能体，诱导系统更严重失效，并学习反映各智能体脆弱性的价值函数。

Conclusion: 该方法在保持原问题最优解前提下，显著降低计算复杂度，为大规模多智能体系统安全分析提供新思路。

Abstract: Partial agent failure becomes inevitable when systems scale up, making it
crucial to identify the subset of agents whose compromise would most severely
degrade overall performance. In this paper, we study this Vulnerable Agent
Identification (VAI) problem in large-scale multi-agent reinforcement learning
(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field
Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task
of selecting the most vulnerable agents, and the lower level learns worst-case
adversarial policies for these agents using mean-field MARL. The two problems
are coupled together, making HAD-MFC difficult to solve. To solve this, we
first decouple the hierarchical process by Fenchel-Rockafellar transform,
resulting a regularized mean-field Bellman operator for upper level that
enables independent learning at each level, thus reducing computational
complexity. We then reformulate the upper-level combinatorial problem as a MDP
with dense rewards from our regularized mean-field Bellman operator, enabling
us to sequentially identify the most vulnerable agents by greedy and RL
algorithms. This decomposition provably preserves the optimal solution of the
original HAD-MFC. Experiments show our method effectively identifies more
vulnerable agents in large-scale MARL and the rule-based system, fooling system
into worse failures, and learns a value function that reveals the vulnerability
of each agent.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures](https://arxiv.org/abs/2509.14920)
*Amine Barrak,Fabio Petrillo,Fehmi Jaafar*

Main category: cs.DC

TL;DR: 本文比较了SPIRT与ScatterReduce、AllReduce、MLLess等无服务器分布式机器学习架构，发现SPIRT在训练效率、通信开销和长期成本效益方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 应对分布式机器学习对可扩展、低成本训练方案日益增长的需求。

Method: 通过对比分析SPIRT与传统架构在训练时间、通信开销、容错性及成本等关键指标上的表现。

Result: SPIRT通过并行批处理与RedisAI数据库内操作显著降低训练时间和通信开销，长期成本效益更优，但初始部署成本较高。

Conclusion: SPIRT在多项指标上优于传统架构，为未来融合各架构优势的新模型研究奠定基础。

Abstract: The field of distributed machine learning (ML) faces increasing demands for
scalable and cost-effective training solutions, particularly in the context of
large, complex models. Serverless computing has emerged as a promising paradigm
to address these challenges by offering dynamic scalability and
resource-efficient execution. Building upon our previous work, which introduced
the Serverless Peer Integrated for Robust Training (SPIRT) architecture, this
paper presents a comparative analysis of several serverless distributed ML
architectures. We examine SPIRT alongside established architectures like
ScatterReduce, AllReduce, and MLLess, focusing on key metrics such as training
time efficiency, cost-effectiveness, communication overhead, and fault
tolerance capabilities. Our findings reveal that SPIRT provides significant
improvements in reducing training times and communication overhead through
strategies such as parallel batch processing and in-database operations
facilitated by RedisAI. However, traditional architectures exhibit scalability
challenges and varying degrees of vulnerability to faults and adversarial
attacks. The cost analysis underscores the long-term economic benefits of SPIRT
despite its higher initial setup costs. This study not only highlights the
strengths and limitations of current serverless ML architectures but also sets
the stage for future research aimed at developing new models that combine the
most effective features of existing systems.

</details>


### [5] [Conditional Prior-based Non-stationary Channel Estimation Using Accelerated Diffusion Models](https://arxiv.org/abs/2509.15182)
*Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Umer,Asad Aali,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出基于条件先验扩散的信道估计方法，在动态城市微蜂窝环境中显著优于传统和深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 城市微蜂窝环境中无线信道因移动性和散射体动态呈现非平稳性，导致传统估计器性能下降。

Method: 采用条件先验扩散模型，结合时序编码器与跨时注意力机制压缩观测窗口，通过特征调制引导去噪，并利用SNR匹配初始化与几何步长调度加速推理。

Result: 在3GPP基准测试中，所有信噪比下NMSE均优于LMMSE、GMM、LSTM和LDAMP等基线方法，尤其在高信噪比下表现优异。

Conclusion: 该方法在非平稳信道中具备稳定性能与高保真度，适合动态无线环境下的高效信道估计。

Abstract: Wireless channels in motion-rich urban microcell (UMi) settings are
non-stationary; mobility and scatterer dynamics shift the distribution over
time, degrading classical and deep estimators. This work proposes conditional
prior diffusion for channel estimation, which learns a history-conditioned
score to denoise noisy channel snapshots. A temporal encoder with cross-time
attention compresses a short observation window into a context vector, which
captures the channel's instantaneous coherence and steers the denoiser via
feature-wise modulation. In inference, an SNR-matched initialization selects
the diffusion step whose marginal aligns with the measured input SNR, and the
process follows a shortened, geometrically spaced schedule, preserving the
signal-to-noise trajectory with far fewer iterations. Temporal
self-conditioning with the previous channel estimate and a training-only
smoothness penalty further stabilizes evolution without biasing the test-time
estimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than
LMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and
strong high SNR fidelity.

</details>


### [6] [Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization](https://arxiv.org/abs/2509.15192)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ibtsaam Qadir,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 本文提出基于损失正则化的持续学习框架，以解决无线信道预测中的灾难性遗忘问题，其中SI方法在高信噪比下显著降低NMSE且内存效率更优。


<details>
  <summary>Details</summary>
Motivation: 传统预测器在跨配置切换时性能急剧下降，需解决分布偏移下的灾难性遗忘问题。

Method: 采用弹性权重固化（EWC）和突触智能（SI）两种正则化策略，在训练目标中加入惩罚项，选择性保留旧配置关键参数。

Result: 在3GPP场景中，SI使高信噪比NMSE降低最多1.8dB（约32-34%），EWC降低最多1.4dB（约17-28%）；SI内存复杂度为O(M)，优于EWC的O(MK)。

Conclusion: SI在性能和内存效率上均优于EWC，更适合资源受限的无线基础设施部署。

Abstract: Modern wireless networks face critical challenges when mobile users traverse
heterogeneous network configurations with varying antenna layouts, carrier
frequencies, and scattering statistics. Traditional predictors degrade under
distribution shift, with NMSE rising by 37.5\% during cross-configuration
handovers. This work addresses catastrophic forgetting in channel prediction by
proposing a continual learning framework based on loss regularization. The
approach augments standard training objectives with penalty terms that
selectively preserve network parameters essential for previous configurations
while enabling adaptation to new environments. Two prominent regularization
strategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic
Intelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers
the high-SNR NMSE floor by up to 1.8 dB ($\approx$32--34\%), while EWC achieves
up to 1.4 dB ($\approx$17--28\%). Notably, standard EWC incurs
$\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and
corresponding parameter snapshots across $K$ tasks) unless consolidated,
whereas SI maintains $\mathcal{O}(M)$ memory complexity (storing $M$ model
parameters), independent of task sequence length, making it suitable for
resource-constrained wireless infrastructure

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [7] [Taming Serverless Cold Starts Through OS Co-Design](https://arxiv.org/abs/2509.14292)
*Ben Holmes,Baltasar Dinis,Lana Honcharuk,Joshua Fried,Adam Belay*

Main category: cs.OS

TL;DR: Spice通过与操作系统深度集成，实现从磁盘快速恢复无服务器函数状态，显著降低冷启动延迟，打破性能与内存弹性的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决无服务器计算中因冷启动导致的高延迟问题，突破现有系统依赖内存驻留状态的限制。

Method: 设计Spice执行引擎，直接与OS集成以高效恢复内核状态，并引入专用内存映射原语避免运行时页错误。

Result: 相比最先进的进程和虚拟机系统，冷启动延迟分别降低14.9倍和10.6倍，实现接近热启动的性能。

Conclusion: 证明在无服务器计算中，高性能与内存弹性可兼得，无需再做取舍。

Abstract: Serverless computing promises fine-grained elasticity and operational
simplicity, fueling widespread interest from both industry and academia. Yet
this promise is undercut by the cold setart problem, where invoking a function
after a period of inactivity triggers costly initialization before any work can
begin. Even with today's high-speed storage, the prevailing view is that
achieving sub-millisecond cold starts requires keeping state resident in
memory.
  This paper challenges that assumption. Our analysis of existing
snapshot/restore mechanisms show that OS-level limitations, not storage speed,
are the real barrier to ultra-fast restores from disk. These limitations force
current systems to either restore state piecemeal in a costly manner or capture
too much state, leading to longer restore times and unpredictable performance.
Futhermore, current memory primitives exposed by the OS make it difficult to
reliably fetch data into memory and avoid costly runtime page faults.
  To overcome these barriers, we present Spice, an execution engine
purpose-built for serverless snapshot/restore. Spice integrates directly with
the OS to restore kernel state without costly replay and introduces dedicated
primitives for restoring memory mappings efficiently and reliably. As a result,
Spice delivers near-warm performance on cold restores from disk, reducing
latency by up to 14.9x over state-of-the-art process-based systems and 10.6x
over VM-based systems. This proves that high performance and memory elasticity
no longer need to be a trade-off in serverless computing.

</details>
