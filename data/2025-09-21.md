<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity](https://arxiv.org/abs/2509.14276)
*Yuxiang Mai,Qiyue Yin,Wancheng Ni,Pei Xu,Kaiqi Huang*

Main category: cs.MA

TL;DR: 提出CoDiCon方法，通过引入竞争激励促进合作场景中智能体策略多样性，提升多智能体强化学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视智能体间互动影响，难以有效激发策略多样性。

Method: 设计基于排名特征的内在奖励机制，平衡竞争与合作，优化集中式奖励模块以对齐任务目标。

Result: 在SMAC和GRF环境中优于现有方法，有效提升智能体策略多样性与适应性。

Conclusion: 适度竞争与建设性冲突可显著增强合作型多智能体系统的策略表现。

Abstract: In recent years, diversity has emerged as a useful mechanism to enhance the
efficiency of multi-agent reinforcement learning (MARL). However, existing
methods predominantly focus on designing policies based on individual agent
characteristics, often neglecting the interplay and mutual influence among
agents during policy formation. To address this gap, we propose Competitive
Diversity through Constructive Conflict (CoDiCon), a novel approach that
incorporates competitive incentives into cooperative scenarios to encourage
policy exchange and foster strategic diversity among agents. Drawing
inspiration from sociological research, which highlights the benefits of
moderate competition and constructive conflict in group decision-making, we
design an intrinsic reward mechanism using ranking features to introduce
competitive motivations. A centralized intrinsic reward module generates and
distributes varying reward values to agents, ensuring an effective balance
between competition and cooperation. By optimizing the parameterized
centralized reward module to maximize environmental rewards, we reformulate the
constrained bilevel optimization problem to align with the original task
objectives. We evaluate our algorithm against state-of-the-art methods in the
SMAC and GRF environments. Experimental results demonstrate that CoDiCon
achieves superior performance, with competitive intrinsic rewards effectively
promoting diverse and adaptive strategies among cooperative agents.

</details>


### [2] [LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.14680)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Dong Huang,Yuanye Zhao,Zheng Lin,Zihan Fang,Dianxin Luan,Heming Cui,Yong Cui*

Main category: cs.MA

TL;DR: LEED框架利用大语言模型生成高质量示范，提升多智能体强化学习的样本效率、时间效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习在智能体数量增加时的协调与扩展瓶颈问题。

Method: 提出LEED框架，包含示范生成模块（利用大语言模型）与策略优化模块（结合专家策略与个体经验进行去中心化训练）。

Result: 实验表明LEED在样本效率、时间效率与可扩展性上优于现有基线方法。

Conclusion: LEED有效缓解多智能体强化学习的协调与扩展难题，具备实际应用潜力。

Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for
intelligent decision-making in complex environments. However, it suffers from a
coordination and scalability bottleneck as the number of agents increases. To
address these issues, we propose the LLM-empowered expert demonstrations
framework for multi-agent reinforcement learning (LEED). LEED consists of two
components: a demonstration generation (DG) module and a policy optimization
(PO) module. Specifically, the DG module leverages large language models to
generate instructions for interacting with the environment, thereby producing
high-quality demonstrations. The PO module adopts a decentralized training
paradigm, where each agent utilizes the generated demonstrations to construct
an expert policy loss, which is then integrated with its own policy loss. This
enables each agent to effectively personalize and optimize its local policy
based on both expert knowledge and individual experience. Experimental results
show that LEED achieves superior sample efficiency, time efficiency, and robust
scalability compared to state-of-the-art baselines.

</details>


### [3] [Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.15103)
*Simin Li,Zheng Yuwei,Zihao Mao,Linhao Wang,Ruixiao Xu,Chengdong Ma,Xin Yu,Yuqing Ma,Qi Dou,Xin Wang,Jie Luo,Bo An,Yaodong Yang,Weifeng Lv,Xianglong Liu*

Main category: cs.MA

TL;DR: 本文提出HAD-MFC框架，通过解耦分层对抗控制，高效识别大规模多智能体系统中最脆弱的智能体。


<details>
  <summary>Details</summary>
Motivation: 系统规模扩大导致部分智能体失效不可避免，需识别最易被攻击的智能体以保障整体性能。

Method: 构建分层对抗均值场控制框架，利用Fenchel-Rockafellar变换解耦上下层问题，上层转为MDP并用贪婪/RL求解，下层学习对抗策略。

Result: 实验表明方法能有效识别脆弱智能体，诱导系统更严重失效，并学习反映各智能体脆弱性的价值函数。

Conclusion: 所提方法在保持最优解前提下显著降低计算复杂度，为大规模MARL系统安全分析提供新思路。

Abstract: Partial agent failure becomes inevitable when systems scale up, making it
crucial to identify the subset of agents whose compromise would most severely
degrade overall performance. In this paper, we study this Vulnerable Agent
Identification (VAI) problem in large-scale multi-agent reinforcement learning
(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field
Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task
of selecting the most vulnerable agents, and the lower level learns worst-case
adversarial policies for these agents using mean-field MARL. The two problems
are coupled together, making HAD-MFC difficult to solve. To solve this, we
first decouple the hierarchical process by Fenchel-Rockafellar transform,
resulting a regularized mean-field Bellman operator for upper level that
enables independent learning at each level, thus reducing computational
complexity. We then reformulate the upper-level combinatorial problem as a MDP
with dense rewards from our regularized mean-field Bellman operator, enabling
us to sequentially identify the most vulnerable agents by greedy and RL
algorithms. This decomposition provably preserves the optimal solution of the
original HAD-MFC. Experiments show our method effectively identifies more
vulnerable agents in large-scale MARL and the rule-based system, fooling system
into worse failures, and learns a value function that reveals the vulnerability
of each agent.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [4] [Taming Serverless Cold Starts Through OS Co-Design](https://arxiv.org/abs/2509.14292)
*Ben Holmes,Baltasar Dinis,Lana Honcharuk,Joshua Fried,Adam Belay*

Main category: cs.OS

TL;DR: Spice通过与操作系统深度集成，实现从磁盘快速恢复无服务器函数状态，显著降低冷启动延迟，打破性能与内存弹性的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决无服务器计算中冷启动问题，当前系统受限于操作系统机制，导致恢复缓慢或性能不稳定。

Method: 设计Spice执行引擎，直接与OS集成以高效恢复内核状态，并提供专用内存映射原语避免运行时页错误。

Result: 相比最先进的进程和虚拟机系统，冷启动延迟分别降低14.9倍和10.6倍，实现接近热启动的性能。

Conclusion: 在无服务器计算中，高性能与内存弹性不再需要权衡，Spice为此提供了有效解决方案。

Abstract: Serverless computing promises fine-grained elasticity and operational
simplicity, fueling widespread interest from both industry and academia. Yet
this promise is undercut by the cold setart problem, where invoking a function
after a period of inactivity triggers costly initialization before any work can
begin. Even with today's high-speed storage, the prevailing view is that
achieving sub-millisecond cold starts requires keeping state resident in
memory.
  This paper challenges that assumption. Our analysis of existing
snapshot/restore mechanisms show that OS-level limitations, not storage speed,
are the real barrier to ultra-fast restores from disk. These limitations force
current systems to either restore state piecemeal in a costly manner or capture
too much state, leading to longer restore times and unpredictable performance.
Futhermore, current memory primitives exposed by the OS make it difficult to
reliably fetch data into memory and avoid costly runtime page faults.
  To overcome these barriers, we present Spice, an execution engine
purpose-built for serverless snapshot/restore. Spice integrates directly with
the OS to restore kernel state without costly replay and introduces dedicated
primitives for restoring memory mappings efficiently and reliably. As a result,
Spice delivers near-warm performance on cold restores from disk, reducing
latency by up to 14.9x over state-of-the-art process-based systems and 10.6x
over VM-based systems. This proves that high performance and memory elasticity
no longer need to be a trade-off in serverless computing.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures](https://arxiv.org/abs/2509.14920)
*Amine Barrak,Fabio Petrillo,Fehmi Jaafar*

Main category: cs.DC

TL;DR: 本文比较了多种无服务器分布式机器学习架构，发现SPIRT在训练效率与通信开销上表现优异，具备长期成本优势。


<details>
  <summary>Details</summary>
Motivation: 应对分布式机器学习对可扩展性与成本效益日益增长的需求。

Method: 对比分析SPIRT、ScatterReduce、AllReduce和MLLess等架构在训练时间、成本、通信开销与容错性上的表现。

Result: SPIRT通过并行批处理与RedisAI操作显著降低训练时间和通信开销，长期经济性更优，但初始成本较高。

Conclusion: SPIRT综合性能领先，为未来融合各架构优势的新模型研究奠定基础。

Abstract: The field of distributed machine learning (ML) faces increasing demands for
scalable and cost-effective training solutions, particularly in the context of
large, complex models. Serverless computing has emerged as a promising paradigm
to address these challenges by offering dynamic scalability and
resource-efficient execution. Building upon our previous work, which introduced
the Serverless Peer Integrated for Robust Training (SPIRT) architecture, this
paper presents a comparative analysis of several serverless distributed ML
architectures. We examine SPIRT alongside established architectures like
ScatterReduce, AllReduce, and MLLess, focusing on key metrics such as training
time efficiency, cost-effectiveness, communication overhead, and fault
tolerance capabilities. Our findings reveal that SPIRT provides significant
improvements in reducing training times and communication overhead through
strategies such as parallel batch processing and in-database operations
facilitated by RedisAI. However, traditional architectures exhibit scalability
challenges and varying degrees of vulnerability to faults and adversarial
attacks. The cost analysis underscores the long-term economic benefits of SPIRT
despite its higher initial setup costs. This study not only highlights the
strengths and limitations of current serverless ML architectures but also sets
the stage for future research aimed at developing new models that combine the
most effective features of existing systems.

</details>


### [6] [Conditional Prior-based Non-stationary Channel Estimation Using Accelerated Diffusion Models](https://arxiv.org/abs/2509.15182)
*Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Umer,Asad Aali,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出基于条件先验扩散的信道估计方法，在动态城市微蜂窝环境中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 动态环境导致信道非平稳，传统估计器性能下降。

Method: 使用时序编码器与交叉注意力压缩观测窗口，结合SNR匹配初始化和几何步长调度进行去噪。

Result: 在3GPP基准测试中，NMSE优于LMMSE、GMM、LSTM和LDAMP等方法，尤其在高SNR下表现优异。

Conclusion: 该方法稳定高效，适合高动态无线信道估计场景。

Abstract: Wireless channels in motion-rich urban microcell (UMi) settings are
non-stationary; mobility and scatterer dynamics shift the distribution over
time, degrading classical and deep estimators. This work proposes conditional
prior diffusion for channel estimation, which learns a history-conditioned
score to denoise noisy channel snapshots. A temporal encoder with cross-time
attention compresses a short observation window into a context vector, which
captures the channel's instantaneous coherence and steers the denoiser via
feature-wise modulation. In inference, an SNR-matched initialization selects
the diffusion step whose marginal aligns with the measured input SNR, and the
process follows a shortened, geometrically spaced schedule, preserving the
signal-to-noise trajectory with far fewer iterations. Temporal
self-conditioning with the previous channel estimate and a training-only
smoothness penalty further stabilizes evolution without biasing the test-time
estimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than
LMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and
strong high SNR fidelity.

</details>


### [7] [Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization](https://arxiv.org/abs/2509.15192)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ibtsaam Qadir,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 本文提出基于损失正则化的持续学习框架，解决无线信道预测中的灾难性遗忘问题，Synaptic Intelligence在高信噪比下显著降低NMSE且内存效率更优。


<details>
  <summary>Details</summary>
Motivation: 传统预测器在跨配置切换时性能急剧下降，需解决分布偏移下的灾难性遗忘。

Method: 采用EWC和SI两种正则化策略，在训练目标中加入惩罚项，选择性保留旧配置关键参数并适应新环境。

Result: SI在高信噪比下NMSE降低1.8dB（约32-34%），EWC降低1.4dB（约17-28%）；SI内存复杂度为O(M)，优于EWC的O(MK)。

Conclusion: SI在性能与资源效率上均优于EWC，更适合资源受限的无线基础设施部署。

Abstract: Modern wireless networks face critical challenges when mobile users traverse
heterogeneous network configurations with varying antenna layouts, carrier
frequencies, and scattering statistics. Traditional predictors degrade under
distribution shift, with NMSE rising by 37.5\% during cross-configuration
handovers. This work addresses catastrophic forgetting in channel prediction by
proposing a continual learning framework based on loss regularization. The
approach augments standard training objectives with penalty terms that
selectively preserve network parameters essential for previous configurations
while enabling adaptation to new environments. Two prominent regularization
strategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic
Intelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers
the high-SNR NMSE floor by up to 1.8 dB ($\approx$32--34\%), while EWC achieves
up to 1.4 dB ($\approx$17--28\%). Notably, standard EWC incurs
$\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and
corresponding parameter snapshots across $K$ tasks) unless consolidated,
whereas SI maintains $\mathcal{O}(M)$ memory complexity (storing $M$ model
parameters), independent of task sequence length, making it suitable for
resource-constrained wireless infrastructure

</details>
