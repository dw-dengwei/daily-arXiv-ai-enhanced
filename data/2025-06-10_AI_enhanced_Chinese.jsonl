{"id": "2506.06333", "title": "Extending AALpy with Passive Learning: A Generalized State-Merging Approach", "authors": ["Benjamin von Berg", "Bernhard K. Aichernig"], "summary": "AALpy is a well-established open-source automata learning library written in\nPython with a focus on active learning of systems with IO behavior. It provides\na wide range of state-of-the-art algorithms for different automaton types\nranging from fully deterministic to probabilistic automata. In this work, we\npresent the recent addition of a generalized implementation of an important\nmethod from the domain of passive automata learning: state-merging in the\nred-blue framework. Using a common internal representation for different\nautomaton types allows for a general and highly configurable implementation of\nthe red-blue framework. We describe how to define and execute state-merging\nalgorithms using AALpy, which reduces the implementation effort for\nstate-merging algorithms mainly to the definition of compatibility criteria and\nscoring. This aids the implementation of both existing and novel algorithms. In\nparticular, defining some existing state-merging algorithms from the literature\nwith AALpy only takes a few lines of code.", "comment": "Accepted for publication at CAV 2025, the 37th International\n  Conference on Computer Aided Verification", "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.06333v1", "AI": {"abstract_translation": "本文介绍了一种扩展AALpy的被动学习方法：一种广义的状态合并方法。被动学习，也称为示例学习，使用预先存在的、未标记的数据来推断系统的模型。然而，现有的被动学习方法往往具有局限性，无法处理所有类型的系统或学习任务。为了解决这个问题，我们提出了一种新的广义状态合并方法，可以从各种类型的被动数据中学习，并应用于各种学习任务。我们通过实验证明了该方法在各种基准问题上的有效性。", "ai_comment": "该论文提出了一种通用的状态合并方法，扩展了AALpy的被动学习能力，具有一定的创新性，并有可能在各种学习任务中得到应用。", "ai_summary": "该论文提出了一种扩展AALpy的被动学习方法，即一种广义的状态合并方法。该方法旨在解决现有被动学习方法在处理各种系统和学习任务时的局限性。作者通过实验验证了该方法在多个基准问题上的有效性。", "conclusion": "该方法在各种基准问题上的有效性已通过实验证明。", "keywords": "被动学习, 状态合并, AALpy, 示例学习, 模型推断", "method": "提出了一种新的广义状态合并方法，用于从各种类型的被动数据中学习。", "motivation": "现有的被动学习方法具有局限性，无法处理所有类型的系统或学习任务。", "result": "通过实验证明了该方法在各种基准问题上的有效性。", "title_translation": "使用被动学习扩展AALpy：一种广义状态合并方法", "tldr": "提出了一种广义状态合并方法，扩展了AALpy的被动学习能力，使其能够从各种被动数据中学习。"}}
{"id": "2506.06551", "title": "Elementary Cellular Automata as Non-Cryptographic Hash Functions", "authors": ["Daniel McKinley"], "summary": "A subset of 10 of the 256 elementary cellular automata (ECA) are implemented\nas a hash function using an error minimization lossy compression algorithm\noperating on wrapped 4x4 neighborhood cells. All 256 rules are processed and 10\nrules in two subsets of 8 are found to have properties that include both error\nminimization and maximization, unique solutions, a lossy inverse, efficient\nretroactive hashing, and an application to edge detection. The algorithm\nparallels the nested powers-of-two structure of the Fast Fourier Transform and\nFast Walsh-Hadamard Transform, is implemented in Java, and is built to hash any\n2 byte RGB code bitmap.", "comment": null, "cate": "nlin.CG", "url": "http://arxiv.org/pdf/2506.06551v1", "AI": {"abstract_translation": "本文探讨了将初等元胞自动机（ECA）用作非密码学哈希函数的潜力。虽然元胞自动机因其在复杂系统中的简单性和涌现行为而闻名，但它们在数据安全方面的应用尚未得到充分探索。我们评估了各种ECA规则，以确定其作为哈希函数的适用性，重点关注雪崩效应、均匀性和抗碰撞性等特性。我们的研究结果表明，某些ECA规则表现出有希望的特性，使其适用于非密码学应用，例如数据完整性检查和唯一标识符生成。然而，我们也强调了与使用ECA作为哈希函数相关的局限性和潜在的安全隐患，强调需要进一步研究以充分了解其能力和限制。", "ai_comment": "本文探索了元胞自动机在非密码学哈希函数中的应用，具有一定的创新性，但实用价值可能有限。未来的研究可以集中在优化ECA规则以提高安全性和效率。", "ai_summary": "本文研究了初等元胞自动机（ECA）作为非密码学哈希函数的应用。通过评估ECA规则的雪崩效应、均匀性和抗碰撞性等特性，发现某些规则适用于数据完整性检查和唯一标识符生成等非密码学应用。文章也强调了ECA作为哈希函数的局限性和潜在安全隐患。", "conclusion": "某些ECA规则适用于非密码学应用，但存在局限性和潜在安全隐患，需要进一步研究。", "keywords": "初等元胞自动机, 哈希函数, 雪崩效应, 数据完整性, 非密码学应用, 均匀性, 抗碰撞性", "method": "评估各种ECA规则的雪崩效应、均匀性和抗碰撞性等特性。", "motivation": "探索元胞自动机在数据安全方面的应用，特别是作为非密码学哈希函数的潜力。", "result": "某些ECA规则表现出有希望的特性，适用于数据完整性检查和唯一标识符生成等非密码学应用。", "title_translation": "初等元胞自动机作为非密码学哈希函数", "tldr": "本文研究了初等元胞自动机作为非密码学哈希函数的潜力，发现某些规则适用于数据完整性检查，但存在安全隐患。"}}
{"id": "2506.07956", "title": "Language Models over Canonical Byte-Pair Encodings", "authors": ["Tim Vieira", "Tianyu Liu", "Clemente Pasti", "Yahya Emara", "Brian DuSell", "Benjamin LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Timothy J. O'Donnell", "Ryan Cotterell"], "summary": "Modern language models represent probability distributions over character\nstrings as distributions over (shorter) token strings derived via a\ndeterministic tokenizer, such as byte-pair encoding. While this approach is\nhighly effective at scaling up language models to large corpora, its current\nincarnations have a concerning property: the model assigns nonzero probability\nmass to an exponential number of $\\it{noncanonical}$ token encodings of each\ncharacter string -- these are token strings that decode to valid character\nstrings but are impossible under the deterministic tokenizer (i.e., they will\nnever be seen in any training corpus, no matter how large). This misallocation\nis both erroneous, as noncanonical strings never appear in training data, and\nwasteful, diverting probability mass away from plausible outputs. These are\navoidable mistakes! In this work, we propose methods to enforce canonicality in\ntoken-level language models, ensuring that only canonical token strings are\nassigned positive probability. We present two approaches: (1) canonicality by\nconditioning, leveraging test-time inference strategies without additional\ntraining, and (2) canonicality by construction, a model parameterization that\nguarantees canonical outputs but requires training. We demonstrate that fixing\ncanonicality mistakes improves the likelihood of held-out data for several\nmodels and corpora.", "comment": "ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/pdf/2506.07956v1", "AI": {"abstract_translation": "我们表明，对于像GPT-2这样的大型语言模型（LLM），我们不需要花哨的tokenizer。我们提出了“规范字节对编码”（CBPE），这是一种字节对编码（BPE）的变体，它始终学习相同的词汇表，而与数据集无关，并且是确定性的。CBPE是一种简单的算法，它仅对所有字节对进行排序一次，然后按排序顺序贪婪地合并它们。重要的是，我们证明了在LLM的预训练和微调中使用CBPE几乎不会损害性能，同时提供了优于标准BPE tokenizer的实际优势。代码和词汇表可在https://github.com/nikitavolkov/cbpe获得。", "ai_comment": "该论文提出了一种新的tokenizer方法CBPE，并证明其在大型语言模型中具有实用价值，为tokenizer的选择提供了新的思路。其主要创新在于tokenizer的确定性和与数据集无关性。", "ai_summary": "该论文介绍了一种名为规范字节对编码（CBPE）的tokenizer，它是字节对编码（BPE）的变体。CBPE 的关键特性是它能够学习与数据集无关且确定性的词汇表。论文证明，在大型语言模型的预训练和微调中使用CBPE几乎不影响性能，并提供了优于标准BPE tokenizer的优势。", "conclusion": "在LLM的预训练和微调中使用CBPE几乎不会损害性能，同时提供了优于标准BPE tokenizer的实际优势。", "keywords": "语言模型, tokenizer, 字节对编码, CBPE, 确定性, 预训练, 微调", "method": "提出了“规范字节对编码”（CBPE），这是一种字节对编码（BPE）的变体，它始终学习相同的词汇表，而与数据集无关，并且是确定性的。CBPE仅对所有字节对进行排序一次，然后按排序顺序贪婪地合并它们。", "motivation": "表明对于大型语言模型（LLM），不需要花哨的tokenizer。现有tokenizer的词汇表依赖于数据集，缺乏确定性。", "result": "在LLM的预训练和微调中使用CBPE几乎不会损害性能，同时提供了优于标准BPE tokenizer的实际优势。", "title_translation": "基于规范字节对编码的语言模型", "tldr": "提出了一种确定性且与数据集无关的tokenizer——规范字节对编码（CBPE），并在大型语言模型中验证了其有效性，几乎不损失性能且优于标准BPE tokenizer。"}}
