{"id": "2506.06333", "title": "Extending AALpy with Passive Learning: A Generalized State-Merging Approach", "authors": ["Benjamin von Berg", "Bernhard K. Aichernig"], "summary": "AALpy is a well-established open-source automata learning library written in\nPython with a focus on active learning of systems with IO behavior. It provides\na wide range of state-of-the-art algorithms for different automaton types\nranging from fully deterministic to probabilistic automata. In this work, we\npresent the recent addition of a generalized implementation of an important\nmethod from the domain of passive automata learning: state-merging in the\nred-blue framework. Using a common internal representation for different\nautomaton types allows for a general and highly configurable implementation of\nthe red-blue framework. We describe how to define and execute state-merging\nalgorithms using AALpy, which reduces the implementation effort for\nstate-merging algorithms mainly to the definition of compatibility criteria and\nscoring. This aids the implementation of both existing and novel algorithms. In\nparticular, defining some existing state-merging algorithms from the literature\nwith AALpy only takes a few lines of code.", "comment": "Accepted for publication at CAV 2025, the 37th International\n  Conference on Computer Aided Verification", "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.06333v1", "AI": {"title_translation": "将AALpy扩展到被动学习：一种广义的状态合并方法", "tldr": "AALpy现在支持被动学习中的广义状态合并方法，大大简化了状态合并算法的实现。", "motivation": "AALpy作为一个专注于主动学习的自动机学习库，缺乏对被动自动机学习中重要状态合并方法的通用支持，限制了其在被动学习领域的应用潜力。", "method": "该工作通过在AALpy中引入红蓝框架下的广义状态合并实现，并利用不同自动机类型的通用内部表示，实现了高度可配置和通用的状态合并算法定义和执行。", "result": "该扩展显著降低了状态合并算法的实现工作量，主要集中在兼容性标准和评分的定义上，极大地简化了现有及新型算法的开发；文献中一些现有的状态合并算法只需少量代码即可在AALpy中实现。", "conclusion": "通过在AALpy中集成广义状态合并方法，该库现在能够更有效地支持被动自动机学习，并显著简化了相关算法的开发和应用。", "translation": "AALpy是一个成熟的开源自动机学习库，用Python编写，专注于具有IO行为的系统的主动学习。它提供了各种最先进的算法，适用于从完全确定性到概率自动机的不同自动机类型。在这项工作中，我们介绍了最近新增的一种被动自动机学习领域重要方法的广义实现：红蓝框架中的状态合并。使用不同自动机类型的通用内部表示，可以实现红蓝框架的通用且高度可配置的实现。我们描述了如何使用AALpy定义和执行状态合并算法，这将状态合并算法的实现工作量主要减少到兼容性标准和评分的定义。这有助于实现现有和新颖的算法。特别是，使用AALpy定义文献中一些现有的状态合并算法只需几行代码。", "summary": "这篇论文介绍了AALpy库的一项重要扩展，即集成了被动自动机学习中的广义状态合并方法。通过采用红蓝框架和通用的内部表示，AALpy现在能够以高度可配置的方式支持状态合并算法的定义和执行，从而显著降低了算法的实现复杂性，使其更容易开发和应用现有及新颖的状态合并算法。", "keywords": "自动机学习, 被动学习, 状态合并, AALpy, 红蓝框架", "comments": "这项工作通过将广义状态合并方法引入AALpy，极大地增强了该库在被动自动机学习领域的功能和实用性。其创新之处在于利用通用内部表示实现了红蓝框架的高度可配置性，这不仅简化了现有算法的实现，也为未来新算法的开发提供了强大的平台。这对于自动化学习领域的研究和实践具有重要意义。"}}
{"id": "2506.06551", "title": "Elementary Cellular Automata as Non-Cryptographic Hash Functions", "authors": ["Daniel McKinley"], "summary": "A subset of 10 of the 256 elementary cellular automata (ECA) are implemented\nas a hash function using an error minimization lossy compression algorithm\noperating on wrapped 4x4 neighborhood cells. All 256 rules are processed and 10\nrules in two subsets of 8 are found to have properties that include both error\nminimization and maximization, unique solutions, a lossy inverse, efficient\nretroactive hashing, and an application to edge detection. The algorithm\nparallels the nested powers-of-two structure of the Fast Fourier Transform and\nFast Walsh-Hadamard Transform, is implemented in Java, and is built to hash any\n2 byte RGB code bitmap.", "comment": null, "cate": "nlin.CG", "url": "http://arxiv.org/pdf/2506.06551v1", "AI": {"title_translation": "基本元胞自动机作为非加密哈希函数", "tldr": "该研究将10种基本元胞自动机（ECA）规则作为非加密哈希函数实现，利用误差最小化有损压缩算法，发现这些规则具有误差最小化和最大化、唯一解、有损逆、高效追溯哈希以及应用于边缘检测等特性。", "motivation": "该研究旨在探索基本元胞自动机（ECA）作为非加密哈希函数的潜力，并识别具有理想特性的特定规则。", "method": "该方法使用一种在4x4邻域单元上运行的误差最小化有损压缩算法，将256个基本元胞自动机规则中的10个子集作为哈希函数实现。该算法并行快速傅里叶变换和快速沃尔什-哈达玛变换的嵌套2的幂结构，并用Java实现，用于哈希任意2字节RGB代码位图。", "result": "在处理所有256个规则后，发现有10个规则（分为两个8个规则的子集）具有误差最小化和最大化、唯一解、有损逆、高效追溯哈希以及应用于边缘检测等特性。", "conclusion": "某些基本元胞自动机规则可以有效地用作具有特定理想特性和应用的非加密哈希函数。", "translation": "256个基本元胞自动机（ECA）中的10个子集被实现为哈希函数，使用在包裹的4x4邻域单元上运行的误差最小化有损压缩算法。处理了所有256个规则，发现两个8个规则的子集中的10个规则具有包括误差最小化和最大化、唯一解、有损逆、高效追溯哈希以及应用于边缘检测等特性。该算法与快速傅里叶变换和快速沃尔什-哈达玛变换的嵌套2的幂结构并行，用Java实现，并用于哈希任意2字节RGB代码位图。", "summary": "本研究将10个基本元胞自动机（ECA）规则子集实现为非加密哈希函数，采用在4x4单元上运行的误差最小化有损压缩算法。研究处理了所有256个ECA规则，并识别出10个（分属两个8个规则的子集）具有误差最小化/最大化、唯一解、有损逆、高效追溯哈希以及边缘检测应用等特性的规则。该算法用Java实现，其结构与快速傅里叶变换和快速沃尔什-哈达玛变换相似，旨在对2字节RGB位图代码进行哈希处理。", "keywords": "基本元胞自动机, 哈希函数, 误差最小化, 边缘检测, 有损压缩", "comments": "该论文的创新之处在于利用基本元胞自动机（ECA）作为非加密哈希函数，并探索其固有特性以实现此目的，同时将其与FFT等成熟变换进行类比。其重要性可能在于提供了替代的哈希方法或在边缘检测等特定应用中的潜力。"}}
{"id": "2506.07956", "title": "Language Models over Canonical Byte-Pair Encodings", "authors": ["Tim Vieira", "Tianyu Liu", "Clemente Pasti", "Yahya Emara", "Brian DuSell", "Benjamin LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Timothy J. O'Donnell", "Ryan Cotterell"], "summary": "Modern language models represent probability distributions over character\nstrings as distributions over (shorter) token strings derived via a\ndeterministic tokenizer, such as byte-pair encoding. While this approach is\nhighly effective at scaling up language models to large corpora, its current\nincarnations have a concerning property: the model assigns nonzero probability\nmass to an exponential number of $\\it{noncanonical}$ token encodings of each\ncharacter string -- these are token strings that decode to valid character\nstrings but are impossible under the deterministic tokenizer (i.e., they will\nnever be seen in any training corpus, no matter how large). This misallocation\nis both erroneous, as noncanonical strings never appear in training data, and\nwasteful, diverting probability mass away from plausible outputs. These are\navoidable mistakes! In this work, we propose methods to enforce canonicality in\ntoken-level language models, ensuring that only canonical token strings are\nassigned positive probability. We present two approaches: (1) canonicality by\nconditioning, leveraging test-time inference strategies without additional\ntraining, and (2) canonicality by construction, a model parameterization that\nguarantees canonical outputs but requires training. We demonstrate that fixing\ncanonicality mistakes improves the likelihood of held-out data for several\nmodels and corpora.", "comment": "ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/pdf/2506.07956v1", "AI": {"title_translation": "基于规范字节对编码的语言模型", "tldr": "语言模型会将非规范的字节对编码分配非零概率，这既是错误的也是浪费的。本文提出了两种方法来强制规范性，并证明这能提高模型在未见数据上的似然性。", "motivation": "现代语言模型通过确定性分词器（如字节对编码）将字符字符串的概率分布表示为分词字符串的分布。然而，当前的方法存在一个问题：模型会为每个字符字符串的指数级数量的“非规范”分词编码分配非零概率质量。这些编码虽然能解码成有效的字符字符串，但在确定性分词器下是不可能的（即无论训练语料库多大，都不会出现）。这种错误分配既是错误的（因为非规范字符串从不出现在训练数据中），也是浪费的（将概率质量从合理输出中转移开）。", "method": "本文提出了两种方法来强制令牌级语言模型中的规范性，确保只有规范的令牌字符串被赋予正概率：\n1.  **通过条件化实现规范性**：利用测试时推理策略，无需额外训练。\n2.  **通过构建实现规范性**：一种模型参数化方法，保证规范输出，但需要训练。", "result": "我们证明了纠正规范性错误可以提高多个模型和语料库在保留数据上的似然性。", "conclusion": "通过本文提出的方法，可以在令牌级语言模型中强制执行规范性，从而纠正概率分配错误，并有效提高模型在未见数据上的性能（似然性）。", "translation": "现代语言模型将字符字符串的概率分布表示为通过确定性分词器（例如字节对编码）派生出的（更短的）分词字符串的分布。虽然这种方法在将语言模型扩展到大型语料库方面非常有效，但其目前的实现具有一个令人担忧的特性：模型会为每个字符字符串的指数级数量的“非规范”分词编码分配非零概率质量——这些是能解码为有效字符字符串，但在确定性分词器下不可能出现的（即，无论训练语料库多大，它们都不会在任何训练语料库中出现）分词字符串。这种错误分配既是错误的，因为非规范字符串从不出现在训练数据中，也是浪费的，因为它将概率质量从合理输出中转移开。这些都是可避免的错误！在这项工作中，我们提出了在令牌级语言模型中强制规范性的方法，确保只有规范的令牌字符串被赋予正概率。我们提出了两种方法：(1) 通过条件化实现规范性，利用测试时推理策略而无需额外训练；(2) 通过构建实现规范性，这是一种保证规范输出但需要训练的模型参数化。我们证明，纠正规范性错误可以提高多个模型和语料库在保留数据上的似然性。", "summary": "现代语言模型使用确定性分词器（如BPE）时，会错误地为“非规范”分词编码分配非零概率，这既不合理也浪费资源。本文提出两种方法解决此问题：一是通过条件化在推理时强制规范性，无需额外训练；二是通过模型参数化在训练时构建规范性。实验证明，纠正这些规范性错误能显著提高模型在未见数据上的似然性。", "keywords": "语言模型, 字节对编码, 分词, 规范性, 概率分布", "comments": "这篇论文解决了令牌级语言模型中一个基础但常被忽视的问题，即令牌编码的有效性。通过强制执行规范性，它提高了模型的效率和准确性，避免了概率质量的错误分配。这是对语言模型鲁棒性和可解释性的重要贡献。"}}
