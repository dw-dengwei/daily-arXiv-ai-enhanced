{"id": "2506.06333", "title": "Extending AALpy with Passive Learning: A Generalized State-Merging Approach", "authors": ["Benjamin von Berg", "Bernhard K. Aichernig"], "summary": "AALpy is a well-established open-source automata learning library written in\nPython with a focus on active learning of systems with IO behavior. It provides\na wide range of state-of-the-art algorithms for different automaton types\nranging from fully deterministic to probabilistic automata. In this work, we\npresent the recent addition of a generalized implementation of an important\nmethod from the domain of passive automata learning: state-merging in the\nred-blue framework. Using a common internal representation for different\nautomaton types allows for a general and highly configurable implementation of\nthe red-blue framework. We describe how to define and execute state-merging\nalgorithms using AALpy, which reduces the implementation effort for\nstate-merging algorithms mainly to the definition of compatibility criteria and\nscoring. This aids the implementation of both existing and novel algorithms. In\nparticular, defining some existing state-merging algorithms from the literature\nwith AALpy only takes a few lines of code.", "comment": "Accepted for publication at CAV 2025, the 37th International\n  Conference on Computer Aided Verification", "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.06333v1", "AI": {"title_translation": "将 AALpy 扩展到被动学习：一种广义的状态合并方法", "tldr": "AALpy 库现在增加了被动学习的状态合并功能，通过广义的红蓝框架实现，大大简化了状态合并算法的开发。", "motivation": "AALpy 主要专注于主动学习，但缺少被动学习中的重要方法。引入被动学习中的状态合并方法可以扩展其功能，并简化相关算法的实现。", "method": "作者在 AALpy 中引入了基于红蓝框架的广义状态合并实现。通过使用通用的内部表示，使得该框架具有高度可配置性。该方法将状态合并算法的实现工作量主要简化为兼容性标准和评分的定义。", "result": "成功将广义的状态合并方法集成到 AALpy 中，显著降低了实现状态合并算法的工作量，现有算法只需少量代码即可定义，有助于开发现有和新颖的算法。", "conclusion": "在 AALpy 中引入广义状态合并功能，极大地增强了其在被动学习领域的能力，并简化了相关算法的开发和实现。", "translation": "AALpy 是一个成熟的开源自动机学习库，用 Python 编写，专注于具有 IO 行为的系统的主动学习。它提供了广泛的最新算法，适用于从完全确定性到概率自动机的不同自动机类型。在这项工作中，我们介绍了最近添加的被动自动机学习领域中一个重要方法的广义实现：红蓝框架中的状态合并。使用不同自动机类型的通用内部表示允许红蓝框架的通用且高度可配置的实现。我们描述了如何使用 AALpy 定义和执行状态合并算法，这主要将状态合并算法的实现工作量减少到兼容性标准和评分的定义。这有助于现有和新颖算法的实现。特别是，使用 AALpy 定义文献中一些现有的状态合并算法只需要几行代码。", "summary": "本文介绍了在 AALpy 开源自动机学习库中新增的被动学习功能，具体是基于红蓝框架的广义状态合并方法。通过采用通用的内部表示，该实现具有高度可配置性，并显著简化了状态合并算法的开发，将其主要工作量降低到定义兼容性标准和评分。这使得现有和新的状态合并算法能够以极少的代码量实现。", "keywords": "自动机学习, 被动学习, 状态合并, AALpy, 红蓝框架", "comments": "这项工作通过将被动学习中的核心状态合并方法集成到 AALpy 中，显著扩展了该库的功能。其创新之处在于提供了一个广义且高度可配置的红蓝框架实现，极大地降低了开发和实验不同状态合并算法的门槛，对于自动化学习领域的研究和应用具有重要意义。"}}
{"id": "2506.06551", "title": "Elementary Cellular Automata as Non-Cryptographic Hash Functions", "authors": ["Daniel McKinley"], "summary": "A subset of 10 of the 256 elementary cellular automata (ECA) are implemented\nas a hash function using an error minimization lossy compression algorithm\noperating on wrapped 4x4 neighborhood cells. All 256 rules are processed and 10\nrules in two subsets of 8 are found to have properties that include both error\nminimization and maximization, unique solutions, a lossy inverse, efficient\nretroactive hashing, and an application to edge detection. The algorithm\nparallels the nested powers-of-two structure of the Fast Fourier Transform and\nFast Walsh-Hadamard Transform, is implemented in Java, and is built to hash any\n2 byte RGB code bitmap.", "comment": null, "cate": "nlin.CG", "url": "http://arxiv.org/pdf/2506.06551v1", "AI": {"title_translation": "基本元胞自动机作为非加密哈希函数", "tldr": "10种基本元胞自动机被实现为一种使用误差最小化有损压缩算法的哈希函数，并具有独特属性，可应用于边缘检测。", "motivation": "Not mentioned in abstract", "method": "将256个基本元胞自动机（ECA）中的10个子集实现为哈希函数。使用误差最小化有损压缩算法，作用于环绕的4x4邻域单元。该算法与快速傅里叶变换和快速沃尔什-哈达马变换的嵌套二的幂结构并行。用Java实现，旨在对任何2字节RGB代码位图进行哈希处理。", "result": "发现10条规则（分为两个8个的子集）具有以下特性：误差最小化和最大化、独特解决方案、有损逆、高效追溯哈希以及应用于边缘检测的潜力。", "conclusion": "Not mentioned in abstract", "translation": "256个基本元胞自动机（ECA）中的10个子集被实现为哈希函数，使用在环绕的4x4邻域单元上运行的误差最小化有损压缩算法。所有256条规则都经过处理，发现其中10条规则（分为两个8个的子集）具有包括误差最小化和最大化、独特解决方案、有损逆、高效追溯哈希以及应用于边缘检测等特性。该算法与快速傅里叶变换和快速沃尔什-哈达马变换的嵌套二的幂结构并行，用Java实现，旨在对任何2字节RGB代码位图进行哈希处理。", "summary": "本文将256个基本元胞自动机（ECA）中的10个子集实现为一种非加密哈希函数，该函数采用误差最小化有损压缩算法，作用于环绕的4x4邻域单元。研究发现，这10条规则具有误差最小化和最大化、独特解决方案、有损逆、高效追溯哈希以及应用于边缘检测等特性。该算法在结构上与快速傅里叶变换和快速沃尔什-哈达马变换相似，使用Java实现，可用于哈希2字节RGB位图。", "keywords": "基本元胞自动机, 哈希函数, 误差最小化, 有损压缩, 边缘检测", "comments": "本文创新性地将基本元胞自动机应用于非加密哈希函数，并结合误差最小化有损压缩算法，发现其具有独特的属性，如误差最小化/最大化和有损逆，以及应用于边缘检测的潜力。其算法结构与快速傅里叶变换等并行，这可能为哈希函数设计提供新的思路。然而，摘要中未提及该方法的性能基准或与现有哈希函数的比较，且其主要应用于2字节RGB位图，表明其可能是一个特定领域的应用。"}}
{"id": "2506.07956", "title": "Language Models over Canonical Byte-Pair Encodings", "authors": ["Tim Vieira", "Tianyu Liu", "Clemente Pasti", "Yahya Emara", "Brian DuSell", "Benjamin LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Timothy J. O'Donnell", "Ryan Cotterell"], "summary": "Modern language models represent probability distributions over character\nstrings as distributions over (shorter) token strings derived via a\ndeterministic tokenizer, such as byte-pair encoding. While this approach is\nhighly effective at scaling up language models to large corpora, its current\nincarnations have a concerning property: the model assigns nonzero probability\nmass to an exponential number of $\\it{noncanonical}$ token encodings of each\ncharacter string -- these are token strings that decode to valid character\nstrings but are impossible under the deterministic tokenizer (i.e., they will\nnever be seen in any training corpus, no matter how large). This misallocation\nis both erroneous, as noncanonical strings never appear in training data, and\nwasteful, diverting probability mass away from plausible outputs. These are\navoidable mistakes! In this work, we propose methods to enforce canonicality in\ntoken-level language models, ensuring that only canonical token strings are\nassigned positive probability. We present two approaches: (1) canonicality by\nconditioning, leveraging test-time inference strategies without additional\ntraining, and (2) canonicality by construction, a model parameterization that\nguarantees canonical outputs but requires training. We demonstrate that fixing\ncanonicality mistakes improves the likelihood of held-out data for several\nmodels and corpora.", "comment": "ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/pdf/2506.07956v1", "AI": {"title_translation": "规范字节对编码上的语言模型", "tldr": "现代语言模型中的字节对编码存在非规范性问题，本文提出两种方法解决此问题，提高模型性能。", "motivation": "现代语言模型在字节对编码中会将非规范的token编码分配非零概率，这既是错误的（因为非规范字符串从不出现在训练数据中），也是浪费的（它将概率质量从合理的输出中转移开）。", "method": "提出了两种强制规范性的方法：1) 通过条件作用实现规范性（利用测试时推理策略，无需额外训练）；2) 通过构造实现规范性（一种模型参数化，保证规范输出，但需要训练）。", "result": "纠正规范性错误可以提高多个模型和语料库在保留数据上的似然性。", "conclusion": "解决语言模型中非规范性token编码问题是可行的，并且能够提升模型性能。", "translation": "现代语言模型将字符字符串上的概率分布表示为通过确定性分词器（例如字节对编码）导出的（较短的）token字符串上的分布。虽然这种方法在将语言模型扩展到大型语料库方面非常有效，但其目前的实现方式具有一个令人担忧的特性：模型为每个字符字符串的指数数量的非规范token编码分配非零概率质量——这些token字符串解码为有效的字符字符串，但在确定性分词器下是不可能的（即，无论训练语料库多大，它们都不会出现在任何训练语料库中）。这种错误分配既是错误的，因为非规范字符串从不出现在训练数据中，也是浪费的，它将概率质量从合理的输出中转移开。这些都是可以避免的错误！在这项工作中，我们提出了在token级别语言模型中强制规范性的方法，确保只有规范的token字符串被赋予正概率。我们提出了两种方法：（1）通过条件作用实现规范性，利用测试时推理策略而无需额外训练，以及（2）通过构造实现规范性，这是一种模型参数化，保证规范输出但需要训练。我们证明，纠正规范性错误可以提高多个模型和语料库在保留数据上的似然性。", "summary": "本文解决了现代语言模型中字节对编码（BPE）存在的非规范性问题，即模型会错误地为训练数据中不存在的非规范token编码分配概率，导致概率质量浪费。作者提出了两种解决策略：一种是通过条件作用在测试时强制规范性（无需额外训练），另一种是通过模型参数化构造规范性（需要训练）。实验证明，这些方法能有效提升模型在保留数据上的似然性。", "keywords": "语言模型, 字节对编码, 非规范性, token化, 概率分配", "comments": "这篇论文识别并解决了一个现代语言模型中普遍存在但常被忽视的问题，即字节对编码的非规范性。通过提出两种不同的解决方案（无需训练和需要训练），该研究为提升语言模型的效率和准确性提供了新的视角和实用方法。其创新性在于深入理解了BPE的工作机制，并针对其固有限制提出了系统性的改进方案。"}}
