{"id": "2506.06333", "title": "Extending AALpy with Passive Learning: A Generalized State-Merging Approach", "authors": ["Benjamin von Berg", "Bernhard K. Aichernig"], "summary": "AALpy is a well-established open-source automata learning library written in\nPython with a focus on active learning of systems with IO behavior. It provides\na wide range of state-of-the-art algorithms for different automaton types\nranging from fully deterministic to probabilistic automata. In this work, we\npresent the recent addition of a generalized implementation of an important\nmethod from the domain of passive automata learning: state-merging in the\nred-blue framework. Using a common internal representation for different\nautomaton types allows for a general and highly configurable implementation of\nthe red-blue framework. We describe how to define and execute state-merging\nalgorithms using AALpy, which reduces the implementation effort for\nstate-merging algorithms mainly to the definition of compatibility criteria and\nscoring. This aids the implementation of both existing and novel algorithms. In\nparticular, defining some existing state-merging algorithms from the literature\nwith AALpy only takes a few lines of code.", "comment": "Accepted for publication at CAV 2025, the 37th International\n  Conference on Computer Aided Verification", "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.06333v1", "AI": {"title_translation": "使用被动学习扩展AALpy：一种广义状态合并方法", "tldr": "AALpy is extended with a generalized state-merging implementation for passive learning.", "motivation": "To extend AALpy with passive learning capabilities, specifically state-merging in the red-blue framework.", "method": "generalized implementation of state-merging in the red-blue framework using a common internal representation for different automaton types", "result": "A general and highly configurable implementation of the red-blue framework within AALpy, reducing the implementation effort for state-merging algorithms.", "conclusion": "Defining existing state-merging algorithms from the literature with AALpy only takes a few lines of code.", "translation": "本文介绍了一种在AALpy库中实现的广义状态合并方法，通过使用红蓝框架扩展了其被动自动机学习的能力。该实现利用了不同自动机类型的通用内部表示，从而实现了一种高度可配置且高效的状态合并方法。使用AALpy定义现有算法只需要很少的代码。", "summary": "This paper introduces a generalized state-merging implementation within the AALpy library, extending its capabilities to passive automata learning using the red-blue framework. The implementation leverages a common internal representation for different automaton types, enabling a highly configurable and efficient approach to state-merging. Defining existing algorithms with AALpy requires minimal code.", "keywords": "automata learning, AALpy, state-merging, passive learning, red-blue framework", "comments": "Not mentioned in abstract"}}
{"id": "2506.06551", "title": "Elementary Cellular Automata as Non-Cryptographic Hash Functions", "authors": ["Daniel McKinley"], "summary": "A subset of 10 of the 256 elementary cellular automata (ECA) are implemented\nas a hash function using an error minimization lossy compression algorithm\noperating on wrapped 4x4 neighborhood cells. All 256 rules are processed and 10\nrules in two subsets of 8 are found to have properties that include both error\nminimization and maximization, unique solutions, a lossy inverse, efficient\nretroactive hashing, and an application to edge detection. The algorithm\nparallels the nested powers-of-two structure of the Fast Fourier Transform and\nFast Walsh-Hadamard Transform, is implemented in Java, and is built to hash any\n2 byte RGB code bitmap.", "comment": null, "cate": "nlin.CG", "url": "http://arxiv.org/pdf/2506.06551v1", "AI": {"title_translation": "作为非密码散列函数的初等细胞自动机", "tldr": "Elementary cellular automata are implemented as a hash function using error minimization.", "motivation": "To implement a subset of elementary cellular automata (ECA) as a hash function.", "method": "An error minimization lossy compression algorithm operating on wrapped 4x4 neighborhood cells.", "result": "10 rules in two subsets of 8 are found to have properties that include both error minimization and maximization, unique solutions, a lossy inverse, efficient retroactive hashing, and an application to edge detection.", "conclusion": "The algorithm parallels the nested powers-of-two structure of the Fast Fourier Transform and Fast Walsh-Hadamard Transform, is implemented in Java, and is built to hash any 2 byte RGB code bitmap.", "translation": "本文将256个基本细胞自动机（ECA）中的一个子集中的10个实现为哈希函数，该哈希函数使用在包裹的4x4邻域单元上运行的误差最小化有损压缩算法。处理所有256条规则，发现两个8个子集中的10条规则具有包括误差最小化和最大化、唯一解、有损逆、高效追溯哈希以及边缘检测应用等属性。该算法与快速傅里叶变换和快速沃尔什-哈达玛变换的嵌套二的幂结构相似，用Java实现，并且构建为散列任何2字节RGB代码位图。", "summary": "This paper implements a subset of elementary cellular automata (ECA) as a hash function using an error minimization lossy compression algorithm on wrapped 4x4 neighborhood cells. The study processed all 256 rules and found 10 rules with properties including error minimization/maximization, unique solutions, a lossy inverse, efficient retroactive hashing, and edge detection application. The algorithm, implemented in Java, parallels Fast Fourier/Walsh-Hadamard Transforms and hashes 2-byte RGB bitmaps.", "keywords": "cellular automata, hash function, error minimization, lossy compression, edge detection", "comments": "Not mentioned in abstract"}}
{"id": "2506.07956", "title": "Language Models over Canonical Byte-Pair Encodings", "authors": ["Tim Vieira", "Tianyu Liu", "Clemente Pasti", "Yahya Emara", "Brian DuSell", "Benjamin LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Timothy J. O'Donnell", "Ryan Cotterell"], "summary": "Modern language models represent probability distributions over character\nstrings as distributions over (shorter) token strings derived via a\ndeterministic tokenizer, such as byte-pair encoding. While this approach is\nhighly effective at scaling up language models to large corpora, its current\nincarnations have a concerning property: the model assigns nonzero probability\nmass to an exponential number of $\\it{noncanonical}$ token encodings of each\ncharacter string -- these are token strings that decode to valid character\nstrings but are impossible under the deterministic tokenizer (i.e., they will\nnever be seen in any training corpus, no matter how large). This misallocation\nis both erroneous, as noncanonical strings never appear in training data, and\nwasteful, diverting probability mass away from plausible outputs. These are\navoidable mistakes! In this work, we propose methods to enforce canonicality in\ntoken-level language models, ensuring that only canonical token strings are\nassigned positive probability. We present two approaches: (1) canonicality by\nconditioning, leveraging test-time inference strategies without additional\ntraining, and (2) canonicality by construction, a model parameterization that\nguarantees canonical outputs but requires training. We demonstrate that fixing\ncanonicality mistakes improves the likelihood of held-out data for several\nmodels and corpora.", "comment": "ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/pdf/2506.07956v1", "AI": {"title_translation": "规范字节对编码上的语言模型", "tldr": "Language models waste probability on impossible token sequences. This paper introduces methods to fix this, improving performance.", "motivation": "Modern language models assign nonzero probability mass to an exponential number of noncanonical token encodings of each character string. This misallocation is both erroneous and wasteful.", "method": "The paper presents two approaches: (1) canonicality by conditioning, leveraging test-time inference strategies without additional training, and (2) canonicality by construction, a model parameterization that guarantees canonical outputs but requires training.", "result": "Fixing canonicality mistakes improves the likelihood of held-out data for several models and corpora.", "conclusion": "Fixing canonicality mistakes improves the likelihood of held-out data for several models and corpora.", "translation": "现代语言模型将字符概率分布表示为通过确定性分词器（如字节对编码）导出的（较短）token字符串的分布。虽然这种方法在将语言模型扩展到大型语料库方面非常有效，但其目前的实现方式存在一个令人担忧的特性：模型为每个字符的指数数量的非规范token编码分配非零概率质量——这些token字符串解码为有效的字符字符串，但在确定性分词器下是不可能的（即，无论训练语料库有多大，它们都不会出现在任何训练语料库中）。这种错误分配既是错误的（因为非规范字符串永远不会出现在训练数据中），也是浪费的，将概率质量从合理的输出中转移出去。这些都是可以避免的错误！在这项工作中，我们提出了在token级别语言模型中强制执行规范性的方法，确保只有规范的token字符串被分配正概率。我们提出了两种方法：（1）通过条件反射实现规范性，利用测试时推理策略，无需额外训练；（2）通过构造实现规范性，这是一种保证规范输出但需要训练的模型参数化。我们证明了修复规范性错误可以提高几个模型和语料库的保留数据的可能性。", "summary": "This paper addresses the issue of language models assigning probability to noncanonical token encodings when using byte-pair encoding. It proposes two methods to enforce canonicality: canonicality by conditioning (test-time inference) and canonicality by construction (model parameterization with training). Results show improved likelihood on held-out data.", "keywords": "language models, byte-pair encoding, canonicality, tokenization", "comments": "This paper introduces methods to address the issue of noncanonical token encodings in language models using byte-pair encoding. The proposed solutions, canonicality by conditioning and canonicality by construction, offer different trade-offs between implementation complexity and the need for retraining. The empirical results demonstrate improved likelihood on held-out data, suggesting the effectiveness of the proposed approaches."}}
