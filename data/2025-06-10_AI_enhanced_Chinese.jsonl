{"id": "2506.06333", "title": "Extending AALpy with Passive Learning: A Generalized State-Merging Approach", "authors": ["Benjamin von Berg", "Bernhard K. Aichernig"], "summary": "AALpy is a well-established open-source automata learning library written in\nPython with a focus on active learning of systems with IO behavior. It provides\na wide range of state-of-the-art algorithms for different automaton types\nranging from fully deterministic to probabilistic automata. In this work, we\npresent the recent addition of a generalized implementation of an important\nmethod from the domain of passive automata learning: state-merging in the\nred-blue framework. Using a common internal representation for different\nautomaton types allows for a general and highly configurable implementation of\nthe red-blue framework. We describe how to define and execute state-merging\nalgorithms using AALpy, which reduces the implementation effort for\nstate-merging algorithms mainly to the definition of compatibility criteria and\nscoring. This aids the implementation of both existing and novel algorithms. In\nparticular, defining some existing state-merging algorithms from the literature\nwith AALpy only takes a few lines of code.", "comment": "Accepted for publication at CAV 2025, the 37th International\n  Conference on Computer Aided Verification", "cate": "cs.LG", "url": "http://arxiv.org/pdf/2506.06333v1", "AI": {"title_translation": "使用被动学习扩展 AALpy：一种广义状态合并方法", "tldr": "AALpy is extended with a generalized state-merging implementation for passive learning, simplifying the implementation of state-merging algorithms.", "motivation": "To extend AALpy with passive learning capabilities by adding a generalized state-merging implementation.", "method": "generalized implementation of state-merging in the red-blue framework using a common internal representation for different automaton types.", "result": "A general and highly configurable implementation of the red-blue framework within AALpy, reducing the implementation effort for state-merging algorithms.", "conclusion": "Defining some existing state-merging algorithms from the literature with AALpy only takes a few lines of code.", "translation": "AALpy是一个完善的开源自动机学习库，用Python编写，专注于具有IO行为的系统的主动学习。它为不同类型的自动机提供了广泛的最新算法，范围从完全确定性自动机到概率自动机。在这项工作中，我们介绍了最近添加的一种来自被动自动机学习领域的重要方法的通用实现：红蓝框架中的状态合并。使用不同自动机类型的通用内部表示，可以实现红蓝框架的通用且高度可配置的实现。我们描述了如何使用AALpy定义和执行状态合并算法，这主要将状态合并算法的实现工作减少到兼容性标准和评分的定义。这有助于现有算法和新算法的实现。特别是，使用AALpy定义文献中的一些现有状态合并算法只需几行代码。", "summary": "This paper introduces a generalized state-merging implementation in AALpy, an open-source automata learning library. It uses a common internal representation for various automaton types, enabling a highly configurable red-blue framework. This reduces the effort required to implement state-merging algorithms, simplifying the definition of compatibility criteria and scoring. Defining existing algorithms with AALpy requires minimal code.", "keywords": "automata learning, AALpy, state-merging, passive learning, red-blue framework", "comments": "Not mentioned in abstract"}}
{"id": "2506.06551", "title": "Elementary Cellular Automata as Non-Cryptographic Hash Functions", "authors": ["Daniel McKinley"], "summary": "A subset of 10 of the 256 elementary cellular automata (ECA) are implemented\nas a hash function using an error minimization lossy compression algorithm\noperating on wrapped 4x4 neighborhood cells. All 256 rules are processed and 10\nrules in two subsets of 8 are found to have properties that include both error\nminimization and maximization, unique solutions, a lossy inverse, efficient\nretroactive hashing, and an application to edge detection. The algorithm\nparallels the nested powers-of-two structure of the Fast Fourier Transform and\nFast Walsh-Hadamard Transform, is implemented in Java, and is built to hash any\n2 byte RGB code bitmap.", "comment": null, "cate": "nlin.CG", "url": "http://arxiv.org/pdf/2506.06551v1", "AI": {"title_translation": "基本细胞自动机作为非密码散列函数", "tldr": "Elementary cellular automata are explored as non-cryptographic hash functions, identifying 10 rules with desirable properties.", "motivation": "To explore the potential of elementary cellular automata (ECA) as non-cryptographic hash functions.", "method": "Implementation of 10 elementary cellular automata (ECA) as a hash function using an error minimization lossy compression algorithm operating on wrapped 4x4 neighborhood cells. Processing all 256 rules to find suitable subsets.", "result": "Identified 10 ECA rules with error minimization and maximization properties, unique solutions, a lossy inverse, efficient retroactive hashing, and an application to edge detection.", "conclusion": "The paper identifies 10 ECA rules with properties suitable for non-cryptographic hashing, including error minimization/maximization, unique solutions, lossy inverse, efficient retroactive hashing, and edge detection application. The algorithm, inspired by FFT and FWT, is implemented in Java and hashes 2-byte RGB bitmaps.", "translation": "本文探讨了将基本细胞自动机（ECA）用作非密码散列函数的可能性。通过使用在包裹的4x4邻域单元上运行的误差最小化有损压缩算法，实现了256个规则中的10个规则子集作为散列函数。发现这两个8个子集中的10个规则具有误差最小化和最大化、唯一解、有损逆、高效追溯散列以及边缘检测应用等特性。该算法与快速傅里叶变换和快速沃尔什-哈达玛变换的嵌套二次幂结构相似，用Java实现，旨在散列任何2字节RGB代码位图。", "summary": "This paper explores the use of elementary cellular automata (ECA) as non-cryptographic hash functions. A subset of 10 rules out of 256 are implemented using an error minimization lossy compression algorithm on wrapped 4x4 neighborhood cells. These rules exhibit properties like error minimization/maximization, unique solutions, lossy inverse, efficient retroactive hashing, and edge detection application. The algorithm, inspired by FFT and FWT, is implemented in Java and designed for 2-byte RGB code bitmaps.", "keywords": "cellular automata, hash function, error minimization, lossy compression, edge detection", "comments": "Not mentioned in abstract"}}
{"id": "2506.07956", "title": "Language Models over Canonical Byte-Pair Encodings", "authors": ["Tim Vieira", "Tianyu Liu", "Clemente Pasti", "Yahya Emara", "Brian DuSell", "Benjamin LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Timothy J. O'Donnell", "Ryan Cotterell"], "summary": "Modern language models represent probability distributions over character\nstrings as distributions over (shorter) token strings derived via a\ndeterministic tokenizer, such as byte-pair encoding. While this approach is\nhighly effective at scaling up language models to large corpora, its current\nincarnations have a concerning property: the model assigns nonzero probability\nmass to an exponential number of $\\it{noncanonical}$ token encodings of each\ncharacter string -- these are token strings that decode to valid character\nstrings but are impossible under the deterministic tokenizer (i.e., they will\nnever be seen in any training corpus, no matter how large). This misallocation\nis both erroneous, as noncanonical strings never appear in training data, and\nwasteful, diverting probability mass away from plausible outputs. These are\navoidable mistakes! In this work, we propose methods to enforce canonicality in\ntoken-level language models, ensuring that only canonical token strings are\nassigned positive probability. We present two approaches: (1) canonicality by\nconditioning, leveraging test-time inference strategies without additional\ntraining, and (2) canonicality by construction, a model parameterization that\nguarantees canonical outputs but requires training. We demonstrate that fixing\ncanonicality mistakes improves the likelihood of held-out data for several\nmodels and corpora.", "comment": "ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/pdf/2506.07956v1", "AI": {"title_translation": "基于规范字节对编码的语言模型", "tldr": "Language models waste probability on impossible token sequences. This paper introduces methods to fix this, improving model accuracy.", "motivation": "Modern language models assign nonzero probability mass to noncanonical token encodings, which is erroneous and wastes probability mass. This paper aims to correct this misallocation.", "method": "The paper proposes two methods: (1) canonicality by conditioning, using test-time inference strategies without additional training, and (2) canonicality by construction, a model parameterization that guarantees canonical outputs but requires training.", "result": "Fixing canonicality mistakes improves the likelihood of held-out data for several models and corpora.", "conclusion": "The paper demonstrates that correcting canonicality mistakes enhances the likelihood of held-out data across various models and corpora.", "translation": "现代语言模型将字符概率分布表示为通过确定性分词器（如字节对编码）导出的（较短）token字符串的分布。虽然这种方法在将语言模型扩展到大型语料库方面非常有效，但其目前的实现方式存在一个令人担忧的属性：该模型为每个字符的指数数量的非规范token编码分配非零概率质量——这些token字符串解码为有效的字符字符串，但在确定性分词器下是不可能的（即，无论训练语料库有多大，它们都不会出现在任何训练语料库中）。这种错误分配既是错误的，因为非规范字符串永远不会出现在训练数据中，也是浪费的，将概率质量从合理的输出中转移出去。这些是可避免的错误！在这项工作中，我们提出了在token级别语言模型中强制执行规范性的方法，确保只有规范的token字符串被分配正概率。我们提出了两种方法：（1）通过条件反射实现规范性，利用测试时推理策略而无需额外的训练，以及（2）通过构建实现规范性，这是一种保证规范输出但需要训练的模型参数化。我们证明了修复规范性错误可以提高几种模型和语料库的保留数据的可能性。", "summary": "This paper addresses the issue of language models assigning probability to noncanonical token encodings in byte-pair encoding. It proposes two methods to enforce canonicality: canonicality by conditioning and canonicality by construction. The results show improved likelihood of held-out data.", "keywords": "language models, byte-pair encoding, canonicality, tokenization, probability distribution", "comments": "This paper introduces methods to address the issue of noncanonical token encodings in language models using byte-pair encoding, which is a novel approach to improve the efficiency and accuracy of these models."}}
