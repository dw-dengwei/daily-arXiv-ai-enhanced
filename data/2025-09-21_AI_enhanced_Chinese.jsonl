{"id": "2509.14920", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.14920", "abs": "https://arxiv.org/abs/2509.14920", "authors": ["Amine Barrak", "Fabio Petrillo", "Fehmi Jaafar"], "title": "Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures", "comment": null, "summary": "The field of distributed machine learning (ML) faces increasing demands for\nscalable and cost-effective training solutions, particularly in the context of\nlarge, complex models. Serverless computing has emerged as a promising paradigm\nto address these challenges by offering dynamic scalability and\nresource-efficient execution. Building upon our previous work, which introduced\nthe Serverless Peer Integrated for Robust Training (SPIRT) architecture, this\npaper presents a comparative analysis of several serverless distributed ML\narchitectures. We examine SPIRT alongside established architectures like\nScatterReduce, AllReduce, and MLLess, focusing on key metrics such as training\ntime efficiency, cost-effectiveness, communication overhead, and fault\ntolerance capabilities. Our findings reveal that SPIRT provides significant\nimprovements in reducing training times and communication overhead through\nstrategies such as parallel batch processing and in-database operations\nfacilitated by RedisAI. However, traditional architectures exhibit scalability\nchallenges and varying degrees of vulnerability to faults and adversarial\nattacks. The cost analysis underscores the long-term economic benefits of SPIRT\ndespite its higher initial setup costs. This study not only highlights the\nstrengths and limitations of current serverless ML architectures but also sets\nthe stage for future research aimed at developing new models that combine the\nmost effective features of existing systems.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86SPIRT\u4e0eScatterReduce\u3001AllReduce\u3001MLLess\u7b49\u65e0\u670d\u52a1\u5668\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff0c\u53d1\u73b0SPIRT\u5728\u8bad\u7ec3\u6548\u7387\u3001\u901a\u4fe1\u5f00\u9500\u548c\u957f\u671f\u6210\u672c\u6548\u76ca\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5e94\u5bf9\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u5bf9\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u8bad\u7ec3\u65b9\u6848\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790SPIRT\u4e0e\u4f20\u7edf\u67b6\u6784\u5728\u8bad\u7ec3\u65f6\u95f4\u3001\u901a\u4fe1\u5f00\u9500\u3001\u5bb9\u9519\u6027\u53ca\u6210\u672c\u7b49\u5173\u952e\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "result": "SPIRT\u901a\u8fc7\u5e76\u884c\u6279\u5904\u7406\u4e0eRedisAI\u6570\u636e\u5e93\u5185\u64cd\u4f5c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u957f\u671f\u6210\u672c\u6548\u76ca\u66f4\u4f18\uff0c\u4f46\u521d\u59cb\u90e8\u7f72\u6210\u672c\u8f83\u9ad8\u3002", "conclusion": "SPIRT\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u67b6\u6784\uff0c\u4e3a\u672a\u6765\u878d\u5408\u5404\u67b6\u6784\u4f18\u52bf\u7684\u65b0\u6a21\u578b\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.14276", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14276", "abs": "https://arxiv.org/abs/2509.14276", "authors": ["Yuxiang Mai", "Qiyue Yin", "Wancheng Ni", "Pei Xu", "Kaiqi Huang"], "title": "Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity", "comment": "Accepted by IJCAI 2025", "summary": "In recent years, diversity has emerged as a useful mechanism to enhance the\nefficiency of multi-agent reinforcement learning (MARL). However, existing\nmethods predominantly focus on designing policies based on individual agent\ncharacteristics, often neglecting the interplay and mutual influence among\nagents during policy formation. To address this gap, we propose Competitive\nDiversity through Constructive Conflict (CoDiCon), a novel approach that\nincorporates competitive incentives into cooperative scenarios to encourage\npolicy exchange and foster strategic diversity among agents. Drawing\ninspiration from sociological research, which highlights the benefits of\nmoderate competition and constructive conflict in group decision-making, we\ndesign an intrinsic reward mechanism using ranking features to introduce\ncompetitive motivations. A centralized intrinsic reward module generates and\ndistributes varying reward values to agents, ensuring an effective balance\nbetween competition and cooperation. By optimizing the parameterized\ncentralized reward module to maximize environmental rewards, we reformulate the\nconstrained bilevel optimization problem to align with the original task\nobjectives. We evaluate our algorithm against state-of-the-art methods in the\nSMAC and GRF environments. Experimental results demonstrate that CoDiCon\nachieves superior performance, with competitive intrinsic rewards effectively\npromoting diverse and adaptive strategies among cooperative agents.", "AI": {"tldr": "CoDiCon\u901a\u8fc7\u5f15\u5165\u7ade\u4e89\u6fc0\u52b1\u4fc3\u8fdb\u5408\u4f5c\u667a\u80fd\u4f53\u95f4\u7684\u7b56\u7565\u591a\u6837\u6027\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u667a\u80fd\u4f53\u95f4\u4e92\u52a8\u5f71\u54cd\uff0c\u7f3a\u4e4f\u7b56\u7565\u591a\u6837\u6027\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u6392\u540d\u7279\u5f81\u7684\u5185\u5728\u5956\u52b1\u673a\u5236\uff0c\u5e73\u8861\u7ade\u4e89\u4e0e\u5408\u4f5c\uff0c\u4f18\u5316\u53cc\u5c42\u7ea6\u675f\u95ee\u9898\u3002", "result": "\u5728SMAC\u548cGRF\u73af\u5883\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7b56\u7565\u66f4\u5bcc\u591a\u6837\u6027\u4e0e\u9002\u5e94\u6027\u3002", "conclusion": "\u9002\u5ea6\u7ade\u4e89\u53ef\u6709\u6548\u63d0\u5347\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7b56\u7565\u8868\u73b0\u3002"}}
{"id": "2509.15182", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.15182", "abs": "https://arxiv.org/abs/2509.15182", "authors": ["Muhammad Ahmed Mohsin", "Ahsan Bilal", "Muhammad Umer", "Asad Aali", "Muhammad Ali Jamshed", "Dean F. Hougen", "John M. Cioffi"], "title": "Conditional Prior-based Non-stationary Channel Estimation Using Accelerated Diffusion Models", "comment": "ICASSP 2026", "summary": "Wireless channels in motion-rich urban microcell (UMi) settings are\nnon-stationary; mobility and scatterer dynamics shift the distribution over\ntime, degrading classical and deep estimators. This work proposes conditional\nprior diffusion for channel estimation, which learns a history-conditioned\nscore to denoise noisy channel snapshots. A temporal encoder with cross-time\nattention compresses a short observation window into a context vector, which\ncaptures the channel's instantaneous coherence and steers the denoiser via\nfeature-wise modulation. In inference, an SNR-matched initialization selects\nthe diffusion step whose marginal aligns with the measured input SNR, and the\nprocess follows a shortened, geometrically spaced schedule, preserving the\nsignal-to-noise trajectory with far fewer iterations. Temporal\nself-conditioning with the previous channel estimate and a training-only\nsmoothness penalty further stabilizes evolution without biasing the test-time\nestimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than\nLMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and\nstrong high SNR fidelity.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u5148\u9a8c\u6269\u6563\u7684\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u52a8\u6001\u57ce\u5e02\u5fae\u8702\u7a9d\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002", "motivation": "\u57ce\u5e02\u5fae\u8702\u7a9d\u73af\u5883\u4e2d\u65e0\u7ebf\u4fe1\u9053\u56e0\u79fb\u52a8\u6027\u548c\u6563\u5c04\u4f53\u52a8\u6001\u5448\u73b0\u975e\u5e73\u7a33\u6027\uff0c\u5bfc\u81f4\u4f20\u7edf\u4f30\u8ba1\u5668\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u5148\u9a8c\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u65f6\u5e8f\u7f16\u7801\u5668\u4e0e\u8de8\u65f6\u6ce8\u610f\u529b\u673a\u5236\u538b\u7f29\u89c2\u6d4b\u7a97\u53e3\uff0c\u901a\u8fc7\u7279\u5f81\u8c03\u5236\u5f15\u5bfc\u53bb\u566a\uff0c\u5e76\u5229\u7528SNR\u5339\u914d\u521d\u59cb\u5316\u4e0e\u51e0\u4f55\u6b65\u957f\u8c03\u5ea6\u52a0\u901f\u63a8\u7406\u3002", "result": "\u57283GPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u6709\u4fe1\u566a\u6bd4\u4e0bNMSE\u5747\u4f18\u4e8eLMMSE\u3001GMM\u3001LSTM\u548cLDAMP\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9ad8\u4fe1\u566a\u6bd4\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u975e\u5e73\u7a33\u4fe1\u9053\u4e2d\u5177\u5907\u7a33\u5b9a\u6027\u80fd\u4e0e\u9ad8\u4fdd\u771f\u5ea6\uff0c\u9002\u5408\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u4fe1\u9053\u4f30\u8ba1\u3002"}}
{"id": "2509.14680", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14680", "abs": "https://arxiv.org/abs/2509.14680", "authors": ["Tianyang Duan", "Zongyuan Zhang", "Songxiao Guo", "Dong Huang", "Yuanye Zhao", "Zheng Lin", "Zihan Fang", "Dianxin Luan", "Heming Cui", "Yong Cui"], "title": "LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning", "comment": "5 pages, 4 figures", "summary": "Multi-agent reinforcement learning (MARL) holds substantial promise for\nintelligent decision-making in complex environments. However, it suffers from a\ncoordination and scalability bottleneck as the number of agents increases. To\naddress these issues, we propose the LLM-empowered expert demonstrations\nframework for multi-agent reinforcement learning (LEED). LEED consists of two\ncomponents: a demonstration generation (DG) module and a policy optimization\n(PO) module. Specifically, the DG module leverages large language models to\ngenerate instructions for interacting with the environment, thereby producing\nhigh-quality demonstrations. The PO module adopts a decentralized training\nparadigm, where each agent utilizes the generated demonstrations to construct\nan expert policy loss, which is then integrated with its own policy loss. This\nenables each agent to effectively personalize and optimize its local policy\nbased on both expert knowledge and individual experience. Experimental results\nshow that LEED achieves superior sample efficiency, time efficiency, and robust\nscalability compared to state-of-the-art baselines.", "AI": {"tldr": "LEED\u6846\u67b6\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e13\u5bb6\u793a\u8303\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u3001\u65f6\u95f4\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\u65f6\u7684\u534f\u8c03\u4e0e\u53ef\u6269\u5c55\u6027\u74f6\u9888\u95ee\u9898\u3002", "method": "\u63d0\u51faLEED\u6846\u67b6\uff0c\u5305\u542b\u793a\u8303\u751f\u6210\u6a21\u5757\uff08\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u73af\u5883\u4ea4\u4e92\u6307\u4ee4\uff09\u548c\u7b56\u7565\u4f18\u5316\u6a21\u5757\uff08\u7ed3\u5408\u4e13\u5bb6\u7b56\u7565\u635f\u5931\u4e0e\u4e2a\u4f53\u7b56\u7565\u635f\u5931\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLEED\u5728\u6837\u672c\u6548\u7387\u3001\u65f6\u95f4\u6548\u7387\u548c\u9c81\u68d2\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "LEED\u6709\u6548\u878d\u5408\u4e13\u5bb6\u77e5\u8bc6\u4e0e\u4e2a\u4f53\u7ecf\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2509.15192", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.15192", "abs": "https://arxiv.org/abs/2509.15192", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Muhammad Ibtsaam Qadir", "Muhammad Ali Jamshed", "Dean F. Hougen", "John M. Cioffi"], "title": "Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization", "comment": "ICASSP 2026", "summary": "Modern wireless networks face critical challenges when mobile users traverse\nheterogeneous network configurations with varying antenna layouts, carrier\nfrequencies, and scattering statistics. Traditional predictors degrade under\ndistribution shift, with NMSE rising by 37.5\\% during cross-configuration\nhandovers. This work addresses catastrophic forgetting in channel prediction by\nproposing a continual learning framework based on loss regularization. The\napproach augments standard training objectives with penalty terms that\nselectively preserve network parameters essential for previous configurations\nwhile enabling adaptation to new environments. Two prominent regularization\nstrategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic\nIntelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers\nthe high-SNR NMSE floor by up to 1.8 dB ($\\approx$32--34\\%), while EWC achieves\nup to 1.4 dB ($\\approx$17--28\\%). Notably, standard EWC incurs\n$\\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and\ncorresponding parameter snapshots across $K$ tasks) unless consolidated,\nwhereas SI maintains $\\mathcal{O}(M)$ memory complexity (storing $M$ model\nparameters), independent of task sequence length, making it suitable for\nresource-constrained wireless infrastructure", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u635f\u5931\u6b63\u5219\u5316\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u65e0\u7ebf\u4fe1\u9053\u9884\u6d4b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5176\u4e2dSI\u65b9\u6cd5\u5728\u9ad8\u4fe1\u566a\u6bd4\u4e0b\u663e\u8457\u964d\u4f4eNMSE\u4e14\u5185\u5b58\u6548\u7387\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u5668\u5728\u8de8\u914d\u7f6e\u5207\u6362\u65f6\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5f39\u6027\u6743\u91cd\u56fa\u5316\uff08EWC\uff09\u548c\u7a81\u89e6\u667a\u80fd\uff08SI\uff09\u4e24\u79cd\u6b63\u5219\u5316\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u76ee\u6807\u4e2d\u52a0\u5165\u60e9\u7f5a\u9879\uff0c\u9009\u62e9\u6027\u4fdd\u7559\u65e7\u914d\u7f6e\u5173\u952e\u53c2\u6570\u3002", "result": "\u57283GPP\u573a\u666f\u4e2d\uff0cSI\u4f7f\u9ad8\u4fe1\u566a\u6bd4NMSE\u964d\u4f4e\u6700\u591a1.8dB\uff08\u7ea632-34%\uff09\uff0cEWC\u964d\u4f4e\u6700\u591a1.4dB\uff08\u7ea617-28%\uff09\uff1bSI\u5185\u5b58\u590d\u6742\u5ea6\u4e3aO(M)\uff0c\u4f18\u4e8eEWC\u7684O(MK)\u3002", "conclusion": "SI\u5728\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\u4e0a\u5747\u4f18\u4e8eEWC\uff0c\u66f4\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u7ebf\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\u3002"}}
{"id": "2509.15103", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15103", "abs": "https://arxiv.org/abs/2509.15103", "authors": ["Simin Li", "Zheng Yuwei", "Zihao Mao", "Linhao Wang", "Ruixiao Xu", "Chengdong Ma", "Xin Yu", "Yuqing Ma", "Qi Dou", "Xin Wang", "Jie Luo", "Bo An", "Yaodong Yang", "Weifeng Lv", "Xianglong Liu"], "title": "Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning", "comment": "submitted to NIPS 2025", "summary": "Partial agent failure becomes inevitable when systems scale up, making it\ncrucial to identify the subset of agents whose compromise would most severely\ndegrade overall performance. In this paper, we study this Vulnerable Agent\nIdentification (VAI) problem in large-scale multi-agent reinforcement learning\n(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field\nControl (HAD-MFC), where the upper level involves an NP-hard combinatorial task\nof selecting the most vulnerable agents, and the lower level learns worst-case\nadversarial policies for these agents using mean-field MARL. The two problems\nare coupled together, making HAD-MFC difficult to solve. To solve this, we\nfirst decouple the hierarchical process by Fenchel-Rockafellar transform,\nresulting a regularized mean-field Bellman operator for upper level that\nenables independent learning at each level, thus reducing computational\ncomplexity. We then reformulate the upper-level combinatorial problem as a MDP\nwith dense rewards from our regularized mean-field Bellman operator, enabling\nus to sequentially identify the most vulnerable agents by greedy and RL\nalgorithms. This decomposition provably preserves the optimal solution of the\noriginal HAD-MFC. Experiments show our method effectively identifies more\nvulnerable agents in large-scale MARL and the rule-based system, fooling system\ninto worse failures, and learns a value function that reveals the vulnerability\nof each agent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHAD-MFC\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u5206\u5c42\u5bf9\u6297\u63a7\u5236\uff0c\u9ad8\u6548\u8bc6\u522b\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6700\u8106\u5f31\u7684\u667a\u80fd\u4f53\u3002", "motivation": "\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u90e8\u5206\u667a\u80fd\u4f53\u5931\u6548\u4e0d\u53ef\u907f\u514d\uff0c\u9700\u5b9a\u4f4d\u6700\u6613\u5bfc\u81f4\u7cfb\u7edf\u6027\u80fd\u5d29\u6e83\u7684\u5173\u952e\u667a\u80fd\u4f53\u3002", "method": "\u6784\u5efa\u5206\u5c42\u5bf9\u6297\u5747\u503c\u573a\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528Fenchel-Rockafellar\u53d8\u6362\u89e3\u8026\u4e0a\u4e0b\u5c42\u95ee\u9898\uff0c\u5c06\u7ec4\u5408\u4f18\u5316\u8f6c\u4e3a\u5e26\u5bc6\u96c6\u5956\u52b1\u7684MDP\uff0c\u7ed3\u5408\u8d2a\u5fc3\u4e0e\u5f3a\u5316\u5b66\u4e60\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u8106\u5f31\u667a\u80fd\u4f53\uff0c\u8bf1\u5bfc\u7cfb\u7edf\u66f4\u4e25\u91cd\u5931\u6548\uff0c\u5e76\u5b66\u4e60\u53cd\u6620\u5404\u667a\u80fd\u4f53\u8106\u5f31\u6027\u7684\u4ef7\u503c\u51fd\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u539f\u95ee\u9898\u6700\u4f18\u89e3\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2509.14292", "categories": ["cs.OS", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.14292", "abs": "https://arxiv.org/abs/2509.14292", "authors": ["Ben Holmes", "Baltasar Dinis", "Lana Honcharuk", "Joshua Fried", "Adam Belay"], "title": "Taming Serverless Cold Starts Through OS Co-Design", "comment": null, "summary": "Serverless computing promises fine-grained elasticity and operational\nsimplicity, fueling widespread interest from both industry and academia. Yet\nthis promise is undercut by the cold setart problem, where invoking a function\nafter a period of inactivity triggers costly initialization before any work can\nbegin. Even with today's high-speed storage, the prevailing view is that\nachieving sub-millisecond cold starts requires keeping state resident in\nmemory.\n  This paper challenges that assumption. Our analysis of existing\nsnapshot/restore mechanisms show that OS-level limitations, not storage speed,\nare the real barrier to ultra-fast restores from disk. These limitations force\ncurrent systems to either restore state piecemeal in a costly manner or capture\ntoo much state, leading to longer restore times and unpredictable performance.\nFuthermore, current memory primitives exposed by the OS make it difficult to\nreliably fetch data into memory and avoid costly runtime page faults.\n  To overcome these barriers, we present Spice, an execution engine\npurpose-built for serverless snapshot/restore. Spice integrates directly with\nthe OS to restore kernel state without costly replay and introduces dedicated\nprimitives for restoring memory mappings efficiently and reliably. As a result,\nSpice delivers near-warm performance on cold restores from disk, reducing\nlatency by up to 14.9x over state-of-the-art process-based systems and 10.6x\nover VM-based systems. This proves that high performance and memory elasticity\nno longer need to be a trade-off in serverless computing.", "AI": {"tldr": "Spice\u901a\u8fc7\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u6df1\u5ea6\u96c6\u6210\uff0c\u5b9e\u73b0\u4ece\u78c1\u76d8\u5feb\u901f\u6062\u590d\u65e0\u670d\u52a1\u5668\u51fd\u6570\u72b6\u6001\uff0c\u663e\u8457\u964d\u4f4e\u51b7\u542f\u52a8\u5ef6\u8fdf\uff0c\u6253\u7834\u6027\u80fd\u4e0e\u5185\u5b58\u5f39\u6027\u7684\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u4e2d\u56e0\u51b7\u542f\u52a8\u5bfc\u81f4\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u7a81\u7834\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u5185\u5b58\u9a7b\u7559\u72b6\u6001\u7684\u9650\u5236\u3002", "method": "\u8bbe\u8ba1Spice\u6267\u884c\u5f15\u64ce\uff0c\u76f4\u63a5\u4e0eOS\u96c6\u6210\u4ee5\u9ad8\u6548\u6062\u590d\u5185\u6838\u72b6\u6001\uff0c\u5e76\u5f15\u5165\u4e13\u7528\u5185\u5b58\u6620\u5c04\u539f\u8bed\u907f\u514d\u8fd0\u884c\u65f6\u9875\u9519\u8bef\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8fdb\u7a0b\u548c\u865a\u62df\u673a\u7cfb\u7edf\uff0c\u51b7\u542f\u52a8\u5ef6\u8fdf\u5206\u522b\u964d\u4f4e14.9\u500d\u548c10.6\u500d\uff0c\u5b9e\u73b0\u63a5\u8fd1\u70ed\u542f\u52a8\u7684\u6027\u80fd\u3002", "conclusion": "\u8bc1\u660e\u5728\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u4e2d\uff0c\u9ad8\u6027\u80fd\u4e0e\u5185\u5b58\u5f39\u6027\u53ef\u517c\u5f97\uff0c\u65e0\u9700\u518d\u505a\u53d6\u820d\u3002"}}
