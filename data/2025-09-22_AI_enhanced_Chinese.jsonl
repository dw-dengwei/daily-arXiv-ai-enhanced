{"id": "2509.15238", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15238", "abs": "https://arxiv.org/abs/2509.15238", "authors": ["Dylan L\u00e9veill\u00e9"], "title": "Generating Plans for Belief-Desire-Intention (BDI) Agents Using Alternating-Time Temporal Logic (ATL)", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "Belief-Desire-Intention (BDI) is a framework for modelling agents based on\ntheir beliefs, desires, and intentions. Plans are a central component of BDI\nagents, and define sequences of actions that an agent must undertake to achieve\na certain goal. Existing approaches to plan generation often require\nsignificant manual effort, and are mainly focused on single-agent systems. As a\nresult, in this work, we have developed a tool that automatically generates BDI\nplans using Alternating-Time Temporal Logic (ATL). By using ATL, the plans\ngenerated accommodate for possible competition or cooperation between the\nagents in the system. We demonstrate the effectiveness of the tool by\ngenerating plans for an illustrative game that requires agent collaboration to\nachieve a shared goal. We show that the generated plans allow the agents to\nsuccessfully attain this goal.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ea4\u66ff\u65f6\u5e8f\u903b\u8f91(ATL)\u7684\u5de5\u5177\uff0c\u81ea\u52a8\u751f\u6210BDI\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u8ba1\u5212\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u7ade\u4e89\u4e0e\u5408\u4f5c\u3002", "motivation": "\u73b0\u6709BDI\u8ba1\u5212\u751f\u6210\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5de5\u4f5c\u4e14\u4e3b\u8981\u9488\u5bf9\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u591a\u667a\u80fd\u4f53\u95f4\u7684\u534f\u4f5c\u4e0e\u7ade\u4e89\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u4ea4\u66ff\u65f6\u5e8f\u903b\u8f91(ATL)\u6765\u81ea\u52a8\u751f\u6210BDI\u8ba1\u5212\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u8003\u8651\u7cfb\u7edf\u4e2d\u667a\u80fd\u4f53\u4e4b\u95f4\u53ef\u80fd\u7684\u7ade\u4e89\u6216\u5408\u4f5c\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u9700\u8981\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u6e38\u620f\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5de5\u5177\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u7684\u8ba1\u5212\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u6210\u529f\u5b9e\u73b0\u5171\u4eab\u76ee\u6807\u3002", "conclusion": "\u63d0\u51fa\u7684ATL-based\u5de5\u5177\u80fd\u591f\u81ea\u52a8\u751f\u6210\u652f\u6301\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684BDI\u8ba1\u5212\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.15381", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2509.15381", "abs": "https://arxiv.org/abs/2509.15381", "authors": ["Tiannan Zhang", "Rishi Veerapaneni", "Shao-Hung Chan", "Jiaoyang Li", "Maxim Likhachev"], "title": "Dynamic Agent Grouping ECBS: Scaling Windowed Multi-Agent Path Finding with Completeness Guarantees", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) is the problem of finding a set of\ncollision-free paths for a team of agents. Although several MAPF methods which\nsolve full-horizon MAPF have completeness guarantees, very few MAPF methods\nthat plan partial paths have completeness guarantees. Recent work introduced\nthe Windowed Complete MAPF (WinC-MAPF) framework, which shows how windowed\noptimal MAPF solvers (e.g., SS-CBS) can use heuristic updates and disjoint\nagent groups to maintain completeness even when planning partial paths\n(Veerapaneni et al. 2024). A core limitation of WinC-MAPF is that they required\noptimal MAPF solvers. Our main contribution is to extend WinC-MAPF by showing\nhow we can use a bounded suboptimal solver while maintaining completeness. In\nparticular, we design Dynamic Agent Grouping ECBS (DAG-ECBS) which dynamically\ncreates and plans agent groups while maintaining that each agent group solution\nis bounded suboptimal. We prove how DAG-ECBS can maintain completeness in the\nWinC-MAPF framework. DAG-ECBS shows improved scalability compared to SS-CBS and\ncan outperform windowed ECBS without completeness guarantees. More broadly, our\nwork serves as a blueprint for designing more MAPF methods that can use the\nWinC-MAPF framework.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86WinC-MAPF\u6846\u67b6\uff0c\u63d0\u51fa\u4e86DAG-ECBS\u65b9\u6cd5\uff0c\u4f7f\u7528\u6709\u754c\u6b21\u4f18\u6c42\u89e3\u5668\u5728\u89c4\u5212\u90e8\u5206\u8def\u5f84\u65f6\u4fdd\u6301\u5b8c\u6574\u6027\u4fdd\u8bc1\uff0c\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7684WinC-MAPF\u6846\u67b6\u9700\u8981\u6700\u4f18MAPF\u6c42\u89e3\u5668\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\u672c\u6587\u65e8\u5728\u6269\u5c55\u8be5\u6846\u67b6\uff0c\u4f7f\u5176\u80fd\u591f\u4f7f\u7528\u6709\u754c\u6b21\u4f18\u6c42\u89e3\u5668\u540c\u65f6\u4fdd\u6301\u5b8c\u6574\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u52a8\u6001\u4ee3\u7406\u5206\u7ec4ECBS\uff08DAG-ECBS\uff09\u65b9\u6cd5\uff0c\u52a8\u6001\u521b\u5efa\u548c\u89c4\u5212\u4ee3\u7406\u7ec4\uff0c\u786e\u4fdd\u6bcf\u4e2a\u4ee3\u7406\u7ec4\u89e3\u51b3\u65b9\u6848\u90fd\u662f\u6709\u754c\u6b21\u4f18\u7684\u3002", "result": "DAG-ECBS\u5728WinC-MAPF\u6846\u67b6\u4e2d\u4fdd\u6301\u4e86\u5b8c\u6574\u6027\uff0c\u76f8\u6bd4SS-CBS\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u4f18\u4e8e\u6ca1\u6709\u5b8c\u6574\u6027\u4fdd\u8bc1\u7684\u7a97\u53e3\u5316ECBS\u3002", "conclusion": "\u672c\u6587\u4e3a\u8bbe\u8ba1\u66f4\u591a\u80fd\u591f\u4f7f\u7528WinC-MAPF\u6846\u67b6\u7684MAPF\u65b9\u6cd5\u63d0\u4f9b\u4e86\u84dd\u56fe\uff0c\u6269\u5c55\u4e86\u8be5\u6846\u67b6\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2509.15450", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.15450", "abs": "https://arxiv.org/abs/2509.15450", "authors": ["Abhishek Vijaya Kumar", "Arjun Devraj", "Rachee Singh"], "title": "PCCL: Photonic circuit-switched collective communication for distributed ML", "comment": null, "summary": "Modern distributed ML suffers from a fundamental gap between the theoretical\nand realized performance of collective communication algorithms due to\ncongestion and hop-count induced dilation in practical GPU clusters. We present\nPCCL, a Photonic Collective Communication Library that reconfigures the network\ntopology to match the communication patterns of collective algorithms, thereby\neliminating congestion and dilation by creating direct, contention-free\ncircuits between communicating GPUs. Unlike prior approaches that synthesize\nalgorithms for specific network topologies and collectives, PCCL generalizes to\nany collective primitive and any topology by adapting the network to match each\nalgorithm's communication pattern. PCCL's key innovation lies in its\nhardware-agnostic optimization framework that intelligently decides when to\nreconfigure based on the trade-off between network reconfiguration delay and\ncongestion/dilation costs, making it practical across different optical\nhardware with varying switching speeds. Our evaluation demonstrates that PCCL\nachieves up to 3X speedup over state-of-the-art algorithms on 128 GPUs across\nvarious workloads, buffer sizes, and topologies, translating to a 1.3X speedup\nin end-to-end training throughput.", "AI": {"tldr": "PCCL\u662f\u4e00\u4e2a\u5149\u5b50\u96c6\u4f53\u901a\u4fe1\u5e93\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u6784\u7f51\u7edc\u62d3\u6251\u6765\u5339\u914d\u96c6\u4f53\u901a\u4fe1\u7b97\u6cd5\u7684\u901a\u4fe1\u6a21\u5f0f\uff0c\u6d88\u9664\u62e5\u585e\u548c\u5ef6\u8fdf\uff0c\u5728128\u4e2aGPU\u4e0a\u5b9e\u73b03\u500d\u52a0\u901f\u548c1.3\u500d\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u5b58\u5728\u7406\u8bba\u6027\u80fd\u4e0e\u5b9e\u9645\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3b\u8981\u7531\u4e8eGPU\u96c6\u7fa4\u4e2d\u7684\u62e5\u585e\u548c\u8df3\u6570\u5f15\u8d77\u7684\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "PCCL\u91c7\u7528\u786c\u4ef6\u65e0\u5173\u7684\u4f18\u5316\u6846\u67b6\uff0c\u667a\u80fd\u51b3\u5b9a\u4f55\u65f6\u91cd\u65b0\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u7f51\u7edc\u91cd\u6784\u5ef6\u8fdf\u4e0e\u62e5\u585e/\u5ef6\u8fdf\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u4e3a\u4efb\u4f55\u96c6\u4f53\u539f\u8bed\u548c\u62d3\u6251\u521b\u5efa\u76f4\u63a5\u3001\u65e0\u4e89\u7528\u7684\u7535\u8def\u3002", "result": "\u5728128\u4e2aGPU\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cPCCL\u5728\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u3001\u7f13\u51b2\u533a\u5927\u5c0f\u548c\u62d3\u6251\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe3\u500d\u7684\u52a0\u901f\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u53471.3\u500d\u3002", "conclusion": "PCCL\u901a\u8fc7\u52a8\u6001\u7f51\u7edc\u91cd\u6784\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0fML\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u4e0d\u540c\u5149\u5b66\u786c\u4ef6\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96c6\u4f53\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2509.15847", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15847", "abs": "https://arxiv.org/abs/2509.15847", "authors": ["Qianyu Yu", "Giuliano Losa", "Nibesh Shrestha", "Xuechao Wang"], "title": "Angelfish: Consensus with Optimal Throughput and Latency Across the Leader-DAG Spectrum", "comment": null, "summary": "To maximize performance, many modern blockchain systems rely on\neventually-synchronous, Byzantine fault-tolerant (BFT) consensus protocols. Two\nprotocol designs have emerged in this space: protocols that minimize latency\nusing a leader that drives both data dissemination and consensus, and protocols\nthat maximize throughput using a separate, asynchronous data dissemination\nlayer. Recent protocols such as Partially-Synchronous Bullshark and Sailfish\ncombine elements of both approaches by using a DAG to enable parallel data\ndissemination and a leader that paces DAG formation. This improves latency\nwhile achieving state-of-the-art throughput. Yet the latency of leader-based\nprotocols is still better under moderate loads.\n  We present Angelfish, a hybrid protocol that adapts smoothly across this\ndesign space, from leader-based to Sailfish-like DAG-based consensus. Angelfish\nlets a dynamically-adjusted subset of parties use best-effort broadcast to\nissue lightweight votes instead of reliably broadcasting costlier DAG vertices.\nThis reduces communication, helps lagging nodes catch up, and lowers latency in\npractice compared to prior DAG-based protocols. Our empirical evaluation shows\nthat Angelfish attains state-of-the-art peak throughput while matching the\nlatency of leader-based protocols under moderate throughput, delivering the\nbest of both worlds.", "AI": {"tldr": "Angelfish\u662f\u4e00\u79cd\u6df7\u5408\u5171\u8bc6\u534f\u8bae\uff0c\u7ed3\u5408\u4e86\u9886\u5bfc\u8005\u9a71\u52a8\u548cDAG\u9a71\u52a8\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf", "motivation": "\u73b0\u6709\u533a\u5757\u94fe\u5171\u8bc6\u534f\u8bae\u5728\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u9886\u5bfc\u8005\u9a71\u52a8\u534f\u8bae\u5ef6\u8fdf\u4f4e\u4f46\u541e\u5410\u91cf\u6709\u9650\uff0cDAG\u9a71\u52a8\u534f\u8bae\u541e\u5410\u91cf\u9ad8\u4f46\u5ef6\u8fdf\u8f83\u5dee\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u517c\u987e\u4e24\u8005\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u52a8\u6001\u8c03\u6574\u7684\u8282\u70b9\u5b50\u96c6\u8fdb\u884c\u8f7b\u91cf\u7ea7\u6295\u7968\uff0c\u66ff\u4ee3\u6602\u8d35\u7684DAG\u9876\u70b9\u5e7f\u64ad\u3002\u7ed3\u5408\u6700\u4f73\u52aa\u529b\u5e7f\u64ad\u548c\u53ef\u9760\u5e7f\u64ad\uff0c\u8ba9\u843d\u540e\u8282\u70b9\u5feb\u901f\u8ffd\u8d76", "result": "Angelfish\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u5cf0\u503c\u541e\u5410\u91cf\uff0c\u540c\u65f6\u5728\u4e2d\u7b49\u8d1f\u8f7d\u4e0b\u5339\u914d\u4e86\u9886\u5bfc\u8005\u9a71\u52a8\u534f\u8bae\u7684\u5ef6\u8fdf\u6027\u80fd", "conclusion": "\u8be5\u534f\u8bae\u6210\u529f\u5b9e\u73b0\u4e86\u9886\u5bfc\u8005\u9a71\u52a8\u548cDAG\u9a71\u52a8\u534f\u8bae\u7684\u4f18\u52bf\u7ed3\u5408\uff0c\u4e3a\u533a\u5757\u94fe\u5171\u8bc6\u63d0\u4f9b\u4e86\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4ff1\u4f73\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.15940", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.15940", "abs": "https://arxiv.org/abs/2509.15940", "authors": ["Guoliang He", "Youhe Jiang", "Wencong Xiao", "Kaihua Jiang", "Shuguang Wang", "Jun Wang", "Zixian Du", "Zhuo Jiang", "Xinlei Zhang", "Binhang Yuan", "Eiko Yoneki"], "title": "Efficient Pre-Training of LLMs via Topology-Aware Communication Alignment on More Than 9600 GPUs", "comment": "NeurIPS 2025", "summary": "The scaling law for large language models (LLMs) depicts that the path\ntowards machine intelligence necessitates training at large scale. Thus,\ncompanies continuously build large-scale GPU clusters, and launch training jobs\nthat span over thousands of computing nodes. However, LLM pre-training presents\nunique challenges due to its complex communication patterns, where GPUs\nexchange data in sparse yet high-volume bursts within specific groups.\nInefficient resource scheduling exacerbates bandwidth contention, leading to\nsuboptimal training performance. This paper presents Arnold, a scheduling\nsystem summarizing our experience to effectively align LLM communication\npatterns with data center topology at scale. An in-depth characteristic study\nis performed to identify the impact of physical network topology to LLM\npre-training jobs. Based on the insights, we develop a scheduling algorithm to\neffectively align communication patterns with the physical network topology in\nmodern data centers. Through simulation experiments, we show the effectiveness\nof our algorithm in reducing the maximum spread of communication groups by up\nto $1.67$x. In production training, our scheduling system improves the\nend-to-end performance by $10.6\\%$ when training with more than $9600$ GPUs, a\nsignificant improvement for our training pipeline.", "AI": {"tldr": "Arnold\u8c03\u5ea6\u7cfb\u7edf\u901a\u8fc7\u5c06LLM\u9884\u8bad\u7ec3\u4f5c\u4e1a\u7684\u901a\u4fe1\u6a21\u5f0f\u4e0e\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u62d3\u6251\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21GPU\u96c6\u7fa4\u7684\u8bad\u7ec3\u6027\u80fd", "motivation": "LLM\u9884\u8bad\u7ec3\u5728\u5927\u89c4\u6a21GPU\u96c6\u7fa4\u4e0a\u5b58\u5728\u590d\u6742\u7684\u901a\u4fe1\u6a21\u5f0f\uff0c\u7a00\u758f\u4f46\u9ad8\u5bb9\u91cf\u7684\u7a81\u53d1\u6570\u636e\u4f20\u8f93\u5bfc\u81f4\u5e26\u5bbd\u7ade\u4e89\uff0c\u4f20\u7edf\u8d44\u6e90\u8c03\u5ea6\u6548\u7387\u4f4e\u4e0b", "method": "\u8fdb\u884c\u6df1\u5165\u7684\u7279\u6027\u7814\u7a76\u5206\u6790\u7269\u7406\u7f51\u7edc\u62d3\u6251\u5bf9LLM\u9884\u8bad\u7ec3\u4f5c\u4e1a\u7684\u5f71\u54cd\uff0c\u5f00\u53d1\u8c03\u5ea6\u7b97\u6cd5\u4f7f\u901a\u4fe1\u6a21\u5f0f\u4e0e\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u7269\u7406\u7f51\u7edc\u62d3\u6251\u6709\u6548\u5bf9\u9f50", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u901a\u4fe1\u7ec4\u6700\u5927\u4f20\u64ad\u8303\u56f4\u51cf\u5c111.67\u500d\uff0c\u751f\u4ea7\u73af\u5883\u4e2d9600+GPU\u8bad\u7ec3\u7aef\u5230\u7aef\u6027\u80fd\u63d0\u534710.6%", "conclusion": "Arnold\u8c03\u5ea6\u7cfb\u7edf\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u7684\u8c03\u5ea6\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898"}}
