<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CV](#cs.CV) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Hello, won't you tell me your name?: Investigating Anonymity Abuse in IPFS](https://arxiv.org/abs/2506.04307)
*Christos Karapapas,Iakovos Pittaras,George C. Polyzos,Constantinos Patsakis*

Main category: cs.CR

TL;DR: None


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: What are the main conclusions of the paper?

Abstract: The InterPlanetary File System~(IPFS) offers a decentralized approach to file
storage and sharing, promising resilience and efficiency while also realizing
the Web3 paradigm. Simultaneously, the offered anonymity raises significant
questions about potential misuse. In this study, we explore methods that
malicious actors can exploit IPFS to upload and disseminate harmful content
while remaining anonymous. We evaluate the role of pinning services and public
gateways, identifying their capabilities and limitations in maintaining content
availability. Using scripts, we systematically test the behavior of these
services by uploading malicious files. Our analysis reveals that pinning
services and public gateways lack mechanisms to assess or restrict the
propagation of malicious content.

</details>


### [2] [The Hashed Fractal Key Recovery (HFKR) Problem: From Symbolic Path Inversion to Post-Quantum Cryptographic Keys](https://arxiv.org/abs/2506.04383)
*Mohamed Aly Bouke*

Main category: cs.CR

TL;DR: A one-sentence summary of the paper.


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: Classical cryptographic systems rely heavily on structured algebraic
problems, such as factorization, discrete logarithms, or lattice-based
assumptions, which are increasingly vulnerable to quantum attacks and
structural cryptanalysis. In response, this work introduces the Hashed Fractal
Key Recovery (HFKR) problem, a non-algebraic cryptographic construction
grounded in symbolic dynamics and chaotic perturbations. HFKR builds on the
Symbolic Path Inversion Problem (SPIP), leveraging symbolic trajectories
generated via contractive affine maps over $\mathbb{Z}^2$, and compressing them
into fixed-length cryptographic keys using hash-based obfuscation. A key
contribution of this paper is the empirical confirmation that these symbolic
paths exhibit fractal behavior, quantified via box counting dimension, path
geometry, and spatial density measures. The observed fractal dimension
increases with trajectory length and stabilizes near 1.06, indicating symbolic
self-similarity and space-filling complexity, both of which reinforce the
entropy foundation of the scheme. Experimental results across 250 perturbation
trials show that SHA3-512 and SHAKE256 amplify symbolic divergence effectively,
achieving mean Hamming distances near 255, ideal bit-flip rates, and negligible
entropy deviation. In contrast, BLAKE3 exhibits statistically uniform but
weaker diffusion. These findings confirm that HFKR post-quantum security arises
from the synergy between symbolic fractality and hash-based entropy
amplification. The resulting construction offers a lightweight, structure-free
foundation for secure key generation in adversarial settings without relying on
algebraic hardness assumptions.

</details>


### [3] [Through the Stealth Lens: Rethinking Attacks and Defenses in RAG](https://arxiv.org/abs/2506.04390)
*Sarthak Choudhary,Nils Palumbo,Ashish Hooda,Krishnamurthy Dj Dvijotham,Somesh Jha*

Main category: cs.CR

TL;DR: None


<details>
  <summary>Details</summary>
Motivation: The abstract discusses the vulnerability of retrieval-augmented generation (RAG) systems to attacks that inject poisoned passages. The need for stealthy attacks arises from the ability to detect and mitigate existing attacks. The motivation is to address the challenge of attackers needing to rigorously analyze intermediate signals to avoid manipulation traces.

Method: The paper leverages attention patterns and proposes a passage-level score called NPAS for identifying potentially poisoned passages. The Attention-Variance Filter algorithm is used to filter these passages. It also analyzes the limitations of attention-based defenses and crafts stealthier adaptive attacks.

Result: The paper proposes the Normalized Passage Attention Score (NPAS) and the Attention-Variance Filter algorithm to identify and filter potentially poisoned passages. It shows that the method improves accuracy by up to 20% over baseline defenses. Additionally, it explores the limitations of attention-based defenses and demonstrates adaptive attacks with up to 35% success rate, highlighting the challenges in improving stealth.

Conclusion: The paper concludes that while existing attacks are detectable, the need for stealthy attacks emphasizes the importance of analyzing intermediate signals and developing effective filtering methods. The limitations of attention-based defenses are highlighted, indicating the challenges in improving stealth.

Abstract: Retrieval-augmented generation (RAG) systems are vulnerable to attacks that
inject poisoned passages into the retrieved set, even at low corruption rates.
We show that existing attacks are not designed to be stealthy, allowing
reliable detection and mitigation. We formalize stealth using a
distinguishability-based security game. If a few poisoned passages are designed
to control the response, they must differentiate themselves from benign ones,
inherently compromising stealth. This motivates the need for attackers to
rigorously analyze intermediate signals involved in
generation$\unicode{x2014}$such as attention patterns or next-token probability
distributions$\unicode{x2014}$to avoid easily detectable traces of
manipulation. Leveraging attention patterns, we propose a passage-level
score$\unicode{x2014}$the Normalized Passage Attention
Score$\unicode{x2014}$used by our Attention-Variance Filter algorithm to
identify and filter potentially poisoned passages. This method mitigates
existing attacks, improving accuracy by up to $\sim 20 \%$ over baseline
defenses. To probe the limits of attention-based defenses, we craft stealthier
adaptive attacks that obscure such traces, achieving up to $35 \%$ attack
success rate, and highlight the challenges in improving stealth.

</details>


### [4] [Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification](https://arxiv.org/abs/2506.04450)
*Payel Bhattacharjee,Fengwei Tian,Ravi Tandon,Joseph Lo,Heidi Hanson,Geoffrey Rubin,Nirav Merchant,John Gounley*

Main category: cs.CR

TL;DR: None


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: What are the main conclusions of the paper? What are the implications of this work?

Abstract: Purpose: This study proposes a framework for fine-tuning large language
models (LLMs) with differential privacy (DP) to perform multi-abnormality
classification on radiology report text. By injecting calibrated noise during
fine-tuning, the framework seeks to mitigate the privacy risks associated with
sensitive patient data and protect against data leakage while maintaining
classification performance. Materials and Methods: We used 50,232 radiology
reports from the publicly available MIMIC-CXR chest radiography and CT-RATE
computed tomography datasets, collected between 2011 and 2019. Fine-tuning of
LLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels
from CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)
in high and moderate privacy regimes (across a range of privacy budgets =
{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1
score across three model architectures: BERT-medium, BERT-small, and
ALBERT-base. Statistical analyses compared model performance across different
privacy levels to quantify the privacy-utility trade-off. Results: We observe a
clear privacy-utility trade-off through our experiments on 2 different datasets
and 3 different models. Under moderate privacy guarantees the DP fine-tuned
models achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on
CT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.
Conclusion: Differentially private fine-tuning using LoRA enables effective and
privacy-preserving multi-abnormality classification from radiology reports,
addressing a key challenge in fine-tuning LLMs on sensitive medical data.

</details>


### [5] [BESA: Boosting Encoder Stealing Attack with Perturbation Recovery](https://arxiv.org/abs/2506.04556)
*Xuhao Ren,Haotian Liang,Yajie Wang,Chuan Zhang,Zehui Xiong,Liehuang Zhu*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: To boost the encoder stealing attack under the perturbation-based defense
that hinders the attack performance, we propose a boosting encoder stealing
attack with perturbation recovery named BESA. It aims to overcome
perturbation-based defenses. The core of BESA consists of two modules:
perturbation detection and perturbation recovery, which can be combined with
canonical encoder stealing attacks. The perturbation detection module utilizes
the feature vectors obtained from the target encoder to infer the defense
mechanism employed by the service provider. Once the defense mechanism is
detected, the perturbation recovery module leverages the well-designed
generative model to restore a clean feature vector from the perturbed one.
Through extensive evaluations based on various datasets, we demonstrate that
BESA significantly enhances the surrogate encoder accuracy of existing encoder
stealing attacks by up to 24.63\% when facing state-of-the-art defenses and
combinations of multiple defenses.

</details>


### [6] [Incentivizing Collaborative Breach Detection](https://arxiv.org/abs/2506.04634)
*Mridu Nanda,Michael K. Reiter*

Main category: cs.CR

TL;DR: A one-sentence summary of the paper.


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: Decoy passwords, or "honeywords," alert a site to its breach if they are ever
entered in a login attempt on that site. However, an attacker can identify a
user-chosen password from among the decoys, without risk of alerting the site
to its breach, by performing credential stuffing, i.e., entering the stolen
passwords at another site where the same user reused her password. Prior work
has thus proposed that sites monitor for the entry of their honeywords at other
sites. Unfortunately, it is not clear what incentives sites have to participate
in this monitoring. In this paper we propose and evaluate an algorithm by which
sites can exchange monitoring favors. Through a model-checking analysis, we
show that using our algorithm, a site improves its ability to detect its own
breach when it increases the monitoring effort it expends for other sites. We
additionally quantify the impacts of various parameters on detection
effectiveness and their implications for the deployment of a system to support
a monitoring ecosystem. Finally, we evaluate our algorithm on a real dataset of
breached credentials and provide a performance analysis that confirms its
scalability and practical viability.

</details>


### [7] [Authenticated Private Set Intersection: A Merkle Tree-Based Approach for Enhancing Data Integrity](https://arxiv.org/abs/2506.04647)
*Zixian Gong,Zhiyong Zheng,Zhe Hu,Kun Tian,Yi Zhang,Zhedanov Oleksiy,Fengxia Liu*

Main category: cs.CR

TL;DR: None


<details>
  <summary>Details</summary>
Motivation: Why is the paper proposing new authenticated PSI schemes? What is the problem with existing schemes?

Method: What is the proposed method for constructing authenticated PSI schemes? How does it integrate Merkle Trees with volePSI and mPSI protocols?

Result: What are the main results of the paper? What is the communication complexity of the proposed schemes?

Conclusion: What are the implications of the proposed authenticated PSI schemes? How do they compare to existing schemes?

Abstract: Private Set Intersection (PSI) enables secure computation of set
intersections while preserving participant privacy, standard PSI existing
protocols remain vulnerable to data integrity attacks allowing malicious
participants to extract additional intersection information or mislead other
parties. In this paper, we propose the definition of data integrity in PSI and
construct two authenticated PSI schemes by integrating Merkle Trees with
state-of-the-art two-party volePSI and multi-party mPSI protocols. The
resulting two-party authenticated PSI achieves communication complexity
$\mathcal{O}(n \lambda+n \log n)$, aligning with the best-known unauthenticated
PSI schemes, while the multi-party construction is $\mathcal{O}(n \kappa+n \log
n)$ which introduces additional overhead due to Merkle tree inclusion proofs.
Due to the incorporation of integrity verification, our authenticated schemes
incur higher costs compared to state-of-the-art unauthenticated schemes. We
also provide efficient implementations of our protocols and discuss potential
improvements, including alternative authentication blocks.

</details>


### [8] [MULTISS: un protocole de stockage confidentiel {à} long terme sur plusieurs r{é}seaux QKD](https://arxiv.org/abs/2506.04800)
*Thomas Prévost,Olivier Alibart,Marc Kaplan,Anne Marin*

Main category: cs.CR

TL;DR: A one-sentence summary of the paper.


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: This paper presents MULTISS, a new protocol for long-term storage distributed
across multiple Quantum Key Distribution (QKD) networks. This protocol is an
extension of LINCOS, a secure storage protocol that uses Shamir secret sharing
for secret storage on a single QKD network. Our protocol uses hierarchical
secret sharing to distribute a secret across multiple QKD networks while
ensuring perfect security. Our protocol further allows for sharing updates
without having to reconstruct the entire secret. We also prove that MULTISS is
strictly more secure than LINCOS, which remains vulnerable when its QKD network
is compromised.

</details>


### [9] [On Automating Security Policies with Contemporary LLMs](https://arxiv.org/abs/2506.04838)
*Pablo Fernández Saura,K. R. Jayaram,Vatche Isahagian,Jorge Bernal Bernabé,Antonio Skarmeta*

Main category: cs.CR

TL;DR: None


<details>
  <summary>Details</summary>
Motivation: 现代计算环境复杂性和网络威胁的日益复杂化需要更加强大、适应性和自动化的安全执行方法。

Method: 提出一个利用大型语言模型（LLMs）的框架，通过创新性地结合情境学习和检索增强生成（RAG）来自动化攻击缓解策略的遵守。

Result: 在采用RAG与基线相比时，实验评估显示了在精度、召回率和F1分数方面的显著改进。

Conclusion: None

Abstract: The complexity of modern computing environments and the growing
sophistication of cyber threats necessitate a more robust, adaptive, and
automated approach to security enforcement. In this paper, we present a
framework leveraging large language models (LLMs) for automating attack
mitigation policy compliance through an innovative combination of in-context
learning and retrieval-augmented generation (RAG). We begin by describing how
our system collects and manages both tool and API specifications, storing them
in a vector database to enable efficient retrieval of relevant information. We
then detail the architectural pipeline that first decomposes high-level
mitigation policies into discrete tasks and subsequently translates each task
into a set of actionable API calls. Our empirical evaluation, conducted using
publicly available CTI policies in STIXv2 format and Windows API documentation,
demonstrates significant improvements in precision, recall, and F1-score when
employing RAG compared to a non-RAG baseline.

</details>


### [10] [A Private Smart Wallet with Probabilistic Compliance](https://arxiv.org/abs/2506.04853)
*Andrea Rizzini,Marco Esposito,Francesco Bruschi,Donatella Sciuto*

Main category: cs.CR

TL;DR: A one-sentence summary of the paper.


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: We propose a privacy-preserving smart wallet with a novel invitation-based
private onboarding mechanism. The solution integrates two levels of compliance
in concert with an authority party: a proof of innocence mechanism and an
ancestral commitment tracking system using bloom filters for probabilistic UTXO
chain states. Performance analysis demonstrates practical efficiency: private
transfers with compliance checks complete within seconds on a consumer-grade
laptop, and overall with proof generation remaining low. On-chain costs stay
minimal, ensuring affordability for all operations on Base layer 2 network. The
wallet facilitates private contact list management through encrypted data blobs
while maintaining transaction unlinkability. Our evaluation validates the
approach's viability for privacy-preserving, compliance-aware digital payments
with minimized computational and financial overhead.

</details>


### [11] [PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm Packages](https://arxiv.org/abs/2506.04962)
*Deniz Simsek,Aryaz Eghbali,Michael Pradel*

Main category: cs.CR

TL;DR: 本文提出了一种名为PoCGen的新方法，用于自动生成和验证npm软件包中漏洞的概念验证（PoC）利用。


<details>
  <summary>Details</summary>
Motivation: 软件包中的安全漏洞是开发者和用户共同关注的问题。及时修补这些漏洞对于恢复软件系统的完整性和安全性至关重要。然而，以前的研究表明，漏洞报告通常缺乏概念验证（PoC）利用，这对于修复漏洞、测试补丁和避免回归至关重要。创建PoC利用具有挑战性，因为漏洞报告是非正式的，通常不完整，并且需要详细了解输入如何传递到可能存在漏洞的API，以及它们如何到达与安全相关的汇合点。

Method: 本文提出了PoCGen，这是一种新颖的方法，用于自动生成和验证npm软件包中漏洞的概念验证（PoC）利用。这是第一个完全自动化的方法，结合使用大型语言模型（LLMs）和静态和动态分析技术来生成PoC利用。PoCGen利用LLM来理解漏洞报告，生成候选PoC利用，并验证和改进它们。

Result: 我们的方法成功生成了SecBench.js数据集中77%的漏洞的利用程序，以及794个新漏洞数据集中39%的利用程序。这种成功率显著优于最近的一个基线（高出45个绝对百分点），而每个生成的利用程序的平均成本为0.02美元。

Conclusion: None

Abstract: Security vulnerabilities in software packages are a significant concern for
developers and users alike. Patching these vulnerabilities in a timely manner
is crucial to restoring the integrity and security of software systems.
However, previous work has shown that vulnerability reports often lack
proof-of-concept (PoC) exploits, which are essential for fixing the
vulnerability, testing patches, and avoiding regressions. Creating a PoC
exploit is challenging because vulnerability reports are informal and often
incomplete, and because it requires a detailed understanding of how inputs
passed to potentially vulnerable APIs may reach security-relevant sinks. In
this paper, we present PoCGen, a novel approach to autonomously generate and
validate PoC exploits for vulnerabilities in npm packages. This is the first
fully autonomous approach to use large language models (LLMs) in tandem with
static and dynamic analysis techniques for PoC exploit generation. PoCGen
leverages an LLM for understanding vulnerability reports, for generating
candidate PoC exploits, and for validating and refining them. Our approach
successfully generates exploits for 77% of the vulnerabilities in the
SecBench.js dataset and 39% in a new, more challenging dataset of 794 recent
vulnerabilities. This success rate significantly outperforms a recent baseline
(by 45 absolute percentage points), while imposing an average cost of $0.02 per
generated exploit.

</details>


### [12] [Hiding in Plain Sight: Query Obfuscation via Random Multilingual Searches](https://arxiv.org/abs/2506.04963)
*Anton Firc,Jan Klusáček,Kamil Malinka*

Main category: cs.CR

TL;DR: None


<details>
  <summary>Details</summary>
Motivation: 现代搜索引擎通过构建详细的用户档案来个性化搜索结果，这虽然提高了相关性，但也引入了隐私风险和过滤泡的风险。本文提出了一个轻量级的客户端查询混淆策略，使用随机生成的多语言搜索查询来干扰用户档案。

Method: 通过在Seznam.cz搜索引擎上进行控制实验，评估了在各种语言配置和比例下，将真实查询与混淆噪声交织在一起的影响。

Result: 结果显示，虽然显示的搜索结果总体上保持稳定，但在混淆的情况下，搜索引擎识别的用户兴趣发生了显著变化。随机查询可以防止准确的用户档案和覆盖现有的用户档案。

Conclusion: 这项研究为查询混淆作为一种可行的隐私保护机制提供了实际证据，并介绍了一个工具，使用户能够自主地保护他们的搜索行为，而无需修改现有基础设施。

Abstract: Modern search engines extensively personalize results by building detailed
user profiles based on query history and behaviour. While personalization can
enhance relevance, it introduces privacy risks and can lead to filter bubbles.
This paper proposes and evaluates a lightweight, client-side query obfuscation
strategy using randomly generated multilingual search queries to disrupt user
profiling. Through controlled experiments on the Seznam.cz search engine, we
assess the impact of interleaving real queries with obfuscating noise in
various language configurations and ratios. Our findings show that while
displayed search results remain largely stable, the search engine's identified
user interests shift significantly under obfuscation. We further demonstrate
that such random queries can prevent accurate profiling and overwrite
established user profiles. This study provides practical evidence for query
obfuscation as a viable privacy-preserving mechanism and introduces a tool that
enables users to autonomously protect their search behaviour without modifying
existing infrastructure.

</details>


### [13] [Evaluating the Impact of Privacy-Preserving Federated Learning on CAN Intrusion Detection](https://arxiv.org/abs/2506.04978)
*Gabriele Digregorio,Elisabetta Cainazzo,Stefano Longari,Michele Carminati,Stefano Zanero*

Main category: cs.CR

TL;DR: A one-sentence summary of the paper.


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: The challenges derived from the data-intensive nature of machine learning in
conjunction with technologies that enable novel paradigms such as V2X and the
potential offered by 5G communication, allow and justify the deployment of
Federated Learning (FL) solutions in the vehicular intrusion detection domain.
In this paper, we investigate the effects of integrating FL strategies into the
machine learning-based intrusion detection process for on-board vehicular
networks. Accordingly, we propose a FL implementation of a state-of-the-art
Intrusion Detection System (IDS) for Controller Area Network (CAN), based on
LSTM autoencoders. We thoroughly evaluate its detection efficiency and
communication overhead, comparing it to a centralized version of the same
algorithm, thereby presenting it as a feasible solution.

</details>


### [14] [Attack Effect Model based Malicious Behavior Detection](https://arxiv.org/abs/2506.05001)
*Limin Wang,Lei Bu,Muzimiao Zhang,Shihong Cang,Kai Ye*

Main category: cs.CR

TL;DR: 提出了一种名为FEAD的框架，通过三个创新解决了传统安全检测方法的问题，提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统安全检测方法存在数据收集不足、监控系统资源密集和检测算法误报率高等问题。

Method: 提出了一种名为FEAD的框架，通过攻击模型驱动的方法提取关键监控项，高效的任务分解以及局部感知异常分析来提高检测准确率。

Result: FEAD在F1-score上比现有解决方案提高了8.23%，且只有5.4%的开销。

Conclusion: 基于关注的FEAD框架显著提高了检测性能。

Abstract: Traditional security detection methods face three key challenges: inadequate
data collection that misses critical security events, resource-intensive
monitoring systems, and poor detection algorithms with high false positive
rates. We present FEAD (Focus-Enhanced Attack Detection), a framework that
addresses these issues through three innovations: (1) an attack model-driven
approach that extracts security-critical monitoring items from online attack
reports for comprehensive coverage; (2) efficient task decomposition that
optimally distributes monitoring across existing collectors to minimize
overhead; and (3) locality-aware anomaly analysis that leverages the clustering
behavior of malicious activities in provenance graphs to improve detection
accuracy. Evaluations demonstrate FEAD achieves 8.23% higher F1-score than
existing solutions with only 5.4% overhead, confirming that focus-based designs
significantly enhance detection performance.

</details>


### [15] [EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers](https://arxiv.org/abs/2506.05074)
*Robert J. Joyce,Gideon Miller,Phil Roth,Richard Zak,Elliott Zaresky-Williams,Hyrum Anderson,Edward Raff,James Holt*

Main category: cs.CR

TL;DR: A one-sentence summary of the paper


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: What are the main conclusions of the paper? What are the implications of this work?

Abstract: A lack of accessible data has historically restricted malware analysis
research, and practitioners have relied heavily on datasets provided by
industry sources to advance. Existing public datasets are limited by narrow
scope - most include files targeting a single platform, have labels supporting
just one type of malware classification task, and make no effort to capture the
evasive files that make malware detection difficult in practice. We present
EMBER2024, a new dataset that enables holistic evaluation of malware
classifiers. Created in collaboration with the authors of EMBER2017 and
EMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,
and labels for more than 3.2 million files from six file formats. Our dataset
supports the training and evaluation of machine learning models on seven
malware classification tasks, including malware detection, malware family
classification, and malware behavior identification. EMBER2024 is the first to
include a collection of malicious files that initially went undetected by a set
of antivirus products, creating a "challenge" set to assess classifier
performance against evasive malware. This work also introduces EMBER feature
version 3, with added support for several new feature types. We are releasing
the EMBER2024 dataset to promote reproducibility and empower researchers in the
pursuit of new malware research topics.

</details>


### [16] [Membership Inference Attacks on Sequence Models](https://arxiv.org/abs/2506.05126)
*Lorenzo Rossi,Michael Aerni,Jie Zhang,Florian Tramèr*

Main category: cs.CR

TL;DR: 本文提出了一种新的方法来测量序列模型中的隐私泄露，通过利用序列生成的内在相关性，显著提高了记忆审计的有效性。


<details>
  <summary>Details</summary>
Motivation: 序列模型，如大型语言模型（LLMs）和自回归图像生成器，倾向于记忆并无意中泄露敏感信息。这种趋势具有重大的法律意义，而现有的工具在审计由此产生的风险方面不足。

Method: 我们假设这些工具的不足是由于假设不匹配。因此，我们认为有效地测量序列模型中的隐私泄露需要利用序列生成的内在相关性。为了说明这一点，我们采用了一种最先进的成员身份推理攻击来显式地建模序列内的相关性，从而展示了如何将一个强大的现有攻击自然地扩展以适应序列模型的架构。

Result: 通过案例研究，我们发现我们的改进在提高记忆审计的有效性方面是一致的，而没有引入额外的计算成本。因此，我们的工作对于可靠的记忆审计对于大型序列模型是一个重要的里程碑。

Conclusion: 本文提出了一种新的方法来测量序列模型中的隐私泄露，通过利用序列生成的内在相关性，显著提高了记忆审计的有效性。

Abstract: Sequence models, such as Large Language Models (LLMs) and autoregressive
image generators, have a tendency to memorize and inadvertently leak sensitive
information. While this tendency has critical legal implications, existing
tools are insufficient to audit the resulting risks. We hypothesize that those
tools' shortcomings are due to mismatched assumptions. Thus, we argue that
effectively measuring privacy leakage in sequence models requires leveraging
the correlations inherent in sequential generation. To illustrate this, we
adapt a state-of-the-art membership inference attack to explicitly model
within-sequence correlations, thereby demonstrating how a strong existing
attack can be naturally extended to suit the structure of sequence models.
Through a case study, we show that our adaptations consistently improve the
effectiveness of memorization audits without introducing additional
computational costs. Our work hence serves as an important stepping stone
toward reliable memorization audits for large sequence models.

</details>


### [17] [OpenCCA: An Open Framework to Enable Arm CCA Research](https://arxiv.org/abs/2506.05129)
*Andrin Bertschi,Shweta Shinde*

Main category: cs.CR

TL;DR: None


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: Confidential computing has gained traction across major architectures with
Intel TDX, AMD SEV-SNP, and Arm CCA. Unlike TDX and SEV-SNP, a key challenge in
researching Arm CCA is the absence of hardware support, forcing researchers to
develop ad-hoc performance prototypes on non-CCA Arm boards. This approach
leads to duplicated efforts, inconsistent performance comparisons, and high
barriers to entry. To address this, we present OpenCCA, an open research
platform that enables the execution of CCA-bound code on commodity Armv8.2
hardware. By systematically adapting the software stack -- including
bootloader, firmware, hypervisor, and kernel -- OpenCCA emulates CCA operations
for performance evaluation while preserving functional correctness. We
demonstrate its effectiveness with typical life-cycle measurements and
case-studies inspired by prior CCA-based papers on a easily available Armv8.2
Rockchip board that costs $250.

</details>


### [18] [SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid Neuron Encryption](https://arxiv.org/abs/2506.05242)
*Zhiqiang Wang,Haohua Du,Junyang Wang,Haifeng Sun,Kaiwen Guo,Haikuo Yu,Chao Liu,Xiang-Yang Li*

Main category: cs.CR

TL;DR: A one-sentence summary of the paper.


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: Large language models (LLMs) with diverse capabilities are increasingly being
deployed in local environments, presenting significant security and
controllability challenges. These locally deployed LLMs operate outside the
direct control of developers, rendering them more susceptible to abuse.
Existing mitigation techniques mainly designed for cloud-based LLM services are
frequently circumvented or ineffective in deployer-controlled environments. We
propose SECNEURON, the first framework that seamlessly embeds classic access
control within the intrinsic capabilities of LLMs, achieving reliable,
cost-effective, flexible, and certified abuse control for local deployed LLMs.
SECNEURON employs neuron-level encryption and selective decryption to
dynamically control the task-specific capabilities of LLMs, limiting
unauthorized task abuse without compromising others. We first design a
task-specific neuron extraction mechanism to decouple logically related neurons
and construct a layered policy tree for handling coupled neurons. We then
introduce a flexible and efficient hybrid encryption framework for millions of
neurons in LLMs. Finally, we developed a distribution-based decrypted neuron
detection mechanism on ciphertext to ensure the effectiveness of partially
decrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and
Collusion Resistance Security under the Task Controllability Principle.
Experiments on various task settings show that SECNEURON limits unauthorized
task accuracy to below 25% while keeping authorized accuracy loss with 2%.
Using an unauthorized Code task example, the accuracy of abuse-related
malicious code generation was reduced from 59% to 15%. SECNEURON also mitigates
unauthorized data leakage, reducing PII extraction rates to below 5% and
membership inference to random guesses.

</details>


### [19] [Big Bird: Privacy Budget Management for W3C's Privacy-Preserving Attribution API](https://arxiv.org/abs/2506.05290)
*Pierre Tholoniat,Alison Caulfield,Giorgio Cavicchioli,Mark Chen,Nikos Goutzoulias,Benjamin Case,Asaf Cidon,Roxana Geambasu,Mathias Lécuyer,Martin Thomson*

Main category: cs.CR

TL;DR: None


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: What are the main conclusions of the paper? What are the implications of this work?

Abstract: Privacy-preserving advertising APIs like Privacy-Preserving Attribution (PPA)
are designed to enhance web privacy while enabling effective ad measurement.
PPA offers an alternative to cross-site tracking with encrypted reports
governed by differential privacy (DP), but current designs lack a principled
approach to privacy budget management, creating uncertainty around critical
design decisions. We present Big Bird, a privacy budget manager for PPA that
clarifies per-site budget semantics and introduces a global budgeting system
grounded in resource isolation principles. Big Bird enforces utility-preserving
limits via quota budgets and improves global budget utilization through a novel
batched scheduling algorithm. Together, these mechanisms establish a robust
foundation for enforcing privacy protections in adversarial environments. We
implement Big Bird in Firefox and evaluate it on real-world ad data,
demonstrating its resilience and effectiveness.

</details>


### [20] [Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets](https://arxiv.org/abs/2506.05346)
*Lei Hsiung,Tianyu Pang,Yung-Chen Tang,Linyue Song,Tsung-Yi Ho,Pin-Yu Chen,Yaoqing Yang*

Main category: cs.CR

TL;DR: A one-sentence summary of the paper.


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)
*Kai Wang,Yihao Zhang,Meng Sun*

Main category: cs.AI

TL;DR: This paper proposes a method to detect and control deception in CoT-enabled LLMs using representation engineering and activation steering, achieving high detection accuracy and providing tools for trustworthy AI alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the issue of honesty in large language models (LLMs), particularly in advanced systems with chain-of-thought (CoT) reasoning that may strategically deceive humans. The paper aims to study strategic deception, which is goal-driven, intentional misinformation where reasoning contradicts outputs.

Method: The paper proposes a method using representation engineering to systematically induce, detect, and control deception in CoT-enabled LLMs. It uses Linear Artificial Tomography (LAT) to extract 'deception vectors' for 89% detection accuracy. Additionally, it employs activation steering to achieve a 40% success rate in eliciting context-appropriate deception without explicit prompts.

Result: The paper achieves 89% detection accuracy in deception vectors using Linear Artificial Tomography (LAT) and a 40% success rate in eliciting context-appropriate deception without explicit prompts. It also provides tools for trustworthy AI alignment and reveals the specific honesty-related issue of reasoning models.

Conclusion: None

Abstract: The honesty of large language models (LLMs) is a critical alignment
challenge, especially as advanced systems with chain-of-thought (CoT) reasoning
may strategically deceive humans. Unlike traditional honesty issues on LLMs,
which could be possibly explained as some kind of hallucination, those models'
explicit thought paths enable us to study strategic deception--goal-driven,
intentional misinformation where reasoning contradicts outputs. Using
representation engineering, we systematically induce, detect, and control such
deception in CoT-enabled LLMs, extracting "deception vectors" via Linear
Artificial Tomography (LAT) for 89% detection accuracy. Through activation
steering, we achieve a 40% success rate in eliciting context-appropriate
deception without explicit prompts, unveiling the specific honesty-related
issue of reasoning models and providing tools for trustworthy AI alignment.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
*Apurv Verma,NhatHai Phan,Shubhendu Trivedi*

Main category: cs.CL

TL;DR: A one-sentence summary of the paper.


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: Watermarking techniques for large language models (LLMs) can significantly
impact output quality, yet their effects on truthfulness, safety, and
helpfulness remain critically underexamined. This paper presents a systematic
analysis of how two popular watermarking approaches-Gumbel and KGW-affect these
core alignment properties across four aligned LLMs. Our experiments reveal two
distinct degradation patterns: guard attenuation, where enhanced helpfulness
undermines model safety, and guard amplification, where excessive caution
reduces model helpfulness. These patterns emerge from watermark-induced shifts
in token distribution, surfacing the fundamental tension that exists between
alignment objectives.
  To mitigate these degradations, we propose Alignment Resampling (AR), an
inference-time sampling method that uses an external reward model to restore
alignment. We establish a theoretical lower bound on the improvement in
expected reward score as the sample size is increased and empirically
demonstrate that sampling just 2-4 watermarked generations effectively recovers
or surpasses baseline (unwatermarked) alignment scores. To overcome the limited
response diversity of standard Gumbel watermarking, our modified implementation
sacrifices strict distortion-freeness while maintaining robust detectability,
ensuring compatibility with AR. Experimental results confirm that AR
successfully recovers baseline alignment in both watermarking approaches, while
maintaining strong watermark detectability. This work reveals the critical
balance between watermark strength and model alignment, providing a simple
inference-time solution to responsibly deploy watermarked LLMs in practice.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [23] [Tech-ASan: Two-stage check for Address Sanitizer](https://arxiv.org/abs/2506.05022)
*Yixuan Cao,Yuhong Feng,Huafeng Li,Chongyi Huang,Fangcao Jian,Haoran Li,Xu Wang*

Main category: cs.SE

TL;DR: A one-sentence summary of the paper


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: What are the main conclusions of the paper? What are the implications of this work?

Abstract: Address Sanitizer (ASan) is a sharp weapon for detecting memory safety
violations, including temporal and spatial errors hidden in C/C++ programs
during execution. However, ASan incurs significant runtime overhead, which
limits its efficiency in testing large software. The overhead mainly comes from
sanitizer checks due to the frequent and expensive shadow memory access. Over
the past decade, many methods have been developed to speed up ASan by
eliminating and accelerating sanitizer checks, however, they either fail to
adequately eliminate redundant checks or compromise detection capabilities. To
address this issue, this paper presents Tech-ASan, a two-stage check based
technique to accelerate ASan with safety assurance. First, we propose a novel
two-stage check algorithm for ASan, which leverages magic value comparison to
reduce most of the costly shadow memory accesses. Second, we design an
efficient optimizer to eliminate redundant checks, which integrates a novel
algorithm for removing checks in loops. Third, we implement Tech-ASan as a
memory safety tool based on the LLVM compiler infrastructure. Our evaluation
using the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the
state-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan
and ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative
cases than ASan and ASan-- when testing on the Juliet Test Suite under the same
redzone setting.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Urania: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)
*Daogao Liu,Edith Cohen,Badih Ghazi,Peter Kairouz,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Adam Sealfon,Da Yu,Chiyuan Zhang*

Main category: cs.LG

TL;DR: A one-sentence summary of the paper.


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: We introduce $Urania$, a novel framework for generating insights about LLM
chatbot interactions with rigorous differential privacy (DP) guarantees. The
framework employs a private clustering mechanism and innovative keyword
extraction methods, including frequency-based, TF-IDF-based, and LLM-guided
approaches. By leveraging DP tools such as clustering, partition selection, and
histogram-based summarization, $Urania$ provides end-to-end privacy protection.
Our evaluation assesses lexical and semantic content preservation, pair
similarity, and LLM-based metrics, benchmarking against a non-private
Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple
empirical privacy evaluation that demonstrates the enhanced robustness of our
DP pipeline. The results show the framework's ability to extract meaningful
conversational insights while maintaining stringent user privacy, effectively
balancing data utility with privacy preservation.

</details>


### [25] [Identifying and Understanding Cross-Class Features in Adversarial Training](https://arxiv.org/abs/2506.05032)
*Zeming Wei,Yiwen Guo,Yisen Wang*

Main category: cs.LG

TL;DR: A one-sentence summary of the paper


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: What are the main conclusions of the paper? What are the implications of this work?

Abstract: Adversarial training (AT) has been considered one of the most effective
methods for making deep neural networks robust against adversarial attacks,
while the training mechanisms and dynamics of AT remain open research problems.
In this paper, we present a novel perspective on studying AT through the lens
of class-wise feature attribution. Specifically, we identify the impact of a
key family of features on AT that are shared by multiple classes, which we call
cross-class features. These features are typically useful for robust
classification, which we offer theoretical evidence to illustrate through a
synthetic data model. Through systematic studies across multiple model
architectures and settings, we find that during the initial stage of AT, the
model tends to learn more cross-class features until the best robustness
checkpoint. As AT further squeezes the training robust loss and causes robust
overfitting, the model tends to make decisions based on more class-specific
features. Based on these discoveries, we further provide a unified view of two
existing properties of AT, including the advantage of soft-label training and
robust overfitting. Overall, these insights refine the current understanding of
AT mechanisms and provide new perspectives on studying them. Our code is
available at https://github.com/PKU-ML/Cross-Class-Features-AT.

</details>


### [26] [Privacy Amplification Through Synthetic Data: Insights from Linear Regression](https://arxiv.org/abs/2506.05101)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.LG

TL;DR: None


<details>
  <summary>Details</summary>
Motivation: 研究合成数据在隐私保护方面的特性，特别是当生成模型被隐藏时。

Method: 通过线性回归框架进行理论分析，包括负结果和正结果的证明。

Result: 证明了当生成模型种子被控制时，单个合成数据点可能泄露与模型本身相同的信息；而当合成数据从随机输入生成时，释放有限数量的合成数据点可以增强隐私保护。

Conclusion: 线性回归框架下的研究结果为未来更广泛的理论分析提供了基础。

Abstract: Synthetic data inherits the differential privacy guarantees of the model used
to generate it. Additionally, synthetic data may benefit from privacy
amplification when the generative model is kept hidden. While empirical studies
suggest this phenomenon, a rigorous theoretical understanding is still lacking.
In this paper, we investigate this question through the well-understood
framework of linear regression. First, we establish negative results showing
that if an adversary controls the seed of the generative model, a single
synthetic data point can leak as much information as releasing the model
itself. Conversely, we show that when synthetic data is generated from random
inputs, releasing a limited number of synthetic data points amplifies privacy
beyond the model's inherent guarantees. We believe our findings in linear
regression can serve as a foundation for deriving more general bounds in the
future.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [27] [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)
*Zhanhui Zhou,Lingjie Chen,Chao Yang,Chaochao Lu*

Main category: cs.CV

TL;DR: A one-sentence summary of the paper.


<details>
  <summary>Details</summary>
Motivation: What problem is the paper trying to solve? What is the motivation for this work?

Method: How did the paper solve the problem? What is the proposed method?

Result: What are the main results of the paper?

Conclusion: None

Abstract: One way to mitigate risks in vision-language models (VLMs) is to remove
dangerous samples in their training data. However, such data moderation can be
easily bypassed when harmful images are split into small, benign-looking
patches, scattered across many training samples. VLMs may then learn to piece
these fragments together during training and generate harmful responses at
inference, either from full images or text references. For instance, if trained
on image patches from a bloody scene paired with the descriptions "safe," VLMs
may later describe, the full image or a text reference to the scene, as "safe."
We define the core ability of VLMs enabling this attack as $\textit{visual
stitching}$ -- the ability to integrate visual information spread across
multiple training samples that share the same textual descriptions. In our
work, we first demonstrate visual stitching abilities in common open-source
VLMs on three datasets where each image is labeled with a unique synthetic ID:
we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch},
\texttt{ID})\}$ pairs at different granularity for finetuning, and we find that
tuned models can verbalize the correct IDs from full images or text reference.
Building on this, we simulate the adversarial data poisoning scenario mentioned
above by using patches from dangerous images and replacing IDs with text
descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can
evade moderation in patches and later be reconstructed through visual
stitching, posing serious VLM safety risks. Code is available at
https://github.com/ZHZisZZ/visual-stitching.

</details>
