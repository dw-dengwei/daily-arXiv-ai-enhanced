<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 20]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Hello, won't you tell me your name?: Investigating Anonymity Abuse in IPFS](https://arxiv.org/abs/2506.04307)
*Christos Karapapas,Iakovos Pittaras,George C. Polyzos,Constantinos Patsakis*

Main category: cs.CR

TL;DR: IPFS匿名性存在风险，需改进


<details>
  <summary>Details</summary>
Motivation: 研究IPFS的匿名性可能被恶意使用者滥用的问题

Method: 通过上传恶意文件测试pinning服务和公共网关的行为

Result: 发现pinning服务和公共网关缺乏评估或限制恶意内容传播的机制

Conclusion: IPFS的匿名性存在潜在风险，需要进一步研究和改进

Abstract: The InterPlanetary File System~(IPFS) offers a decentralized approach to file
storage and sharing, promising resilience and efficiency while also realizing
the Web3 paradigm. Simultaneously, the offered anonymity raises significant
questions about potential misuse. In this study, we explore methods that
malicious actors can exploit IPFS to upload and disseminate harmful content
while remaining anonymous. We evaluate the role of pinning services and public
gateways, identifying their capabilities and limitations in maintaining content
availability. Using scripts, we systematically test the behavior of these
services by uploading malicious files. Our analysis reveals that pinning
services and public gateways lack mechanisms to assess or restrict the
propagation of malicious content.

</details>


### [2] [The Hashed Fractal Key Recovery (HFKR) Problem: From Symbolic Path Inversion to Post-Quantum Cryptographic Keys](https://arxiv.org/abs/2506.04383)
*Mohamed Aly Bouke*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Classical cryptographic systems rely heavily on structured algebraic
problems, such as factorization, discrete logarithms, or lattice-based
assumptions, which are increasingly vulnerable to quantum attacks and
structural cryptanalysis. In response, this work introduces the Hashed Fractal
Key Recovery (HFKR) problem, a non-algebraic cryptographic construction
grounded in symbolic dynamics and chaotic perturbations. HFKR builds on the
Symbolic Path Inversion Problem (SPIP), leveraging symbolic trajectories
generated via contractive affine maps over $\mathbb{Z}^2$, and compressing them
into fixed-length cryptographic keys using hash-based obfuscation. A key
contribution of this paper is the empirical confirmation that these symbolic
paths exhibit fractal behavior, quantified via box counting dimension, path
geometry, and spatial density measures. The observed fractal dimension
increases with trajectory length and stabilizes near 1.06, indicating symbolic
self-similarity and space-filling complexity, both of which reinforce the
entropy foundation of the scheme. Experimental results across 250 perturbation
trials show that SHA3-512 and SHAKE256 amplify symbolic divergence effectively,
achieving mean Hamming distances near 255, ideal bit-flip rates, and negligible
entropy deviation. In contrast, BLAKE3 exhibits statistically uniform but
weaker diffusion. These findings confirm that HFKR post-quantum security arises
from the synergy between symbolic fractality and hash-based entropy
amplification. The resulting construction offers a lightweight, structure-free
foundation for secure key generation in adversarial settings without relying on
algebraic hardness assumptions.

</details>


### [3] [Through the Stealth Lens: Rethinking Attacks and Defenses in RAG](https://arxiv.org/abs/2506.04390)
*Sarthak Choudhary,Nils Palumbo,Ashish Hooda,Krishnamurthy Dj Dvijotham,Somesh Jha*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Retrieval-augmented generation (RAG) systems are vulnerable to attacks that
inject poisoned passages into the retrieved set, even at low corruption rates.
We show that existing attacks are not designed to be stealthy, allowing
reliable detection and mitigation. We formalize stealth using a
distinguishability-based security game. If a few poisoned passages are designed
to control the response, they must differentiate themselves from benign ones,
inherently compromising stealth. This motivates the need for attackers to
rigorously analyze intermediate signals involved in
generation$\unicode{x2014}$such as attention patterns or next-token probability
distributions$\unicode{x2014}$to avoid easily detectable traces of
manipulation. Leveraging attention patterns, we propose a passage-level
score$\unicode{x2014}$the Normalized Passage Attention
Score$\unicode{x2014}$used by our Attention-Variance Filter algorithm to
identify and filter potentially poisoned passages. This method mitigates
existing attacks, improving accuracy by up to $\sim 20 \%$ over baseline
defenses. To probe the limits of attention-based defenses, we craft stealthier
adaptive attacks that obscure such traces, achieving up to $35 \%$ attack
success rate, and highlight the challenges in improving stealth.

</details>


### [4] [Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification](https://arxiv.org/abs/2506.04450)
*Payel Bhattacharjee,Fengwei Tian,Ravi Tandon,Joseph Lo,Heidi Hanson,Geoffrey Rubin,Nirav Merchant,John Gounley*

Main category: cs.CR

TL;DR: 本研究提出了一种基于差分隐私的LLMs微调框架，用于放射学报告的多异常分类，实现了隐私保护和性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 为了在放射学报告中进行多异常分类，本研究提出了一种框架，通过差分隐私（DP）对大型语言模型（LLMs）进行微调。

Method: 使用MIMIC-CXR胸部X光和CT-RATE计算机断层扫描数据集，通过Differentially Private Low-Rank Adaptation (DP-LoRA)在高级和中级隐私制度下对LLMs进行微调。

Result: 在MIMIC-CXR和CT-RATE数据集上，DP微调模型在中等隐私保证下，分别实现了0.88和0.59的加权F1分数，与非私有LoRA基线（分别为0.90和0.78）相比。

Conclusion: 使用LoRA进行差分隐私微调，可以从放射学报告中有效地进行多异常分类，同时保护隐私，解决在敏感医疗数据上微调LLMs的关键挑战。

Abstract: Purpose: This study proposes a framework for fine-tuning large language
models (LLMs) with differential privacy (DP) to perform multi-abnormality
classification on radiology report text. By injecting calibrated noise during
fine-tuning, the framework seeks to mitigate the privacy risks associated with
sensitive patient data and protect against data leakage while maintaining
classification performance. Materials and Methods: We used 50,232 radiology
reports from the publicly available MIMIC-CXR chest radiography and CT-RATE
computed tomography datasets, collected between 2011 and 2019. Fine-tuning of
LLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels
from CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)
in high and moderate privacy regimes (across a range of privacy budgets =
{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1
score across three model architectures: BERT-medium, BERT-small, and
ALBERT-base. Statistical analyses compared model performance across different
privacy levels to quantify the privacy-utility trade-off. Results: We observe a
clear privacy-utility trade-off through our experiments on 2 different datasets
and 3 different models. Under moderate privacy guarantees the DP fine-tuned
models achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on
CT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.
Conclusion: Differentially private fine-tuning using LoRA enables effective and
privacy-preserving multi-abnormality classification from radiology reports,
addressing a key challenge in fine-tuning LLMs on sensitive medical data.

</details>


### [5] [BESA: Boosting Encoder Stealing Attack with Perturbation Recovery](https://arxiv.org/abs/2506.04556)
*Xuhao Ren,Haotian Liang,Yajie Wang,Chuan Zhang,Zehui Xiong,Liehuang Zhu*

Main category: cs.CR

TL;DR: BESA：一种增强型编码器窃取攻击，通过扰动检测和恢复，显著提高攻击性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升在基于扰动的防御下编码器窃取攻击的性能，我们提出了一种名为BESA的增强型编码器窃取攻击，旨在克服基于扰动的防御。

Method: BESA的核心包括两个模块：扰动检测和扰动恢复，可以与规范编码器窃取攻击相结合。扰动检测模块利用从目标编码器获得的特征向量来推断服务提供商采用的防御机制。一旦检测到防御机制，扰动恢复模块利用精心设计的生成模型从扰动的特征向量中恢复干净的特性向量。

Result: 在多个数据集上的广泛评估表明，BESA在面对最先进的防御和多种防御的组合时，将现有编码器窃取攻击的代理编码器准确性提高了高达24.63%。

Conclusion: BESA是一种有效的增强型编码器窃取攻击，可以显著提高攻击性能，克服基于扰动的防御。

Abstract: To boost the encoder stealing attack under the perturbation-based defense
that hinders the attack performance, we propose a boosting encoder stealing
attack with perturbation recovery named BESA. It aims to overcome
perturbation-based defenses. The core of BESA consists of two modules:
perturbation detection and perturbation recovery, which can be combined with
canonical encoder stealing attacks. The perturbation detection module utilizes
the feature vectors obtained from the target encoder to infer the defense
mechanism employed by the service provider. Once the defense mechanism is
detected, the perturbation recovery module leverages the well-designed
generative model to restore a clean feature vector from the perturbed one.
Through extensive evaluations based on various datasets, we demonstrate that
BESA significantly enhances the surrogate encoder accuracy of existing encoder
stealing attacks by up to 24.63\% when facing state-of-the-art defenses and
combinations of multiple defenses.

</details>


### [6] [Incentivizing Collaborative Breach Detection](https://arxiv.org/abs/2506.04634)
*Mridu Nanda,Michael K. Reiter*

Main category: cs.CR

TL;DR: 本文提出了一种通过交换监控互助来提高网站入侵检测能力的算法，并通过实证研究验证了其有效性和可行性。


<details>
  <summary>Details</summary>
Motivation: 为了提高网站对其自身被入侵的检测能力，研究者们提出了监控其他网站登录尝试中的蜜罐密码（honeywords）的方法。然而，这种方法并不明确网站为何要参与监控。

Method: 本文提出并评估了一种算法，通过该算法，网站可以交换监控互助。通过模型检查分析，我们发现，使用我们的算法，当网站增加其用于其他网站的监控努力时，它可以提高其检测自身被入侵的能力。

Result: 研究结果表明，通过增加对其他网站的监控努力，网站可以提高其检测自身被入侵的能力。此外，我们还量化了各种参数对检测有效性的影响及其对支持监控生态系统的系统的部署的启示。最后，我们在真实的数据集上评估了我们的算法，并提供了性能分析，证实了其可扩展性和实际可行性。

Conclusion: 本文提出的算法能够有效提高网站检测自身被入侵的能力，并通过量化分析和实际评估验证了其可扩展性和实际可行性。

Abstract: Decoy passwords, or "honeywords," alert a site to its breach if they are ever
entered in a login attempt on that site. However, an attacker can identify a
user-chosen password from among the decoys, without risk of alerting the site
to its breach, by performing credential stuffing, i.e., entering the stolen
passwords at another site where the same user reused her password. Prior work
has thus proposed that sites monitor for the entry of their honeywords at other
sites. Unfortunately, it is not clear what incentives sites have to participate
in this monitoring. In this paper we propose and evaluate an algorithm by which
sites can exchange monitoring favors. Through a model-checking analysis, we
show that using our algorithm, a site improves its ability to detect its own
breach when it increases the monitoring effort it expends for other sites. We
additionally quantify the impacts of various parameters on detection
effectiveness and their implications for the deployment of a system to support
a monitoring ecosystem. Finally, we evaluate our algorithm on a real dataset of
breached credentials and provide a performance analysis that confirms its
scalability and practical viability.

</details>


### [7] [Authenticated Private Set Intersection: A Merkle Tree-Based Approach for Enhancing Data Integrity](https://arxiv.org/abs/2506.04647)
*Zixian Gong,Zhiyong Zheng,Zhe Hu,Kun Tian,Yi Zhang,Zhedanov Oleksiy,Fengxia Liu*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Private Set Intersection (PSI) enables secure computation of set
intersections while preserving participant privacy, standard PSI existing
protocols remain vulnerable to data integrity attacks allowing malicious
participants to extract additional intersection information or mislead other
parties. In this paper, we propose the definition of data integrity in PSI and
construct two authenticated PSI schemes by integrating Merkle Trees with
state-of-the-art two-party volePSI and multi-party mPSI protocols. The
resulting two-party authenticated PSI achieves communication complexity
$\mathcal{O}(n \lambda+n \log n)$, aligning with the best-known unauthenticated
PSI schemes, while the multi-party construction is $\mathcal{O}(n \kappa+n \log
n)$ which introduces additional overhead due to Merkle tree inclusion proofs.
Due to the incorporation of integrity verification, our authenticated schemes
incur higher costs compared to state-of-the-art unauthenticated schemes. We
also provide efficient implementations of our protocols and discuss potential
improvements, including alternative authentication blocks.

</details>


### [8] [MULTISS: un protocole de stockage confidentiel {à} long terme sur plusieurs r{é}seaux QKD](https://arxiv.org/abs/2506.04800)
*Thomas Prévost,Olivier Alibart,Marc Kaplan,Anne Marin*

Main category: cs.CR

TL;DR: MULTISS是一种安全高效的量子密钥分发网络长期存储协议。


<details>
  <summary>Details</summary>
Motivation: 为了提高量子密钥分发（QKD）网络的长期存储安全性，提出了一种新的协议MULTISS。

Method: 利用分层秘密共享技术，将秘密分布在多个QKD网络上，并确保完美安全性。

Result: MULTISS协议比LINCOS协议更安全，LINCOS协议在QKD网络被破坏时存在漏洞。

Conclusion: MULTISS协议是一种安全高效的量子密钥分发网络长期存储协议。

Abstract: This paper presents MULTISS, a new protocol for long-term storage distributed
across multiple Quantum Key Distribution (QKD) networks. This protocol is an
extension of LINCOS, a secure storage protocol that uses Shamir secret sharing
for secret storage on a single QKD network. Our protocol uses hierarchical
secret sharing to distribute a secret across multiple QKD networks while
ensuring perfect security. Our protocol further allows for sharing updates
without having to reconstruct the entire secret. We also prove that MULTISS is
strictly more secure than LINCOS, which remains vulnerable when its QKD network
is compromised.

</details>


### [9] [On Automating Security Policies with Contemporary LLMs](https://arxiv.org/abs/2506.04838)
*Pablo Fernández Saura,K. R. Jayaram,Vatche Isahagian,Jorge Bernal Bernabé,Antonio Skarmeta*

Main category: cs.CR

TL;DR: 本文提出了一种利用LLMs和RAG的自动化攻击缓解策略合规性框架，显著提高了安全执行效果。


<details>
  <summary>Details</summary>
Motivation: 现代计算环境复杂性和网络威胁日益复杂化，需要更强大、更适应性和自动化的安全执行方法。

Method: 提出一个利用大型语言模型（LLMs）的框架，通过创新性地结合情境学习和检索增强生成（RAG）来自动化攻击缓解策略的合规性。

Result: 使用公开可用的CTI策略和Windows API文档进行实证评估，与未使用RAG的基线相比，采用RAG在精确度、召回率和F1分数方面表现出显著提高。

Conclusion: 该框架有效地提高了攻击缓解策略的自动化合规性，为网络安全提供了新的解决方案。

Abstract: The complexity of modern computing environments and the growing
sophistication of cyber threats necessitate a more robust, adaptive, and
automated approach to security enforcement. In this paper, we present a
framework leveraging large language models (LLMs) for automating attack
mitigation policy compliance through an innovative combination of in-context
learning and retrieval-augmented generation (RAG). We begin by describing how
our system collects and manages both tool and API specifications, storing them
in a vector database to enable efficient retrieval of relevant information. We
then detail the architectural pipeline that first decomposes high-level
mitigation policies into discrete tasks and subsequently translates each task
into a set of actionable API calls. Our empirical evaluation, conducted using
publicly available CTI policies in STIXv2 format and Windows API documentation,
demonstrates significant improvements in precision, recall, and F1-score when
employing RAG compared to a non-RAG baseline.

</details>


### [10] [A Private Smart Wallet with Probabilistic Compliance](https://arxiv.org/abs/2506.04853)
*Andrea Rizzini,Marco Esposito,Francesco Bruschi,Donatella Sciuto*

Main category: cs.CR

TL;DR: 提出一种基于邀请的隐私保护智能钱包，实现高效、低成本的数字支付。


<details>
  <summary>Details</summary>
Motivation: 提出一种基于邀请的隐私保护智能钱包，并使用新颖的私人注册机制。

Method: 结合使用无辜证明机制和祖先承诺跟踪系统，使用Bloom过滤器进行概率UTXO链状态跟踪。

Result: 性能分析表明，具有合规性检查的私人转账在消费级笔记本电脑上仅需几秒钟即可完成，且整体上证明生成保持较低水平。链上成本保持最低，确保Base层2网络上所有操作的可行性。钱包通过加密数据块促进私人联系列表管理，同时保持交易不可链接性。评估验证了该方法的可行性，用于具有最小计算和财务开销的隐私保护和合规性意识数字支付。

Conclusion: 该方法在隐私保护和合规性意识数字支付方面具有可行性，且计算和财务开销最小化。

Abstract: We propose a privacy-preserving smart wallet with a novel invitation-based
private onboarding mechanism. The solution integrates two levels of compliance
in concert with an authority party: a proof of innocence mechanism and an
ancestral commitment tracking system using bloom filters for probabilistic UTXO
chain states. Performance analysis demonstrates practical efficiency: private
transfers with compliance checks complete within seconds on a consumer-grade
laptop, and overall with proof generation remaining low. On-chain costs stay
minimal, ensuring affordability for all operations on Base layer 2 network. The
wallet facilitates private contact list management through encrypted data blobs
while maintaining transaction unlinkability. Our evaluation validates the
approach's viability for privacy-preserving, compliance-aware digital payments
with minimized computational and financial overhead.

</details>


### [11] [PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm Packages](https://arxiv.org/abs/2506.04962)
*Deniz Simsek,Aryaz Eghbali,Michael Pradel*

Main category: cs.CR

TL;DR: PoCGen方法利用LLMs自动生成和验证PoC漏洞利用代码，提高了漏洞修复的效率。


<details>
  <summary>Details</summary>
Motivation: 软件包中的安全漏洞是开发者和用户共同关注的问题，及时修复这些漏洞对于恢复软件系统的完整性和安全性至关重要。然而，以往的研究表明，漏洞报告通常缺乏证明概念（PoC）漏洞利用代码，这对于修复漏洞、测试补丁和避免回归至关重要。创建PoC漏洞利用代码具有挑战性，因为漏洞报告通常是非正式的，往往不完整，并且需要详细了解输入如何传递到可能受影响的API，以及它们如何达到安全相关的接收器。

Method: 提出了一种名为PoCGen的新方法，该方法利用大型语言模型（LLMs）结合静态和动态分析技术来自动生成和验证npm包漏洞的PoC漏洞利用代码。PoCGen利用LLM来理解漏洞报告、生成候选PoC漏洞利用代码以及验证和改进它们。

Result: 在SecBench.js数据集中成功生成77%的漏洞利用代码，在包含794个近期漏洞的新数据集中成功生成39%的漏洞利用代码。这种方法的成功率显著优于最近的一个基线（高出45个百分点），并且生成每个漏洞利用代码的平均成本为0.02美元。

Conclusion: PoCGen方法有效地解决了生成PoC漏洞利用代码的挑战，为软件漏洞的修复提供了新的解决方案。

Abstract: Security vulnerabilities in software packages are a significant concern for
developers and users alike. Patching these vulnerabilities in a timely manner
is crucial to restoring the integrity and security of software systems.
However, previous work has shown that vulnerability reports often lack
proof-of-concept (PoC) exploits, which are essential for fixing the
vulnerability, testing patches, and avoiding regressions. Creating a PoC
exploit is challenging because vulnerability reports are informal and often
incomplete, and because it requires a detailed understanding of how inputs
passed to potentially vulnerable APIs may reach security-relevant sinks. In
this paper, we present PoCGen, a novel approach to autonomously generate and
validate PoC exploits for vulnerabilities in npm packages. This is the first
fully autonomous approach to use large language models (LLMs) in tandem with
static and dynamic analysis techniques for PoC exploit generation. PoCGen
leverages an LLM for understanding vulnerability reports, for generating
candidate PoC exploits, and for validating and refining them. Our approach
successfully generates exploits for 77% of the vulnerabilities in the
SecBench.js dataset and 39% in a new, more challenging dataset of 794 recent
vulnerabilities. This success rate significantly outperforms a recent baseline
(by 45 absolute percentage points), while imposing an average cost of $0.02 per
generated exploit.

</details>


### [12] [Hiding in Plain Sight: Query Obfuscation via Random Multilingual Searches](https://arxiv.org/abs/2506.04963)
*Anton Firc,Jan Klusáček,Kamil Malinka*

Main category: cs.CR

TL;DR: 本研究提出了一种基于随机多语言查询的查询混淆策略，有效保护用户隐私


<details>
  <summary>Details</summary>
Motivation: 为了减少搜索结果个性化带来的隐私风险和过滤泡沫问题

Method: 提出并评估了一种使用随机生成的多语言搜索查询的轻量级客户端查询混淆策略

Result: 实验表明，这种方法可以防止搜索引擎准确地进行用户画像，并覆盖现有的用户画像

Conclusion: 查询混淆是一种可行的隐私保护机制，可以帮助用户在不修改现有基础设施的情况下自主保护其搜索行为

Abstract: Modern search engines extensively personalize results by building detailed
user profiles based on query history and behaviour. While personalization can
enhance relevance, it introduces privacy risks and can lead to filter bubbles.
This paper proposes and evaluates a lightweight, client-side query obfuscation
strategy using randomly generated multilingual search queries to disrupt user
profiling. Through controlled experiments on the Seznam.cz search engine, we
assess the impact of interleaving real queries with obfuscating noise in
various language configurations and ratios. Our findings show that while
displayed search results remain largely stable, the search engine's identified
user interests shift significantly under obfuscation. We further demonstrate
that such random queries can prevent accurate profiling and overwrite
established user profiles. This study provides practical evidence for query
obfuscation as a viable privacy-preserving mechanism and introduces a tool that
enables users to autonomously protect their search behaviour without modifying
existing infrastructure.

</details>


### [13] [Evaluating the Impact of Privacy-Preserving Federated Learning on CAN Intrusion Detection](https://arxiv.org/abs/2506.04978)
*Gabriele Digregorio,Elisabetta Cainazzo,Stefano Longari,Michele Carminati,Stefano Zanero*

Main category: cs.CR

TL;DR: 本文提出了一种基于联邦学习的车载入侵检测系统，并证明了其可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习的数据密集特性，结合V2X等新型范式和5G通信的潜力，使得联邦学习（FL）在车载入侵检测领域得到应用和证明。

Method: 在车载网络入侵检测过程中集成FL策略，提出基于LSTM自动编码器的FL实现，并对检测效率和通信开销进行评估。

Result: 评估结果表明，该方案是一种可行的解决方案。

Conclusion: 联邦学习在车载入侵检测领域具有可行性和潜力。

Abstract: The challenges derived from the data-intensive nature of machine learning in
conjunction with technologies that enable novel paradigms such as V2X and the
potential offered by 5G communication, allow and justify the deployment of
Federated Learning (FL) solutions in the vehicular intrusion detection domain.
In this paper, we investigate the effects of integrating FL strategies into the
machine learning-based intrusion detection process for on-board vehicular
networks. Accordingly, we propose a FL implementation of a state-of-the-art
Intrusion Detection System (IDS) for Controller Area Network (CAN), based on
LSTM autoencoders. We thoroughly evaluate its detection efficiency and
communication overhead, comparing it to a centralized version of the same
algorithm, thereby presenting it as a feasible solution.

</details>


### [14] [Attack Effect Model based Malicious Behavior Detection](https://arxiv.org/abs/2506.05001)
*Limin Wang,Lei Bu,Muzimiao Zhang,Shihong Cang,Kai Ye*

Main category: cs.CR

TL;DR: FEAD框架通过创新的方法显著提高了安全检测的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的安全检测方法面临三个主要挑战：数据收集不足，监测系统资源密集，以及检测算法的误报率较高。

Method: 提出了一种名为FEAD（Focus-Enhanced Attack Detection）的框架，通过三个创新来解决这些问题：1）攻击模型驱动的方法，从在线攻击报告中提取关键安全监测项，实现全面覆盖；2）高效的任务分解，将监测最优地分配到现有的收集器中，以最小化开销；3）本地感知异常分析，利用来源图中恶意活动的聚类行为来提高检测精度。

Result: 评估结果表明，与现有解决方案相比，FEAD实现了8.23%的F1分数提升，仅增加了5.4%的开销，证实了基于关注的设计显著提高了检测性能。

Conclusion: FEAD框架通过创新的方法显著提高了安全检测的性能。

Abstract: Traditional security detection methods face three key challenges: inadequate
data collection that misses critical security events, resource-intensive
monitoring systems, and poor detection algorithms with high false positive
rates. We present FEAD (Focus-Enhanced Attack Detection), a framework that
addresses these issues through three innovations: (1) an attack model-driven
approach that extracts security-critical monitoring items from online attack
reports for comprehensive coverage; (2) efficient task decomposition that
optimally distributes monitoring across existing collectors to minimize
overhead; and (3) locality-aware anomaly analysis that leverages the clustering
behavior of malicious activities in provenance graphs to improve detection
accuracy. Evaluations demonstrate FEAD achieves 8.23% higher F1-score than
existing solutions with only 5.4% overhead, confirming that focus-based designs
significantly enhance detection performance.

</details>


### [15] [EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers](https://arxiv.org/abs/2506.05074)
*Robert J. Joyce,Gideon Miller,Phil Roth,Richard Zak,Elliott Zaresky-Williams,Hyrum Anderson,Edward Raff,James Holt*

Main category: cs.CR

TL;DR: 本研究提出的新数据集EMBER2024，为恶意软件分析研究提供了全面的资源，推动了研究的进步和可重复性。


<details>
  <summary>Details</summary>
Motivation: 历史上一贯缺乏可访问的数据限制了恶意软件分析研究，从业者过分依赖行业来源提供的数据集来推进。现有公共数据集受限于范围狭窄，大多数仅包括针对单个平台的文件，标签只支持一种恶意软件分类任务，并且没有努力捕捉使恶意软件检测在实践变得困难的逃避文件。

Method: 我们提出了一个新的数据集EMBER2024，该数据集包含超过320万文件的哈希、元数据、特征向量和标签，支持在七种恶意软件分类任务上进行机器学习模型的训练和评估，包括恶意软件检测、恶意软件家族分类和恶意软件行为识别。此外，还引入了EMBER特征版本3，增加了对几种新特征类型的支持。

Result: EMBER2024是第一个包括一组最初被一组防病毒产品未检测到的恶意文件的恶意软件，创建了一个“挑战”集来评估分类器对逃避恶意软件的性能。这项工作还发布了EMBER2024数据集，以促进可重复性和赋能研究人员追求新的恶意软件研究课题。

Conclusion: 本研究提出的新数据集EMBER2024能够全面评估恶意软件分类器，有助于推动恶意软件分析的进步和研究的可重复性。

Abstract: A lack of accessible data has historically restricted malware analysis
research, and practitioners have relied heavily on datasets provided by
industry sources to advance. Existing public datasets are limited by narrow
scope - most include files targeting a single platform, have labels supporting
just one type of malware classification task, and make no effort to capture the
evasive files that make malware detection difficult in practice. We present
EMBER2024, a new dataset that enables holistic evaluation of malware
classifiers. Created in collaboration with the authors of EMBER2017 and
EMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,
and labels for more than 3.2 million files from six file formats. Our dataset
supports the training and evaluation of machine learning models on seven
malware classification tasks, including malware detection, malware family
classification, and malware behavior identification. EMBER2024 is the first to
include a collection of malicious files that initially went undetected by a set
of antivirus products, creating a "challenge" set to assess classifier
performance against evasive malware. This work also introduces EMBER feature
version 3, with added support for several new feature types. We are releasing
the EMBER2024 dataset to promote reproducibility and empower researchers in the
pursuit of new malware research topics.

</details>


### [16] [Membership Inference Attacks on Sequence Models](https://arxiv.org/abs/2506.05126)
*Lorenzo Rossi,Michael Aerni,Jie Zhang,Florian Tramèr*

Main category: cs.CR

TL;DR: 该研究提出了一种基于序列内相关性的隐私泄露审计方法，以应对序列模型泄露敏感信息的风险，并通过案例研究证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 序列模型，如大型语言模型（LLMs）和自回归图像生成器，倾向于记住并无意中泄露敏感信息。虽然这种倾向具有重大的法律意义，但现有的工具不足以审计由此产生的风险。

Method: 我们将最先进的成员身份推断攻击适应到序列中，以显式地建模序列内相关性，从而表明一个强大的现有攻击如何自然地扩展以适应序列模型的结构。

Result: 我们的方法在案例研究中表明，与引入额外的计算成本相比，我们的调整始终提高了记忆审计的有效性。

Conclusion: 我们的工作为大型序列模型的可靠记忆审计提供了一个重要的基石。

Abstract: Sequence models, such as Large Language Models (LLMs) and autoregressive
image generators, have a tendency to memorize and inadvertently leak sensitive
information. While this tendency has critical legal implications, existing
tools are insufficient to audit the resulting risks. We hypothesize that those
tools' shortcomings are due to mismatched assumptions. Thus, we argue that
effectively measuring privacy leakage in sequence models requires leveraging
the correlations inherent in sequential generation. To illustrate this, we
adapt a state-of-the-art membership inference attack to explicitly model
within-sequence correlations, thereby demonstrating how a strong existing
attack can be naturally extended to suit the structure of sequence models.
Through a case study, we show that our adaptations consistently improve the
effectiveness of memorization audits without introducing additional
computational costs. Our work hence serves as an important stepping stone
toward reliable memorization audits for large sequence models.

</details>


### [17] [OpenCCA: An Open Framework to Enable Arm CCA Research](https://arxiv.org/abs/2506.05129)
*Andrin Bertschi,Shweta Shinde*

Main category: cs.CR

TL;DR: 提出OpenCCA，一个开源平台，允许在普通Armv8.2硬件上执行CCA-bound代码，降低研究门槛，提高性能评估和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决在研究Arm CCA时缺乏硬件支持的问题，导致研究重复、性能比较不一致和入门门槛高。

Method: 提出OpenCCA，一个开源研究平台，允许在商品Armv8.2硬件上执行CCA-bound代码。通过系统性地调整软件堆栈（包括引导加载程序、固件、虚拟机管理程序和内核），OpenCCA模拟CCA操作以进行性能评估，同时保持功能正确性。

Result: 在250美元的Armv8.2 Rockchip板上进行典型生命周期测量和案例研究，证明了其有效性。

Conclusion: OpenCCA是一个有效的开源研究平台，可以降低研究Arm CCA的门槛，并促进性能评估和功能正确性的保持。

Abstract: Confidential computing has gained traction across major architectures with
Intel TDX, AMD SEV-SNP, and Arm CCA. Unlike TDX and SEV-SNP, a key challenge in
researching Arm CCA is the absence of hardware support, forcing researchers to
develop ad-hoc performance prototypes on non-CCA Arm boards. This approach
leads to duplicated efforts, inconsistent performance comparisons, and high
barriers to entry. To address this, we present OpenCCA, an open research
platform that enables the execution of CCA-bound code on commodity Armv8.2
hardware. By systematically adapting the software stack -- including
bootloader, firmware, hypervisor, and kernel -- OpenCCA emulates CCA operations
for performance evaluation while preserving functional correctness. We
demonstrate its effectiveness with typical life-cycle measurements and
case-studies inspired by prior CCA-based papers on a easily available Armv8.2
Rockchip board that costs $250.

</details>


### [18] [SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid Neuron Encryption](https://arxiv.org/abs/2506.05242)
*Zhiqiang Wang,Haohua Du,Junyang Wang,Haifeng Sun,Kaiwen Guo,Haikuo Yu,Chao Liu,Xiang-Yang Li*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) with diverse capabilities are increasingly being
deployed in local environments, presenting significant security and
controllability challenges. These locally deployed LLMs operate outside the
direct control of developers, rendering them more susceptible to abuse.
Existing mitigation techniques mainly designed for cloud-based LLM services are
frequently circumvented or ineffective in deployer-controlled environments. We
propose SECNEURON, the first framework that seamlessly embeds classic access
control within the intrinsic capabilities of LLMs, achieving reliable,
cost-effective, flexible, and certified abuse control for local deployed LLMs.
SECNEURON employs neuron-level encryption and selective decryption to
dynamically control the task-specific capabilities of LLMs, limiting
unauthorized task abuse without compromising others. We first design a
task-specific neuron extraction mechanism to decouple logically related neurons
and construct a layered policy tree for handling coupled neurons. We then
introduce a flexible and efficient hybrid encryption framework for millions of
neurons in LLMs. Finally, we developed a distribution-based decrypted neuron
detection mechanism on ciphertext to ensure the effectiveness of partially
decrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and
Collusion Resistance Security under the Task Controllability Principle.
Experiments on various task settings show that SECNEURON limits unauthorized
task accuracy to below 25% while keeping authorized accuracy loss with 2%.
Using an unauthorized Code task example, the accuracy of abuse-related
malicious code generation was reduced from 59% to 15%. SECNEURON also mitigates
unauthorized data leakage, reducing PII extraction rates to below 5% and
membership inference to random guesses.

</details>


### [19] [Big Bird: Privacy Budget Management for W3C's Privacy-Preserving Attribution API](https://arxiv.org/abs/2506.05290)
*Pierre Tholoniat,Alison Caulfield,Giorgio Cavicchioli,Mark Chen,Nikos Goutzoulias,Benjamin Case,Asaf Cidon,Roxana Geambasu,Mathias Lécuyer,Martin Thomson*

Main category: cs.CR

TL;DR: Big Bird：一种PPA隐私预算管理器，通过资源隔离原则和批量调度算法提高隐私保护和广告测量效率。


<details>
  <summary>Details</summary>
Motivation: 为了在保护网络隐私的同时实现有效的广告测量，设计了像隐私保护归因（PPA）这样的隐私保护广告API。

Method: 提出了Big Bird，这是一种PPA的隐私预算管理器，它明确了每个网站的预算语义，并引入了一个基于资源隔离原则的全局预算系统。

Result: Big Bird通过配额预算强制执行效用保留限制，并通过一种新颖的批量调度算法提高了全局预算利用率。这些机制共同为在对抗环境中实施隐私保护提供了坚实的基础。在Firefox中实现了Big Bird，并在实际广告数据上进行了评估，证明了其弹性和有效性。

Conclusion: Big Bird为PPA提供了一种有效的隐私预算管理方法，在保护隐私的同时提高了广告测量的准确性。

Abstract: Privacy-preserving advertising APIs like Privacy-Preserving Attribution (PPA)
are designed to enhance web privacy while enabling effective ad measurement.
PPA offers an alternative to cross-site tracking with encrypted reports
governed by differential privacy (DP), but current designs lack a principled
approach to privacy budget management, creating uncertainty around critical
design decisions. We present Big Bird, a privacy budget manager for PPA that
clarifies per-site budget semantics and introduces a global budgeting system
grounded in resource isolation principles. Big Bird enforces utility-preserving
limits via quota budgets and improves global budget utilization through a novel
batched scheduling algorithm. Together, these mechanisms establish a robust
foundation for enforcing privacy protections in adversarial environments. We
implement Big Bird in Firefox and evaluate it on real-world ad data,
demonstrating its resilience and effectiveness.

</details>


### [20] [Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets](https://arxiv.org/abs/2506.05346)
*Lei Hsiung,Tianyu Pang,Yung-Chen Tang,Linyue Song,Tsung-Yi Ho,Pin-Yu Chen,Yaoqing Yang*

Main category: cs.CR

TL;DR: 研究表明，上游数据集设计对安全防护措施的耐用性和对越狱攻击的防御至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）在下游微调中易受安全校准越狱的影响，而现有的缓解策略主要关注在安全防护措施被破坏后的反应性应对、在微调期间移除有害梯度或在微调期间持续强化安全校准。因此，它们往往忽视了上游的一个重要因素：原始安全校准数据的作用。

Method: 通过研究上游对齐数据集和下游微调任务之间的表示相似性来研究安全防护措施的退化。

Result: 实验表明，这些数据集之间的高度相似性会显著削弱安全防护措施，使模型更容易受到越狱的影响。相反，这两种类型的数据集之间的低相似性会产生更加鲁棒的模型，从而将有害性评分降低多达10.33%。

Conclusion: 强调了上游数据集设计在构建耐用的安全防护措施中的重要性，并减少了现实世界中受到越狱攻击的脆弱性。

Abstract: Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [21] [Tech-ASan: Two-stage check for Address Sanitizer](https://arxiv.org/abs/2506.05022)
*Yixuan Cao,Yuhong Feng,Huafeng Li,Chongyi Huang,Fangcao Jian,Haoran Li,Xu Wang*

Main category: cs.SE

TL;DR: Tech-ASan通过两阶段检查技术，显著降低了ASan的运行时开销，同时保证了检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决Address Sanitizer (ASan)在检测内存安全问题时存在的性能开销问题，本文提出了一种名为Tech-ASan的技术。

Method: Tech-ASan采用两阶段检查技术，包括：1. 提出一种新的两阶段检查算法，利用魔术值比较减少昂贵的影子内存访问；2. 设计一个高效的优化器，消除冗余检查，包括一种新的循环检查移除算法；3. 将Tech-ASan作为基于LLVM编译器基础设施的内存安全工具实现。

Result: 评估结果显示，Tech-ASan在SPEC CPU2006基准测试中，相较于ASan和ASan--，运行时开销分别减少了33.70%和17.89%。在Juliet测试套件测试中，Tech-ASan比ASan和ASan--检测出56个更少的假阴性案例。

Conclusion: Tech-ASan是一种有效加速ASan且保证安全性的技术，在内存安全检测领域具有较好的应用前景。

Abstract: Address Sanitizer (ASan) is a sharp weapon for detecting memory safety
violations, including temporal and spatial errors hidden in C/C++ programs
during execution. However, ASan incurs significant runtime overhead, which
limits its efficiency in testing large software. The overhead mainly comes from
sanitizer checks due to the frequent and expensive shadow memory access. Over
the past decade, many methods have been developed to speed up ASan by
eliminating and accelerating sanitizer checks, however, they either fail to
adequately eliminate redundant checks or compromise detection capabilities. To
address this issue, this paper presents Tech-ASan, a two-stage check based
technique to accelerate ASan with safety assurance. First, we propose a novel
two-stage check algorithm for ASan, which leverages magic value comparison to
reduce most of the costly shadow memory accesses. Second, we design an
efficient optimizer to eliminate redundant checks, which integrates a novel
algorithm for removing checks in loops. Third, we implement Tech-ASan as a
memory safety tool based on the LLVM compiler infrastructure. Our evaluation
using the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the
state-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan
and ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative
cases than ASan and ASan-- when testing on the Juliet Test Suite under the same
redzone setting.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
*Apurv Verma,NhatHai Phan,Shubhendu Trivedi*

Main category: cs.CL

TL;DR: 本研究分析了水印技术对大型语言模型的影响，并提出了一个有效的采样方法来恢复对齐。


<details>
  <summary>Details</summary>
Motivation: 研究水印技术对大型语言模型输出质量的影响，特别是对真实性、安全性和有帮助性的影响。

Method: 对两种流行的水印方法（Gumbel和KGW）进行了系统分析，并在四个对齐的LLMs上进行了实验。

Result: 揭示了两种不同的退化模式：保护衰减和保护放大。提出了一种名为Alignment Resampling（AR）的采样方法，用于恢复对齐。实验结果表明，仅对2-4个水印生成进行采样就可以有效地恢复或超过基线（未水印）的对齐分数。

Conclusion: 这项工作揭示了水印强度和模型对齐之间的关键平衡，为在实际中负责地部署水印LLMs提供了一种简单的推理时间解决方案。

Abstract: Watermarking techniques for large language models (LLMs) can significantly
impact output quality, yet their effects on truthfulness, safety, and
helpfulness remain critically underexamined. This paper presents a systematic
analysis of how two popular watermarking approaches-Gumbel and KGW-affect these
core alignment properties across four aligned LLMs. Our experiments reveal two
distinct degradation patterns: guard attenuation, where enhanced helpfulness
undermines model safety, and guard amplification, where excessive caution
reduces model helpfulness. These patterns emerge from watermark-induced shifts
in token distribution, surfacing the fundamental tension that exists between
alignment objectives.
  To mitigate these degradations, we propose Alignment Resampling (AR), an
inference-time sampling method that uses an external reward model to restore
alignment. We establish a theoretical lower bound on the improvement in
expected reward score as the sample size is increased and empirically
demonstrate that sampling just 2-4 watermarked generations effectively recovers
or surpasses baseline (unwatermarked) alignment scores. To overcome the limited
response diversity of standard Gumbel watermarking, our modified implementation
sacrifices strict distortion-freeness while maintaining robust detectability,
ensuring compatibility with AR. Experimental results confirm that AR
successfully recovers baseline alignment in both watermarking approaches, while
maintaining strong watermark detectability. This work reveals the critical
balance between watermark strength and model alignment, providing a simple
inference-time solution to responsibly deploy watermarked LLMs in practice.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [23] [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)
*Zhanhui Zhou,Lingjie Chen,Chao Yang,Chaochao Lu*

Main category: cs.CV

TL;DR: 研究人员发现，视觉-语言模型可以通过视觉拼接能力整合分散的视觉信息，从而绕过数据审查，对VLM的安全构成严重风险。


<details>
  <summary>Details</summary>
Motivation: 为了减轻视觉-语言模型（VLMs）的风险，研究人员尝试去除训练数据中的危险样本。

Method: 研究人员通过实验证明了视觉拼接能力，即在多个具有相同文本描述的训练样本中整合分散的视觉信息的能力。他们使用具有唯一合成ID的图像数据集，将每个图像和ID对分割成不同粒度的图像块和ID对，用于微调模型。

Result: 实验结果表明，经过微调的模型能够从完整的图像或文本参考中正确地描述正确的ID。此外，研究人员通过使用危险图像的块并替换ID为文本描述，如“安全”或“不安全”，模拟了上述对抗性数据中毒场景，展示了有害内容如何通过视觉拼接规避审查并在后续被重建，对VLM的安全构成严重风险。

Conclusion: 这项研究强调了视觉拼接在VLM安全中的重要性，并提出了对抗性数据中毒的场景。

Abstract: One way to mitigate risks in vision-language models (VLMs) is to remove
dangerous samples in their training data. However, such data moderation can be
easily bypassed when harmful images are split into small, benign-looking
patches, scattered across many training samples. VLMs may then learn to piece
these fragments together during training and generate harmful responses at
inference, either from full images or text references. For instance, if trained
on image patches from a bloody scene paired with the descriptions "safe," VLMs
may later describe, the full image or a text reference to the scene, as "safe."
We define the core ability of VLMs enabling this attack as $\textit{visual
stitching}$ -- the ability to integrate visual information spread across
multiple training samples that share the same textual descriptions. In our
work, we first demonstrate visual stitching abilities in common open-source
VLMs on three datasets where each image is labeled with a unique synthetic ID:
we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch},
\texttt{ID})\}$ pairs at different granularity for finetuning, and we find that
tuned models can verbalize the correct IDs from full images or text reference.
Building on this, we simulate the adversarial data poisoning scenario mentioned
above by using patches from dangerous images and replacing IDs with text
descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can
evade moderation in patches and later be reconstructed through visual
stitching, posing serious VLM safety risks. Code is available at
https://github.com/ZHZisZZ/visual-stitching.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Urania: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)
*Daogao Liu,Edith Cohen,Badih Ghazi,Peter Kairouz,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Adam Sealfon,Da Yu,Chiyuan Zhang*

Main category: cs.LG

TL;DR: Urania：一个在LLM聊天机器人交互中提供严格隐私保护的框架。


<details>
  <summary>Details</summary>
Motivation: 为了在LLM聊天机器人交互中提供严格的数据隐私保护，我们介绍了Urania框架。

Method: Urania框架采用私有聚类机制和创新的关键词提取方法，包括基于频率、TF-IDF和LLM引导的方法。利用DP工具如聚类、分区选择和基于直方图的摘要，提供端到端隐私保护。

Result: 评估了词汇和语义内容的保留、成对相似性和基于LLM的指标，与一个非私有的Clio启发式管道进行了基准测试。结果表明，该框架能够提取有意义的对话洞察，同时保持严格的用户隐私，有效地平衡数据效用与隐私保护。

Conclusion: Urania框架在保护用户隐私的同时，有效地提取了有意义的对话洞察，为LLM聊天机器人交互提供了有效的隐私保护方法。

Abstract: We introduce $Urania$, a novel framework for generating insights about LLM
chatbot interactions with rigorous differential privacy (DP) guarantees. The
framework employs a private clustering mechanism and innovative keyword
extraction methods, including frequency-based, TF-IDF-based, and LLM-guided
approaches. By leveraging DP tools such as clustering, partition selection, and
histogram-based summarization, $Urania$ provides end-to-end privacy protection.
Our evaluation assesses lexical and semantic content preservation, pair
similarity, and LLM-based metrics, benchmarking against a non-private
Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple
empirical privacy evaluation that demonstrates the enhanced robustness of our
DP pipeline. The results show the framework's ability to extract meaningful
conversational insights while maintaining stringent user privacy, effectively
balancing data utility with privacy preservation.

</details>


### [25] [Identifying and Understanding Cross-Class Features in Adversarial Training](https://arxiv.org/abs/2506.05032)
*Zeming Wei,Yiwen Guo,Yisen Wang*

Main category: cs.LG

TL;DR: 本文通过类别特征归因研究对抗训练机制，发现跨类别特征对AT有重要影响，并揭示了AT的一些新特性。


<details>
  <summary>Details</summary>
Motivation: 为了研究对抗训练（AT）的机制，特别是其训练机制和动态，作者从类别特征归因的角度提出了一种新的研究视角。

Method: 作者通过合成数据模型和系统研究，确定了共享于多个类别的关键特征族对AT的影响，并将其称为跨类别特征。

Result: 研究发现，在AT的初始阶段，模型倾向于学习更多跨类别特征，直到最佳鲁棒性检查点。随着AT进一步压缩训练鲁棒损失并导致鲁棒过拟合，模型倾向于基于更多类别特定的特征做出决策。

Conclusion: 这些发现深化了对抗训练机制的当前理解，并为研究它们提供了新的视角。

Abstract: Adversarial training (AT) has been considered one of the most effective
methods for making deep neural networks robust against adversarial attacks,
while the training mechanisms and dynamics of AT remain open research problems.
In this paper, we present a novel perspective on studying AT through the lens
of class-wise feature attribution. Specifically, we identify the impact of a
key family of features on AT that are shared by multiple classes, which we call
cross-class features. These features are typically useful for robust
classification, which we offer theoretical evidence to illustrate through a
synthetic data model. Through systematic studies across multiple model
architectures and settings, we find that during the initial stage of AT, the
model tends to learn more cross-class features until the best robustness
checkpoint. As AT further squeezes the training robust loss and causes robust
overfitting, the model tends to make decisions based on more class-specific
features. Based on these discoveries, we further provide a unified view of two
existing properties of AT, including the advantage of soft-label training and
robust overfitting. Overall, these insights refine the current understanding of
AT mechanisms and provide new perspectives on studying them. Our code is
available at https://github.com/PKU-ML/Cross-Class-Features-AT.

</details>


### [26] [Privacy Amplification Through Synthetic Data: Insights from Linear Regression](https://arxiv.org/abs/2506.05101)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Synthetic data inherits the differential privacy guarantees of the model used
to generate it. Additionally, synthetic data may benefit from privacy
amplification when the generative model is kept hidden. While empirical studies
suggest this phenomenon, a rigorous theoretical understanding is still lacking.
In this paper, we investigate this question through the well-understood
framework of linear regression. First, we establish negative results showing
that if an adversary controls the seed of the generative model, a single
synthetic data point can leak as much information as releasing the model
itself. Conversely, we show that when synthetic data is generated from random
inputs, releasing a limited number of synthetic data points amplifies privacy
beyond the model's inherent guarantees. We believe our findings in linear
regression can serve as a foundation for deriving more general bounds in the
future.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [27] [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)
*Kai Wang,Yihao Zhang,Meng Sun*

Main category: cs.AI

TL;DR: 本研究提出了一种新的方法来检测和控制大型语言模型中的欺骗行为，为可信AI的对齐提供了新的工具。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）的诚实性问题，特别是具有思维链（CoT）推理的高级系统可能会战略性地欺骗人类。

Method: 使用表示工程，系统地诱导、检测和控制具有CoT的LLMs中的欺骗，通过线性人工断层扫描（LAT）提取“欺骗向量”，检测准确率为89%。通过激活引导，在不使用明确提示的情况下实现40%的成功率，激发与上下文相关的欺骗。

Result: 实现了对具有CoT的LLMs中欺骗的检测和控制，为可信AI的对齐提供了工具。

Conclusion: 该研究为大型语言模型的诚实性问题提供了新的解决方案，有助于提高AI的可靠性和安全性。

Abstract: The honesty of large language models (LLMs) is a critical alignment
challenge, especially as advanced systems with chain-of-thought (CoT) reasoning
may strategically deceive humans. Unlike traditional honesty issues on LLMs,
which could be possibly explained as some kind of hallucination, those models'
explicit thought paths enable us to study strategic deception--goal-driven,
intentional misinformation where reasoning contradicts outputs. Using
representation engineering, we systematically induce, detect, and control such
deception in CoT-enabled LLMs, extracting "deception vectors" via Linear
Artificial Tomography (LAT) for 89% detection accuracy. Through activation
steering, we achieve a 40% success rate in eliciting context-appropriate
deception without explicit prompts, unveiling the specific honesty-related
issue of reasoning models and providing tools for trustworthy AI alignment.

</details>
