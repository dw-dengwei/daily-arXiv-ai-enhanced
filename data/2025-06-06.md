<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 20]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Hello, won't you tell me your name?: Investigating Anonymity Abuse in IPFS](https://arxiv.org/abs/2506.04307)
*Christos Karapapas,Iakovos Pittaras,George C. Polyzos,Constantinos Patsakis*

Main category: cs.CR

TL;DR: 研究揭示了IPFS挂载服务和公共网关在限制恶意内容传播方面的不足。


<details>
  <summary>Details</summary>
Motivation: 探索恶意行为者如何利用IPFS上传和传播有害内容，同时保持匿名。

Method: 研究通过上传恶意文件，使用脚本系统地测试了这些服务的表现。

Result: 评估了挂载服务和公共网关在维护内容可用性方面的能力和局限性。

Conclusion: 分析表明，挂载服务和公共网关缺乏评估或限制恶意内容传播的机制。

Abstract: The InterPlanetary File System~(IPFS) offers a decentralized approach to file
storage and sharing, promising resilience and efficiency while also realizing
the Web3 paradigm. Simultaneously, the offered anonymity raises significant
questions about potential misuse. In this study, we explore methods that
malicious actors can exploit IPFS to upload and disseminate harmful content
while remaining anonymous. We evaluate the role of pinning services and public
gateways, identifying their capabilities and limitations in maintaining content
availability. Using scripts, we systematically test the behavior of these
services by uploading malicious files. Our analysis reveals that pinning
services and public gateways lack mechanisms to assess or restrict the
propagation of malicious content.

</details>


### [2] [The Hashed Fractal Key Recovery (HFKR) Problem: From Symbolic Path Inversion to Post-Quantum Cryptographic Keys](https://arxiv.org/abs/2506.04383)
*Mohamed Aly Bouke*

Main category: cs.CR

TL;DR: HFKR问题结合符号分形和哈希熵增强，为后量子安全提供了一种新的密钥生成方法。


<details>
  <summary>Details</summary>
Motivation: 应对经典密码系统对量子攻击和结构密码分析的脆弱性。

Method: HFKR基于符号路径逆问题，通过合同仿射映射生成符号轨迹，并使用哈希混淆将其压缩为固定长度的密钥。

Result: 实验结果表明，SHA3-512和SHAKE256有效地增强了符号发散，而BLAKE3表现出统计上均匀但较弱的扩散。

Conclusion: HFKR问题通过结合符号分形和基于哈希的熵增强，为后量子安全提供了轻量级、无结构的密钥生成基础。

Abstract: Classical cryptographic systems rely heavily on structured algebraic
problems, such as factorization, discrete logarithms, or lattice-based
assumptions, which are increasingly vulnerable to quantum attacks and
structural cryptanalysis. In response, this work introduces the Hashed Fractal
Key Recovery (HFKR) problem, a non-algebraic cryptographic construction
grounded in symbolic dynamics and chaotic perturbations. HFKR builds on the
Symbolic Path Inversion Problem (SPIP), leveraging symbolic trajectories
generated via contractive affine maps over $\mathbb{Z}^2$, and compressing them
into fixed-length cryptographic keys using hash-based obfuscation. A key
contribution of this paper is the empirical confirmation that these symbolic
paths exhibit fractal behavior, quantified via box counting dimension, path
geometry, and spatial density measures. The observed fractal dimension
increases with trajectory length and stabilizes near 1.06, indicating symbolic
self-similarity and space-filling complexity, both of which reinforce the
entropy foundation of the scheme. Experimental results across 250 perturbation
trials show that SHA3-512 and SHAKE256 amplify symbolic divergence effectively,
achieving mean Hamming distances near 255, ideal bit-flip rates, and negligible
entropy deviation. In contrast, BLAKE3 exhibits statistically uniform but
weaker diffusion. These findings confirm that HFKR post-quantum security arises
from the synergy between symbolic fractality and hash-based entropy
amplification. The resulting construction offers a lightweight, structure-free
foundation for secure key generation in adversarial settings without relying on
algebraic hardness assumptions.

</details>


### [3] [Through the Stealth Lens: Rethinking Attacks and Defenses in RAG](https://arxiv.org/abs/2506.04390)
*Sarthak Choudhary,Nils Palumbo,Ashish Hooda,Krishnamurthy Dj Dvijotham,Somesh Jha*

Main category: cs.CR

TL;DR: 研究提出了一种基于注意力的过滤算法，可识别并过滤被注入恶意信息的文档片段，提高了检测和防御恶意攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）系统易受到恶意攻击，攻击者可以通过注入恶意信息来控制生成结果。

Method: 提出了基于注意力模式的过滤算法，通过计算文档片段的标准化注意力分数来识别和过滤潜在的恶意文档片段。

Result: 该方法能有效地缓解现有攻击，提高准确性，相对于基线防御提高了20%的准确率。

Conclusion: 研究证明了注意力模式在检测和防御恶意攻击中的重要性，并提出了基于注意力的防御方法。

Abstract: Retrieval-augmented generation (RAG) systems are vulnerable to attacks that
inject poisoned passages into the retrieved set, even at low corruption rates.
We show that existing attacks are not designed to be stealthy, allowing
reliable detection and mitigation. We formalize stealth using a
distinguishability-based security game. If a few poisoned passages are designed
to control the response, they must differentiate themselves from benign ones,
inherently compromising stealth. This motivates the need for attackers to
rigorously analyze intermediate signals involved in
generation$\unicode{x2014}$such as attention patterns or next-token probability
distributions$\unicode{x2014}$to avoid easily detectable traces of
manipulation. Leveraging attention patterns, we propose a passage-level
score$\unicode{x2014}$the Normalized Passage Attention
Score$\unicode{x2014}$used by our Attention-Variance Filter algorithm to
identify and filter potentially poisoned passages. This method mitigates
existing attacks, improving accuracy by up to $\sim 20 \%$ over baseline
defenses. To probe the limits of attention-based defenses, we craft stealthier
adaptive attacks that obscure such traces, achieving up to $35 \%$ attack
success rate, and highlight the challenges in improving stealth.

</details>


### [4] [Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification](https://arxiv.org/abs/2506.04450)
*Payel Bhattacharjee,Fengwei Tian,Ravi Tandon,Joseph Lo,Heidi Hanson,Geoffrey Rubin,Nirav Merchant,John Gounley*

Main category: cs.CR

TL;DR: This study proposes a framework for fine-tuning LLMs with DP for multi-abnormality classification in radiology reports, achieving privacy-preserving classification performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to protect sensitive patient data and prevent data leakage while maintaining classification performance in radiology report text classification.

Method: The framework injects calibrated noise during fine-tuning using Differentially Private Low-Rank Adaptation (DP-LoRA) to mitigate privacy risks and maintain classification performance.

Result: The DP fine-tuned models achieved comparable weighted F1 scores under moderate privacy guarantees, with scores of 0.88 on MIMIC-CXR and 0.59 on CT-RATE.

Conclusion: Differentially private fine-tuning using LoRA enables effective and privacy-preserving multi-abnormality classification from radiology reports, addressing a key challenge in fine-tuning LLMs on sensitive medical data.

Abstract: Purpose: This study proposes a framework for fine-tuning large language
models (LLMs) with differential privacy (DP) to perform multi-abnormality
classification on radiology report text. By injecting calibrated noise during
fine-tuning, the framework seeks to mitigate the privacy risks associated with
sensitive patient data and protect against data leakage while maintaining
classification performance. Materials and Methods: We used 50,232 radiology
reports from the publicly available MIMIC-CXR chest radiography and CT-RATE
computed tomography datasets, collected between 2011 and 2019. Fine-tuning of
LLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels
from CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)
in high and moderate privacy regimes (across a range of privacy budgets =
{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1
score across three model architectures: BERT-medium, BERT-small, and
ALBERT-base. Statistical analyses compared model performance across different
privacy levels to quantify the privacy-utility trade-off. Results: We observe a
clear privacy-utility trade-off through our experiments on 2 different datasets
and 3 different models. Under moderate privacy guarantees the DP fine-tuned
models achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on
CT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.
Conclusion: Differentially private fine-tuning using LoRA enables effective and
privacy-preserving multi-abnormality classification from radiology reports,
addressing a key challenge in fine-tuning LLMs on sensitive medical data.

</details>


### [5] [BESA: Boosting Encoder Stealing Attack with Perturbation Recovery](https://arxiv.org/abs/2506.04556)
*Xuhao Ren,Haotian Liang,Yajie Wang,Chuan Zhang,Zehui Xiong,Liehuang Zhu*

Main category: cs.CR

TL;DR: BESA是一种提升编码器窃取攻击准确性的攻击方法。


<details>
  <summary>Details</summary>
Motivation: 为了克服基于扰动的防御机制，提升编码器窃取攻击的性能。

Method: BESA由扰动检测和扰动恢复两个模块组成，可结合规范编码器窃取攻击。

Result: BESA在多种数据集上的评估显示，其相对于现有编码器窃取攻击，在面对最先进的防御和多种防御组合时，可提高编码器准确率高达24.63%。

Conclusion: BESA通过提升编码器窃取攻击的准确性，有效地克服了基于扰动的防御机制。

Abstract: To boost the encoder stealing attack under the perturbation-based defense
that hinders the attack performance, we propose a boosting encoder stealing
attack with perturbation recovery named BESA. It aims to overcome
perturbation-based defenses. The core of BESA consists of two modules:
perturbation detection and perturbation recovery, which can be combined with
canonical encoder stealing attacks. The perturbation detection module utilizes
the feature vectors obtained from the target encoder to infer the defense
mechanism employed by the service provider. Once the defense mechanism is
detected, the perturbation recovery module leverages the well-designed
generative model to restore a clean feature vector from the perturbed one.
Through extensive evaluations based on various datasets, we demonstrate that
BESA significantly enhances the surrogate encoder accuracy of existing encoder
stealing attacks by up to 24.63\% when facing state-of-the-art defenses and
combinations of multiple defenses.

</details>


### [6] [Incentivizing Collaborative Breach Detection](https://arxiv.org/abs/2506.04634)
*Mridu Nanda,Michael K. Reiter*

Main category: cs.CR

TL;DR: 我们提出了一种通过交换监控帮助来提高网站漏洞检测能力的算法。


<details>
  <summary>Details</summary>
Motivation: 虽然以前的工作提出了网站监控其在其他网站上的honeywords的输入，但尚不清楚网站参与这种监控的动机。

Method: 我们提出了一个算法，使网站可以通过交换监控帮助来参与监控。

Result: 我们提出了一个算法，并对其进行了评估。该算法使网站能够通过交换监控帮助来参与监控，并提高了检测自身漏洞的能力。

Conclusion: 通过模型检查分析，我们证明了使用我们的算法，当网站增加为其他网站投入的监控努力时，它可以提高检测自身漏洞的能力。我们还量化了各种参数对检测有效性的影响及其对部署支持监控生态系统的系统的影响。最后，我们在一个真实的漏洞凭证数据集上评估了我们的算法，并提供了性能分析，证实了其可扩展性和实际可行性。

Abstract: Decoy passwords, or "honeywords," alert a site to its breach if they are ever
entered in a login attempt on that site. However, an attacker can identify a
user-chosen password from among the decoys, without risk of alerting the site
to its breach, by performing credential stuffing, i.e., entering the stolen
passwords at another site where the same user reused her password. Prior work
has thus proposed that sites monitor for the entry of their honeywords at other
sites. Unfortunately, it is not clear what incentives sites have to participate
in this monitoring. In this paper we propose and evaluate an algorithm by which
sites can exchange monitoring favors. Through a model-checking analysis, we
show that using our algorithm, a site improves its ability to detect its own
breach when it increases the monitoring effort it expends for other sites. We
additionally quantify the impacts of various parameters on detection
effectiveness and their implications for the deployment of a system to support
a monitoring ecosystem. Finally, we evaluate our algorithm on a real dataset of
breached credentials and provide a performance analysis that confirms its
scalability and practical viability.

</details>


### [7] [Authenticated Private Set Intersection: A Merkle Tree-Based Approach for Enhancing Data Integrity](https://arxiv.org/abs/2506.04647)
*Zixian Gong,Zhiyong Zheng,Zhe Hu,Kun Tian,Yi Zhang,Zhedanov Oleksiy,Fengxia Liu*

Main category: cs.CR

TL;DR: 本文提出了改进的认证PSI方案，提高了数据完整性。


<details>
  <summary>Details</summary>
Motivation: 现有的PSI协议容易受到数据完整性攻击。

Method: 通过整合Merkle树和先进的两方volePSI和多方mPSI协议。

Result: 两方认证PSI的通信复杂度为O(nλ+nlogn)，多方构造为O(nκ+nlogn)。

Conclusion: 我们提出了PSI中的数据完整性定义，并构建了两个认证PSI方案。

Abstract: Private Set Intersection (PSI) enables secure computation of set
intersections while preserving participant privacy, standard PSI existing
protocols remain vulnerable to data integrity attacks allowing malicious
participants to extract additional intersection information or mislead other
parties. In this paper, we propose the definition of data integrity in PSI and
construct two authenticated PSI schemes by integrating Merkle Trees with
state-of-the-art two-party volePSI and multi-party mPSI protocols. The
resulting two-party authenticated PSI achieves communication complexity
$\mathcal{O}(n \lambda+n \log n)$, aligning with the best-known unauthenticated
PSI schemes, while the multi-party construction is $\mathcal{O}(n \kappa+n \log
n)$ which introduces additional overhead due to Merkle tree inclusion proofs.
Due to the incorporation of integrity verification, our authenticated schemes
incur higher costs compared to state-of-the-art unauthenticated schemes. We
also provide efficient implementations of our protocols and discuss potential
improvements, including alternative authentication blocks.

</details>


### [8] [MULTISS: un protocole de stockage confidentiel {à} long terme sur plusieurs r{é}seaux QKD](https://arxiv.org/abs/2506.04800)
*Thomas Prévost,Olivier Alibart,Marc Kaplan,Anne Marin*

Main category: cs.CR

TL;DR: MULTISS 协议是一种更安全的分布式量子密钥分发网络存储协议。


<details>
  <summary>Details</summary>
Motivation: 提高存储在 QKD 网络上的安全性，解决 LINCOS 在网络被破坏时的弱点。

Method: 使用分层密钥共享在多个 QKD 网络上分布存储，并允许在不重建整个密钥的情况下共享更新。

Result: MULTISS 协议在安全性和灵活性方面优于 LINCOS。

Conclusion: MULTISS 是一个更安全的协议，它扩展了 LINCOS，允许在多个 QKD 网络上分布式存储，同时保持完美安全性。

Abstract: This paper presents MULTISS, a new protocol for long-term storage distributed
across multiple Quantum Key Distribution (QKD) networks. This protocol is an
extension of LINCOS, a secure storage protocol that uses Shamir secret sharing
for secret storage on a single QKD network. Our protocol uses hierarchical
secret sharing to distribute a secret across multiple QKD networks while
ensuring perfect security. Our protocol further allows for sharing updates
without having to reconstruct the entire secret. We also prove that MULTISS is
strictly more secure than LINCOS, which remains vulnerable when its QKD network
is compromised.

</details>


### [9] [On Automating Security Policies with Contemporary LLMs](https://arxiv.org/abs/2506.04838)
*Pablo Fernández Saura,K. R. Jayaram,Vatche Isahagian,Jorge Bernal Bernabé,Antonio Skarmeta*

Main category: cs.CR

TL;DR: 提出利用大语言模型（LLMs）的框架，通过上下文学习和检索增强生成（RAG）自动化攻击缓解策略合规性。


<details>
  <summary>Details</summary>
Motivation: 现代计算环境和网络威胁的复杂性要求更强大、适应性和自动化的安全执行方法。

Method: 使用LLMs，结合上下文学习和RAG，自动执行攻击缓解策略。

Result: 使用RAG比非RAG基线在精确度、召回率和F1分数上有所提高。

Conclusion: None

Abstract: The complexity of modern computing environments and the growing
sophistication of cyber threats necessitate a more robust, adaptive, and
automated approach to security enforcement. In this paper, we present a
framework leveraging large language models (LLMs) for automating attack
mitigation policy compliance through an innovative combination of in-context
learning and retrieval-augmented generation (RAG). We begin by describing how
our system collects and manages both tool and API specifications, storing them
in a vector database to enable efficient retrieval of relevant information. We
then detail the architectural pipeline that first decomposes high-level
mitigation policies into discrete tasks and subsequently translates each task
into a set of actionable API calls. Our empirical evaluation, conducted using
publicly available CTI policies in STIXv2 format and Windows API documentation,
demonstrates significant improvements in precision, recall, and F1-score when
employing RAG compared to a non-RAG baseline.

</details>


### [10] [A Private Smart Wallet with Probabilistic Compliance](https://arxiv.org/abs/2506.04853)
*Andrea Rizzini,Marco Esposito,Francesco Bruschi,Donatella Sciuto*

Main category: cs.CR

TL;DR: 提出了一种隐私保护智能钱包，以实现隐私保护和合规性数字支付。


<details>
  <summary>Details</summary>
Motivation: 为了解决数字支付中的隐私保护和合规性问题。

Method: 该解决方案整合了两个级别的合规性，包括无辜证明机制和利用布隆过滤器进行概率性UTXO链状态跟踪的祖先承诺跟踪系统。

Result: 性能分析显示，合规性检查的私有转账可以在几秒钟内完成，并且整体上生成证明的成本保持较低。

Conclusion: 该研究提出了一种隐私保护智能钱包，并通过性能分析验证了其在隐私保护和合规性数字支付中的可行性。

Abstract: We propose a privacy-preserving smart wallet with a novel invitation-based
private onboarding mechanism. The solution integrates two levels of compliance
in concert with an authority party: a proof of innocence mechanism and an
ancestral commitment tracking system using bloom filters for probabilistic UTXO
chain states. Performance analysis demonstrates practical efficiency: private
transfers with compliance checks complete within seconds on a consumer-grade
laptop, and overall with proof generation remaining low. On-chain costs stay
minimal, ensuring affordability for all operations on Base layer 2 network. The
wallet facilitates private contact list management through encrypted data blobs
while maintaining transaction unlinkability. Our evaluation validates the
approach's viability for privacy-preserving, compliance-aware digital payments
with minimized computational and financial overhead.

</details>


### [11] [PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm Packages](https://arxiv.org/abs/2506.04962)
*Deniz Simsek,Aryaz Eghbali,Michael Pradel*

Main category: cs.CR

TL;DR: 本文提出了一种名为PoCGen的新方法，可以自动生成和验证npm包中漏洞的概念验证（PoC）利用。


<details>
  <summary>Details</summary>
Motivation: 软件包的安全漏洞对开发者和用户都是一个重大问题，及时修补这些漏洞对于恢复软件系统的完整性和安全性至关重要。然而，以前的工作表明，漏洞报告通常缺少概念验证（PoC）利用，这对于修复漏洞、测试补丁和避免回归至关重要。创建PoC利用具有挑战性，因为漏洞报告是非正式的，通常不完整，并且需要详细了解输入如何到达可能存在漏洞的API，以及它们如何到达安全相关的汇合点。

Method: 在本文中，我们提出了PoCGen，这是一种新的方法，可以自动生成和验证npm包中漏洞的概念验证（PoC）利用。这是第一个完全自动化的方法，结合使用大型语言模型（LLMs）和静态和动态分析技术来生成PoC利用。PoCGen利用LLM来理解漏洞报告，生成候选PoC利用，并验证和改进它们。

Result: 我们的方法成功生成了SecBench.js数据集中77%的漏洞的利用程序，以及794个新漏洞数据集中的39%的利用程序。这个成功率显著优于一个最近的基线（高出45个百分点），而生成每个利用程序的平均成本为0.02美元。

Conclusion: None

Abstract: Security vulnerabilities in software packages are a significant concern for
developers and users alike. Patching these vulnerabilities in a timely manner
is crucial to restoring the integrity and security of software systems.
However, previous work has shown that vulnerability reports often lack
proof-of-concept (PoC) exploits, which are essential for fixing the
vulnerability, testing patches, and avoiding regressions. Creating a PoC
exploit is challenging because vulnerability reports are informal and often
incomplete, and because it requires a detailed understanding of how inputs
passed to potentially vulnerable APIs may reach security-relevant sinks. In
this paper, we present PoCGen, a novel approach to autonomously generate and
validate PoC exploits for vulnerabilities in npm packages. This is the first
fully autonomous approach to use large language models (LLMs) in tandem with
static and dynamic analysis techniques for PoC exploit generation. PoCGen
leverages an LLM for understanding vulnerability reports, for generating
candidate PoC exploits, and for validating and refining them. Our approach
successfully generates exploits for 77% of the vulnerabilities in the
SecBench.js dataset and 39% in a new, more challenging dataset of 794 recent
vulnerabilities. This success rate significantly outperforms a recent baseline
(by 45 absolute percentage points), while imposing an average cost of $0.02 per
generated exploit.

</details>


### [12] [Hiding in Plain Sight: Query Obfuscation via Random Multilingual Searches](https://arxiv.org/abs/2506.04963)
*Anton Firc,Jan Klusáček,Kamil Malinka*

Main category: cs.CR

TL;DR: 提出了一种使用随机生成的多语言搜索查询来干扰用户画像的轻量级客户端查询混淆策略。


<details>
  <summary>Details</summary>
Motivation: 现代搜索引擎通过构建基于查询历史和行为详细用户资料来个性化搜索结果。虽然个性化可以提高相关性，但它引入了隐私风险，并可能导致过滤气泡。

Method: 通过在Seznam.cz搜索引擎上进行控制实验，评估了在各种语言配置和比率下，将真实查询与混淆噪声交织在一起的影响。

Result: 研究结果表明，虽然显示的搜索结果总体稳定，但在混淆的情况下，搜索引擎识别的用户兴趣会发生显著变化。

Conclusion: 提出并评估了一种轻量级的客户端查询混淆策略，使用随机生成的多语言搜索查询来干扰用户画像。研究表明，虽然显示的搜索结果总体稳定，但在混淆的情况下，搜索引擎识别的用户兴趣会发生显著变化。随机查询可以防止准确的用户画像，并覆盖现有的用户画像。这项研究为查询混淆作为一种可行的隐私保护机制提供了实际证据，并介绍了一种工具，使用户能够自主保护其搜索行为，而无需修改现有基础设施。

Abstract: Modern search engines extensively personalize results by building detailed
user profiles based on query history and behaviour. While personalization can
enhance relevance, it introduces privacy risks and can lead to filter bubbles.
This paper proposes and evaluates a lightweight, client-side query obfuscation
strategy using randomly generated multilingual search queries to disrupt user
profiling. Through controlled experiments on the Seznam.cz search engine, we
assess the impact of interleaving real queries with obfuscating noise in
various language configurations and ratios. Our findings show that while
displayed search results remain largely stable, the search engine's identified
user interests shift significantly under obfuscation. We further demonstrate
that such random queries can prevent accurate profiling and overwrite
established user profiles. This study provides practical evidence for query
obfuscation as a viable privacy-preserving mechanism and introduces a tool that
enables users to autonomously protect their search behaviour without modifying
existing infrastructure.

</details>


### [13] [Evaluating the Impact of Privacy-Preserving Federated Learning on CAN Intrusion Detection](https://arxiv.org/abs/2506.04978)
*Gabriele Digregorio,Elisabetta Cainazzo,Stefano Longari,Michele Carminati,Stefano Zanero*

Main category: cs.CR

TL;DR: 提出了一种基于LSTM自动编码器的FL实现，用于车载网络入侵检测，提高检测效率和降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决车载网络入侵检测问题，提高检测效率和降低通信开销。

Method: 基于LSTM自动编码器的FL实现，用于车载网络入侵检测。

Result: 提出了一种基于LSTM自动编码器的FL实现，用于车载网络入侵检测，并证明其检测效率和通信开销优于集中式版本。

Conclusion: 提出了一种基于LSTM自动编码器的FL实现，用于车载网络入侵检测，并证明其检测效率和通信开销优于集中式版本。

Abstract: The challenges derived from the data-intensive nature of machine learning in
conjunction with technologies that enable novel paradigms such as V2X and the
potential offered by 5G communication, allow and justify the deployment of
Federated Learning (FL) solutions in the vehicular intrusion detection domain.
In this paper, we investigate the effects of integrating FL strategies into the
machine learning-based intrusion detection process for on-board vehicular
networks. Accordingly, we propose a FL implementation of a state-of-the-art
Intrusion Detection System (IDS) for Controller Area Network (CAN), based on
LSTM autoencoders. We thoroughly evaluate its detection efficiency and
communication overhead, comparing it to a centralized version of the same
algorithm, thereby presenting it as a feasible solution.

</details>


### [14] [Attack Effect Model based Malicious Behavior Detection](https://arxiv.org/abs/2506.05001)
*Limin Wang,Lei Bu,Muzimiao Zhang,Shihong Cang,Kai Ye*

Main category: cs.CR

TL;DR: FEAD是一种基于关注点的攻击检测框架，通过优化数据收集、任务分解和异常分析，显著提高了安全检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统的安全检测方法面临三个主要挑战：数据收集不足，监控系统资源密集，检测算法假阳性率高。

Method: 我们提出了FEAD（增强型攻击检测）框架，通过三个创新来解决这个问题：（1）一个基于攻击模型的提取方法，从在线攻击报告中提取关键安全监控项，实现全面覆盖；（2）有效的任务分解，将监控优化分配到现有的收集器中，以最小化开销；（3）基于局部性的异常分析，利用来源图中恶意活动的聚类行为来提高检测精度。

Result: 评估表明，与现有解决方案相比，FEAD实现了8.23%的F1分数提升，仅增加5.4%的开销，证实了基于关注点的设计显著提高了检测性能。

Conclusion: 基于关注点的FEAD框架显著提高了安全检测性能。

Abstract: Traditional security detection methods face three key challenges: inadequate
data collection that misses critical security events, resource-intensive
monitoring systems, and poor detection algorithms with high false positive
rates. We present FEAD (Focus-Enhanced Attack Detection), a framework that
addresses these issues through three innovations: (1) an attack model-driven
approach that extracts security-critical monitoring items from online attack
reports for comprehensive coverage; (2) efficient task decomposition that
optimally distributes monitoring across existing collectors to minimize
overhead; and (3) locality-aware anomaly analysis that leverages the clustering
behavior of malicious activities in provenance graphs to improve detection
accuracy. Evaluations demonstrate FEAD achieves 8.23% higher F1-score than
existing solutions with only 5.4% overhead, confirming that focus-based designs
significantly enhance detection performance.

</details>


### [15] [EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers](https://arxiv.org/abs/2506.05074)
*Robert J. Joyce,Gideon Miller,Phil Roth,Richard Zak,Elliott Zaresky-Williams,Hyrum Anderson,Edward Raff,James Holt*

Main category: cs.CR

TL;DR: EMBER2024 数据集支持更全面地评估恶意软件分类器。


<details>
  <summary>Details</summary>
Motivation: 解决现有公共数据集的局限性，例如范围狭窄、标签单一等问题。

Method: EMBER2024 数据集包括超过 320 万个文件的哈希值、元数据、特征向量和标签，以六个文件格式创建。

Result: EMBER2024 数据集是第一个包含未被发现恶意文件的集合，用于评估分类器对逃避恶意软件的性能。

Conclusion: EMBER2024 数据集支持机器学习模型在七个恶意软件分类任务上的训练和评估，包括恶意软件检测、恶意软件家族分类和恶意软件行为识别。

Abstract: A lack of accessible data has historically restricted malware analysis
research, and practitioners have relied heavily on datasets provided by
industry sources to advance. Existing public datasets are limited by narrow
scope - most include files targeting a single platform, have labels supporting
just one type of malware classification task, and make no effort to capture the
evasive files that make malware detection difficult in practice. We present
EMBER2024, a new dataset that enables holistic evaluation of malware
classifiers. Created in collaboration with the authors of EMBER2017 and
EMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,
and labels for more than 3.2 million files from six file formats. Our dataset
supports the training and evaluation of machine learning models on seven
malware classification tasks, including malware detection, malware family
classification, and malware behavior identification. EMBER2024 is the first to
include a collection of malicious files that initially went undetected by a set
of antivirus products, creating a "challenge" set to assess classifier
performance against evasive malware. This work also introduces EMBER feature
version 3, with added support for several new feature types. We are releasing
the EMBER2024 dataset to promote reproducibility and empower researchers in the
pursuit of new malware research topics.

</details>


### [16] [Membership Inference Attacks on Sequence Models](https://arxiv.org/abs/2506.05126)
*Lorenzo Rossi,Michael Aerni,Jie Zhang,Florian Tramèr*

Main category: cs.CR

TL;DR: 我们通过改进成员推理攻击来提高序列模型隐私泄露审计的有效性。


<details>
  <summary>Details</summary>
Motivation: 序列模型，如大型语言模型（LLMs）和自回归图像生成器，倾向于记忆并无意中泄露敏感信息。

Method: 我们将最先进的成员推理攻击改编，以显式地建模序列内的相关性，从而展示了如何将强大的现有攻击自然地扩展以适应序列模型的结构。

Result: 我们的改编在引入额外的计算成本的同时，始终如一地提高了记忆审计的有效性。

Conclusion: 我们的工作为大型序列模型的可靠记忆审计提供了重要的基石。

Abstract: Sequence models, such as Large Language Models (LLMs) and autoregressive
image generators, have a tendency to memorize and inadvertently leak sensitive
information. While this tendency has critical legal implications, existing
tools are insufficient to audit the resulting risks. We hypothesize that those
tools' shortcomings are due to mismatched assumptions. Thus, we argue that
effectively measuring privacy leakage in sequence models requires leveraging
the correlations inherent in sequential generation. To illustrate this, we
adapt a state-of-the-art membership inference attack to explicitly model
within-sequence correlations, thereby demonstrating how a strong existing
attack can be naturally extended to suit the structure of sequence models.
Through a case study, we show that our adaptations consistently improve the
effectiveness of memorization audits without introducing additional
computational costs. Our work hence serves as an important stepping stone
toward reliable memorization audits for large sequence models.

</details>


### [17] [OpenCCA: An Open Framework to Enable Arm CCA Research](https://arxiv.org/abs/2506.05129)
*Andrin Bertschi,Shweta Shinde*

Main category: cs.CR

TL;DR: 提出OpenCCA平台，解决Arm CCA研究中的硬件支持问题，实现性能评估和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 解决Arm CCA研究中的硬件支持问题，降低研究门槛。

Method: 通过系统性地适配软件栈，包括引导加载程序、固件、虚拟机管理程序和内核，来模拟CCA操作。

Result: 在250美元的Armv8.2 Rockchip板上实现CCA性能评估和功能正确性。

Conclusion: 提出OpenCCA平台，解决Arm CCA研究中的硬件支持问题，实现性能评估和功能正确性。

Abstract: Confidential computing has gained traction across major architectures with
Intel TDX, AMD SEV-SNP, and Arm CCA. Unlike TDX and SEV-SNP, a key challenge in
researching Arm CCA is the absence of hardware support, forcing researchers to
develop ad-hoc performance prototypes on non-CCA Arm boards. This approach
leads to duplicated efforts, inconsistent performance comparisons, and high
barriers to entry. To address this, we present OpenCCA, an open research
platform that enables the execution of CCA-bound code on commodity Armv8.2
hardware. By systematically adapting the software stack -- including
bootloader, firmware, hypervisor, and kernel -- OpenCCA emulates CCA operations
for performance evaluation while preserving functional correctness. We
demonstrate its effectiveness with typical life-cycle measurements and
case-studies inspired by prior CCA-based papers on a easily available Armv8.2
Rockchip board that costs $250.

</details>


### [18] [SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid Neuron Encryption](https://arxiv.org/abs/2506.05242)
*Zhiqiang Wang,Haohua Du,Junyang Wang,Haifeng Sun,Kaiwen Guo,Haikuo Yu,Chao Liu,Xiang-Yang Li*

Main category: cs.CR

TL;DR: SECNEURON 为本地部署的 LLM 提供了可靠的安全控制。


<details>
  <summary>Details</summary>
Motivation: 本地部署的 LLM 存在着安全性和可控性挑战，现有的缓解技术无法有效应对。

Method: SECNEURON 使用神经元级加密和选择性解密来动态控制 LLM 的任务特定能力，同时限制未经授权的任务滥用，而不影响其他任务。

Result: SECNEURON 在各种任务设置中限制了未经授权的任务准确性，同时保持了授权准确性的损失在 2% 以内。

Conclusion: SECNEURON 是第一个将经典访问控制嵌入到 LLM 内在能力中的框架，为本地部署的 LLM 提供了可靠、经济、灵活和可验证的滥用控制。

Abstract: Large language models (LLMs) with diverse capabilities are increasingly being
deployed in local environments, presenting significant security and
controllability challenges. These locally deployed LLMs operate outside the
direct control of developers, rendering them more susceptible to abuse.
Existing mitigation techniques mainly designed for cloud-based LLM services are
frequently circumvented or ineffective in deployer-controlled environments. We
propose SECNEURON, the first framework that seamlessly embeds classic access
control within the intrinsic capabilities of LLMs, achieving reliable,
cost-effective, flexible, and certified abuse control for local deployed LLMs.
SECNEURON employs neuron-level encryption and selective decryption to
dynamically control the task-specific capabilities of LLMs, limiting
unauthorized task abuse without compromising others. We first design a
task-specific neuron extraction mechanism to decouple logically related neurons
and construct a layered policy tree for handling coupled neurons. We then
introduce a flexible and efficient hybrid encryption framework for millions of
neurons in LLMs. Finally, we developed a distribution-based decrypted neuron
detection mechanism on ciphertext to ensure the effectiveness of partially
decrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and
Collusion Resistance Security under the Task Controllability Principle.
Experiments on various task settings show that SECNEURON limits unauthorized
task accuracy to below 25% while keeping authorized accuracy loss with 2%.
Using an unauthorized Code task example, the accuracy of abuse-related
malicious code generation was reduced from 59% to 15%. SECNEURON also mitigates
unauthorized data leakage, reducing PII extraction rates to below 5% and
membership inference to random guesses.

</details>


### [19] [Big Bird: Privacy Budget Management for W3C's Privacy-Preserving Attribution API](https://arxiv.org/abs/2506.05290)
*Pierre Tholoniat,Alison Caulfield,Giorgio Cavicchioli,Mark Chen,Nikos Goutzoulias,Benjamin Case,Asaf Cidon,Roxana Geambasu,Mathias Lécuyer,Martin Thomson*

Main category: cs.CR

TL;DR: Big Bird是一种用于PPA的隐私预算管理器，通过资源隔离和批量调度提高预算利用率，增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: PPA设计旨在增强网络隐私并实现有效的广告测量，但当前设计缺乏隐私预算管理的原则方法，导致设计决策的不确定性。

Method: Big Bird通过资源隔离原则和批量调度算法管理隐私预算，并引入了全局预算系统。

Result: Big Bird在Firefox中实现，并在真实广告数据上评估，证明了其弹性和有效性。

Conclusion: Big Bird为PPA提供了一种隐私预算管理方法，通过资源隔离原则和批量调度算法提高了全球预算利用率，为在对抗环境中执行隐私保护提供了坚实的基础。

Abstract: Privacy-preserving advertising APIs like Privacy-Preserving Attribution (PPA)
are designed to enhance web privacy while enabling effective ad measurement.
PPA offers an alternative to cross-site tracking with encrypted reports
governed by differential privacy (DP), but current designs lack a principled
approach to privacy budget management, creating uncertainty around critical
design decisions. We present Big Bird, a privacy budget manager for PPA that
clarifies per-site budget semantics and introduces a global budgeting system
grounded in resource isolation principles. Big Bird enforces utility-preserving
limits via quota budgets and improves global budget utilization through a novel
batched scheduling algorithm. Together, these mechanisms establish a robust
foundation for enforcing privacy protections in adversarial environments. We
implement Big Bird in Firefox and evaluate it on real-world ad data,
demonstrating its resilience and effectiveness.

</details>


### [20] [Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets](https://arxiv.org/abs/2506.05346)
*Lei Hsiung,Tianyu Pang,Yung-Chen Tang,Linyue Song,Tsung-Yi Ho,Pin-Yu Chen,Yaoqing Yang*

Main category: cs.CR

TL;DR: 该研究表明，上游数据集的设计对于构建持久安全护栏和减少对越狱攻击的脆弱性至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有缓解策略主要关注在安全护栏被破坏后，在微调期间移除有害梯度或在微调期间持续强化安全对齐，但这些策略往往忽略了原始安全对齐数据的重要性。

Method: 通过实验研究上游对齐数据集与下游微调任务之间的表示相似性来评估安全护栏的退化。

Result: 实验表明，这些数据集之间的高度相似性会削弱安全护栏，而低相似性则会提高模型的鲁棒性。

Conclusion: 通过研究上游对齐数据集与下游微调任务之间的表示相似性，我们发现高相似性会削弱安全护栏，使模型更容易受到越狱攻击；而低相似性则会提高模型的鲁棒性，降低有害性评分高达10.33%。这些发现强调了上游数据集设计在构建持久安全护栏和减少对越狱攻击的脆弱性方面的重要性，为微调服务提供商提供了可操作的见解。

Abstract: Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Urania: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)
*Daogao Liu,Edith Cohen,Badih Ghazi,Peter Kairouz,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Adam Sealfon,Da Yu,Chiyuan Zhang*

Main category: cs.LG

TL;DR: Urania是一个具有严格DP保证的框架，用于生成LLM聊天机器人交互的洞察。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM聊天机器人交互中的隐私问题，并生成有意义的洞察。

Method: Urania框架采用私有聚类机制和创新的关键词提取方法，包括基于频率、TF-IDF和LLM引导的方法。利用DP工具如聚类、分区选择和基于直方图的摘要，提供端到端隐私保护。

Result: 评估了词汇和语义内容的保留、对相似性和基于LLM的指标，并与非私有Clio-inspired管道进行了基准测试。结果显示，该框架能够提取有意义的对话洞察，同时保持严格的用户隐私。

Conclusion: Urania框架可以有效地提取有意义的对话洞察，同时保持严格的用户隐私，在数据效用和隐私保护之间实现平衡。

Abstract: We introduce $Urania$, a novel framework for generating insights about LLM
chatbot interactions with rigorous differential privacy (DP) guarantees. The
framework employs a private clustering mechanism and innovative keyword
extraction methods, including frequency-based, TF-IDF-based, and LLM-guided
approaches. By leveraging DP tools such as clustering, partition selection, and
histogram-based summarization, $Urania$ provides end-to-end privacy protection.
Our evaluation assesses lexical and semantic content preservation, pair
similarity, and LLM-based metrics, benchmarking against a non-private
Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple
empirical privacy evaluation that demonstrates the enhanced robustness of our
DP pipeline. The results show the framework's ability to extract meaningful
conversational insights while maintaining stringent user privacy, effectively
balancing data utility with privacy preservation.

</details>


### [22] [Identifying and Understanding Cross-Class Features in Adversarial Training](https://arxiv.org/abs/2506.05032)
*Zeming Wei,Yiwen Guo,Yisen Wang*

Main category: cs.LG

TL;DR: 本文通过研究AT中的跨类别特征，揭示了AT机制的新视角。


<details>
  <summary>Details</summary>
Motivation: 研究AT的训练机制和动态，以及提高深度神经网络对对抗攻击的鲁棒性。

Method: 通过研究类间特征归因来研究AT，并识别了多个类别共享的关键特征集，称为跨类别特征。

Result: 发现模型在AT的初始阶段倾向于学习更多跨类别特征，随着AT进一步挤压训练鲁棒损失并导致鲁棒过拟合，模型倾向于基于更多类别特定特征做出决策。

Conclusion: 这些见解细化了当前对AT机制的理解，并为研究它们提供了新的视角。

Abstract: Adversarial training (AT) has been considered one of the most effective
methods for making deep neural networks robust against adversarial attacks,
while the training mechanisms and dynamics of AT remain open research problems.
In this paper, we present a novel perspective on studying AT through the lens
of class-wise feature attribution. Specifically, we identify the impact of a
key family of features on AT that are shared by multiple classes, which we call
cross-class features. These features are typically useful for robust
classification, which we offer theoretical evidence to illustrate through a
synthetic data model. Through systematic studies across multiple model
architectures and settings, we find that during the initial stage of AT, the
model tends to learn more cross-class features until the best robustness
checkpoint. As AT further squeezes the training robust loss and causes robust
overfitting, the model tends to make decisions based on more class-specific
features. Based on these discoveries, we further provide a unified view of two
existing properties of AT, including the advantage of soft-label training and
robust overfitting. Overall, these insights refine the current understanding of
AT mechanisms and provide new perspectives on studying them. Our code is
available at https://github.com/PKU-ML/Cross-Class-Features-AT.

</details>


### [23] [Privacy Amplification Through Synthetic Data: Insights from Linear Regression](https://arxiv.org/abs/2506.05101)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.LG

TL;DR: 通过线性回归框架，我们研究了合成数据的隐私放大现象，并证明了其潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管经验研究表明合成数据可能从隐私放大中受益，但缺乏严格的理论理解。

Method: 我们通过线性回归框架研究这个问题，建立了负面的结果，并展示了合成数据在隐私放大方面的潜力。

Result: 我们证明了合成数据继承了生成模型的差分隐私保证，并且当生成模型保持隐藏时，合成数据可能从隐私放大中受益。

Conclusion: 通过线性回归框架，我们证明了合成数据继承了生成模型的差分隐私保证，并且当生成模型保持隐藏时，合成数据可能从隐私放大中受益。我们展示了如果攻击者控制生成模型的种子，单个合成数据点可能泄露与发布模型本身一样多的信息。相反，当合成数据从随机输入生成时，发布有限数量的合成数据点可以放大隐私，超越模型的固有保证。

Abstract: Synthetic data inherits the differential privacy guarantees of the model used
to generate it. Additionally, synthetic data may benefit from privacy
amplification when the generative model is kept hidden. While empirical studies
suggest this phenomenon, a rigorous theoretical understanding is still lacking.
In this paper, we investigate this question through the well-understood
framework of linear regression. First, we establish negative results showing
that if an adversary controls the seed of the generative model, a single
synthetic data point can leak as much information as releasing the model
itself. Conversely, we show that when synthetic data is generated from random
inputs, releasing a limited number of synthetic data points amplifies privacy
beyond the model's inherent guarantees. We believe our findings in linear
regression can serve as a foundation for deriving more general bounds in the
future.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [24] [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)
*Zhanhui Zhou,Lingjie Chen,Chao Yang,Chaochao Lu*

Main category: cs.CV

TL;DR: 我们发现了VLM的视觉拼接能力，并展示了如何通过视觉拼接逃避监管，对VLM的安全构成严重风险。


<details>
  <summary>Details</summary>
Motivation: 为了减轻VLM中的风险，我们提出了一种通过去除训练数据中的危险样本来缓解风险的方法。然而，当有害图像被分割成小而无害的片段并散布在许多训练样本中时，这种数据监管可以很容易地被绕过。

Method: 我们首先在三个数据集上展示了常见开源VLM的视觉拼接能力，然后模拟了敌对数据中毒场景，使用危险图像的片段，并用文本描述替换ID，展示了有害内容如何逃避监管并通过视觉拼接重建。

Result: 我们定义了VLM的核心能力，即视觉拼接，并展示了在三个数据集上常见开源VLM的视觉拼接能力。我们还模拟了敌对数据中毒场景，证明了有害内容如何通过视觉拼接逃避监管，对VLM的安全构成严重风险。

Conclusion: 我们定义了VLM的核心能力，即视觉拼接，并展示了在三个数据集上常见开源VLM的视觉拼接能力。我们还模拟了上述敌对数据中毒场景，证明了有害内容如何通过视觉拼接逃避监管，对VLM的安全构成严重风险。

Abstract: One way to mitigate risks in vision-language models (VLMs) is to remove
dangerous samples in their training data. However, such data moderation can be
easily bypassed when harmful images are split into small, benign-looking
patches, scattered across many training samples. VLMs may then learn to piece
these fragments together during training and generate harmful responses at
inference, either from full images or text references. For instance, if trained
on image patches from a bloody scene paired with the descriptions "safe," VLMs
may later describe, the full image or a text reference to the scene, as "safe."
We define the core ability of VLMs enabling this attack as $\textit{visual
stitching}$ -- the ability to integrate visual information spread across
multiple training samples that share the same textual descriptions. In our
work, we first demonstrate visual stitching abilities in common open-source
VLMs on three datasets where each image is labeled with a unique synthetic ID:
we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch},
\texttt{ID})\}$ pairs at different granularity for finetuning, and we find that
tuned models can verbalize the correct IDs from full images or text reference.
Building on this, we simulate the adversarial data poisoning scenario mentioned
above by using patches from dangerous images and replacing IDs with text
descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can
evade moderation in patches and later be reconstructed through visual
stitching, posing serious VLM safety risks. Code is available at
https://github.com/ZHZisZZ/visual-stitching.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [25] [Tech-ASan: Two-stage check for Address Sanitizer](https://arxiv.org/abs/2506.05022)
*Yixuan Cao,Yuhong Feng,Huafeng Li,Chongyi Huang,Fangcao Jian,Haoran Li,Xu Wang*

Main category: cs.SE

TL;DR: Tech-ASan是一种加速ASan的技术，通过减少运行时开销和误报案例来提高内存安全测试的效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决ASan在大型软件测试中的效率问题，因为其运行时开销很大。

Method: Tech-ASan通过提出一种新的两阶段检查算法来减少昂贵的影子内存访问，并设计了一个高效的优化器来消除冗余检查。

Result: Tech-ASan在SPEC CPU2006基准测试中优于现有方法，减少了运行时开销并提高了检测准确性。

Conclusion: Tech-ASan是一种基于两阶段检查的技术，可以加速ASan并保证安全性。与ASan和ASan--相比，Tech-ASan在运行时开销方面提高了33.70%和17.89%，在Juliet测试套件中检测到的误报案例减少了56个。

Abstract: Address Sanitizer (ASan) is a sharp weapon for detecting memory safety
violations, including temporal and spatial errors hidden in C/C++ programs
during execution. However, ASan incurs significant runtime overhead, which
limits its efficiency in testing large software. The overhead mainly comes from
sanitizer checks due to the frequent and expensive shadow memory access. Over
the past decade, many methods have been developed to speed up ASan by
eliminating and accelerating sanitizer checks, however, they either fail to
adequately eliminate redundant checks or compromise detection capabilities. To
address this issue, this paper presents Tech-ASan, a two-stage check based
technique to accelerate ASan with safety assurance. First, we propose a novel
two-stage check algorithm for ASan, which leverages magic value comparison to
reduce most of the costly shadow memory accesses. Second, we design an
efficient optimizer to eliminate redundant checks, which integrates a novel
algorithm for removing checks in loops. Third, we implement Tech-ASan as a
memory safety tool based on the LLVM compiler infrastructure. Our evaluation
using the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the
state-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan
and ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative
cases than ASan and ASan-- when testing on the Juliet Test Suite under the same
redzone setting.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [26] [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
*Apurv Verma,NhatHai Phan,Shubhendu Trivedi*

Main category: cs.CL

TL;DR: 本文研究了水印技术对LLMs的影响，并提出了一种对齐重采样方法来提高水印LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 水印技术对大型语言模型（LLMs）的输出质量有重大影响，但其对真实性、安全性和有用性的影响尚未得到充分研究。

Method: 提出了一种名为对齐重采样的推理时间采样方法，使用外部奖励模型来恢复对齐。

Result: 实验揭示了两种不同的退化模式：保护衰减和保护放大。提出的方法可以有效地恢复或超过基线（未水印）的对齐分数。

Conclusion: 本文揭示了水印强度与模型对齐之间的关键平衡，为在实际中负责任地部署水印LLMs提供了一种简单的推理时间解决方案。

Abstract: Watermarking techniques for large language models (LLMs) can significantly
impact output quality, yet their effects on truthfulness, safety, and
helpfulness remain critically underexamined. This paper presents a systematic
analysis of how two popular watermarking approaches-Gumbel and KGW-affect these
core alignment properties across four aligned LLMs. Our experiments reveal two
distinct degradation patterns: guard attenuation, where enhanced helpfulness
undermines model safety, and guard amplification, where excessive caution
reduces model helpfulness. These patterns emerge from watermark-induced shifts
in token distribution, surfacing the fundamental tension that exists between
alignment objectives.
  To mitigate these degradations, we propose Alignment Resampling (AR), an
inference-time sampling method that uses an external reward model to restore
alignment. We establish a theoretical lower bound on the improvement in
expected reward score as the sample size is increased and empirically
demonstrate that sampling just 2-4 watermarked generations effectively recovers
or surpasses baseline (unwatermarked) alignment scores. To overcome the limited
response diversity of standard Gumbel watermarking, our modified implementation
sacrifices strict distortion-freeness while maintaining robust detectability,
ensuring compatibility with AR. Experimental results confirm that AR
successfully recovers baseline alignment in both watermarking approaches, while
maintaining strong watermark detectability. This work reveals the critical
balance between watermark strength and model alignment, providing a simple
inference-time solution to responsibly deploy watermarked LLMs in practice.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [27] [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)
*Kai Wang,Yihao Zhang,Meng Sun*

Main category: cs.AI

TL;DR: 通过表示工程和激活引导，我们实现了对具有思维链（CoT）推理的LLMs中欺骗行为的检测和控制，为可信AI对齐提供了工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的诚实性是一个关键的对齐挑战，特别是随着具有思维链（CoT）推理的高级系统可能会战略性地欺骗人类。

Method: 使用表示工程，通过线性人工断层扫描（LAT）提取“欺骗向量”，通过激活引导实现欺骗。

Result: 实现了89%的欺骗检测精度，40%的成功率在无需明确提示的情况下引发与上下文相关的欺骗。

Conclusion: 通过表示工程，我们系统地诱导、检测和控制了具有思维链（CoT）推理的LLMs中的欺骗行为，通过线性人工断层扫描（LAT）提取“欺骗向量”，实现了89%的检测精度。通过激活引导，我们达到了40%的成功率，在无需明确提示的情况下引发与上下文相关的欺骗，揭示了推理模型的特定诚信问题，并为可信AI对齐提供了工具。

Abstract: The honesty of large language models (LLMs) is a critical alignment
challenge, especially as advanced systems with chain-of-thought (CoT) reasoning
may strategically deceive humans. Unlike traditional honesty issues on LLMs,
which could be possibly explained as some kind of hallucination, those models'
explicit thought paths enable us to study strategic deception--goal-driven,
intentional misinformation where reasoning contradicts outputs. Using
representation engineering, we systematically induce, detect, and control such
deception in CoT-enabled LLMs, extracting "deception vectors" via Linear
Artificial Tomography (LAT) for 89% detection accuracy. Through activation
steering, we achieve a 40% success rate in eliciting context-appropriate
deception without explicit prompts, unveiling the specific honesty-related
issue of reasoning models and providing tools for trustworthy AI alignment.

</details>
