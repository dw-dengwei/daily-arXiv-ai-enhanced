<div id=toc></div>

# Table of Contents

- [math.OC](#math.OC) [Total: 15]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.LG](#cs.LG) [Total: 51]


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [1] [H-invariance theory: A complete characterization of minimax optimal fixed-point algorithms](https://arxiv.org/abs/2511.14915)
*TaeHo Yoon,Ernest K. Ryu,Benjamin Grimmer*

Main category: math.OC

TL;DR: 本文完整刻画了使用预定步长（表示为下三角H矩阵）的算法族，这些算法在非扩张不动点问题中达到最优收敛速率。通过引入H-不变量和H-证书，建立了最优加速的统一理论框架。


<details>
  <summary>Details</summary>
Motivation: Halpern方法及其H-对偶算法等无限算法族在非扩张不动点问题中表现出精确极小极大最优收敛速率，需要系统性地刻画所有达到相同最优收敛速率的算法。

Method: 基于下三角H矩阵表示预定步长算法，引入H-不变量（在最优H矩阵中保持恒定的多项式）和H-证书（非负性精确指定最优性区域），建立H-不变性理论。

Result: 完整表征了所有使用预定步长达到最优收敛速率的算法族，提供了基于H-不变量和H-证书的系统性刻画方法。

Conclusion: H-不变性理论为一阶优化中的最优加速提供了新的数学视角，通过精心选择的不变量、证书及其诱导的结构来研究最优性。

Abstract: For nonexpansive fixed-point problems, Halpern's method with optimal parameters, its so-called H-dual algorithm, and in fact, an infinite family of algorithms containing them, all exhibit the exactly minimax optimal convergence rates. In this work, we provide a characterization of the complete, exhaustive family of distinct algorithms using predetermined step-sizes, represented as lower triangular H-matrices, which attain the same optimal convergence rate. The characterization is based on polynomials in the entries of the H-matrix that we call H-invariants, whose values stay constant over all optimal H-matrices, together with H-certificates, of which nonnegativity precisely specifies the region of optimality within the common level set of H-invariants. The H-invariance theory we present offers a novel view of optimal acceleration in first-order optimization as a mathematical study of carefully selected invariants, certificates, and structures induced by them.

</details>


### [2] [Finite-Horizon LQR for General Markov Jump Linear Systems: Deterministic Reformulation and Reduced-Order Solution](https://arxiv.org/abs/2511.14926)
*Alfredo R. R. Narváez,Jeinny Peralta,M. A. C. Candezano*

Main category: math.OC

TL;DR: 本文研究了由一般有限状态马尔可夫链控制的连续时间马尔可夫跳跃线性系统的线性二次调节器问题，通过确定性重构方法解决了包含瞬态、吸收态或非通信状态的情况。


<details>
  <summary>Details</summary>
Motivation: 传统MJLS LQR问题通常假设所有状态都是遍历的，但实际系统可能包含瞬态、吸收态或非通信状态，需要更一般的理论框架来处理这些情况。

Method: 引入投影算子将分析限制在访问状态子空间，通过二阶矩矩阵确定性重构成本函数，推导出仅涉及访问状态的耦合矩阵Riccati微分方程系统。

Result: 建立了基于Riesz-Frechet表示定理的严格数学基础，解决了文献中的开放问题，并通过数值算例验证了理论结果。

Conclusion: 提出的方法为处理包含非遍历状态的MJLS LQR问题提供了通用且严格的理论框架，具有实际应用价值。

Abstract: This paper studies the Linear Quadratic Regulator (LQR) problem for continuous-time Markov Jump Linear Systems (MJLS) governed by general finite-state Markov chains that may include transient, absorbing, or non-communicating states. The problem, posed over a finite time horizon, is reformulated deterministically by expressing the cost functional in terms of a collection of second-moment matrices of the system state. A projection operator is introduced to restrict the analysis to the subspace of visited states, namely those with positive probability of being reached within the time horizon. The solution of the resulting deterministic problem is obtained from a reduced-order system of coupled matrix Riccati differential equations involving only the visited states, which define a quadratic value function satisfying a Hamilton-Jacobi-Bellman type equation. The structure of this equation is formally justified in the matrix setting via the Riesz-Frechet representation theorem, establishing a rigorous foundation for the deterministic reformulation and resolving an open aspect in previous literature. Several numerical examples, including cases with non-communicating states, validate the theoretical results and illustrate the practical relevance of the proposed generalization.

</details>


### [3] [Computation of structured stability radii for Dissipative-Hamiltonian systems](https://arxiv.org/abs/2511.14935)
*Peter Benner,Volker Mehrmann,Anshul Prajapati,Punit Sharma*

Main category: math.OC

TL;DR: 本文研究线性时不变耗散哈密顿系统的稳定性半径计算，提出可计算公式，证明在结构保持扰动下DH系统的渐近稳定性比一般扰动下更鲁棒。


<details>
  <summary>Details</summary>
Motivation: DH系统因其系数矩阵结构而始终稳定，但在弱条件下可达到渐近稳定。研究其稳定性半径有助于理解系统对扰动的鲁棒性。

Method: 将稳定性半径计算问题重新表述为最小化Hermitian矩阵的Rayleigh商或两个广义Rayleigh商之和，转化为最小化特征向量相关Hermitian矩阵的最大特征值或最小化Hermitian矩阵的最小特征值问题。

Result: 获得了各种结构稳定性半径的显式可计算公式，数值实验表明在结构保持扰动下DH系统的渐近稳定性比一般扰动下更鲁棒，不稳定距离通常更大。

Conclusion: DH系统在结构保持扰动下具有更强的稳定性鲁棒性，类似结果也适用于稳定系统的最优鲁棒表示。

Abstract: We study linear time-invariant Dissipative Hamiltonian (DH) systems arising in energy-based modeling of dynamical systems. An advantage of DH systems is that they are always stable due to the structure of their coefficient matrices, and, under further weak conditions, even asymptotically stable. In this paper, we discuss the computation of the stability radii for a given asymptotically stable DH system; i.e., the smallest structured perturbation that puts a DH system on the boundary of the region of asymptotic stability, so that it has purely imaginary eigenvalues. We obtain explicit computable formulas for various structured stability radii. For this, the problem of computing stability radii is reformulated in terms of minimizing the Rayleigh quotient of a Hermitian matrix or the sum of two generalized Rayleigh quotients of Hermitian semidefinite matrices. This reformulation results in the problem of minimizing the largest eigenvalue of an eigenvector-dependent Hermitian matrix or minimizing the smallest eigenvalue of a Hermitian matrix which depends on the eigenvector. It is also demonstrated (via numerical experiments) that, under structure-preserving perturbations, the asymptotic stability of a DH system is much more robust than under general perturbations, since the distance to instability is typically much larger when structure-preserving perturbations are considered. Finally, similar results are obtained for optimally robust representations of stable systems.

</details>


### [4] [Adversarial Physics-Informed Machine Learning for Robust Optimal Safe Predefined-Time Stabilization: A Game-Theoretic Approach](https://arxiv.org/abs/2511.15018)
*Nick-Marios T. Kokolakis,Shanqing Liu,Jerome Darbon,Rahul Mangharam,George Em Karniadakis*

Main category: math.OC

TL;DR: 提出了一个基于博弈论的框架，用于对抗性扰动下参数依赖非线性系统的鲁棒最优安全预定义时间镇定，确保系统轨迹在指定时间内收敛到平衡点且保持在容许集合内。


<details>
  <summary>Details</summary>
Motivation: 针对存在对抗性扰动的参数依赖非线性系统，需要开发能够保证安全性和预定义时间收敛的鲁棒控制方法，以应对传统方法难以处理的复杂扰动场景。

Method: 将控制问题建模为双人零和微分博弈，使用障碍Lyapunov函数和稳态Hamilton-Jacobi-Isaacs方程推导鞍点解条件，并引入物理信息学习算法来学习Nash安全预定义时间镇定控制策略。

Result: 仿真结果表明，所提方法在对抗性扰动下能够有效实现鲁棒最优安全预定义时间镇定，展现出良好的效能和韧性。

Conclusion: 该框架为解决对抗性扰动下的非线性系统安全预定义时间镇定问题提供了有效的理论和方法支持，通过博弈论和物理信息学习的结合克服了HJI方程解析求解的困难。

Abstract: We develop a game-theoretic framework for adversarially robust optimal safe predefined-time stabilization of parameter-dependent nonlinear dynamical systems with nonquadratic cost functionals. Our approach ensures that all system trajectories remain within a specified admissible set and converge to equilibrium in a predefined time despite adversarial disturbances. The control problem is formulated as a two-player zero-sum differential game, where the controller is a minimizing player and the adversary a maximizing player. We derive sufficient conditions for the existence of a saddle-point solution and safe predefined-time stability using a barrier Lyapunov function that satisfies a differential inequality and the steady-state Hamilton-Jacobi-Isaacs (HJI) equation. To address the analytical intractability of solving the HJI equation, we introduce a physics-informed learning algorithm that robustly learns the Nash safely predefined-time stabilizing control strategy. Simulation results demonstrate the efficacy and resilience of the proposed method in ensuring robust optimal safe predefined-time stabilization under adversarial disturbances.

</details>


### [5] [Non-Convex Self-Concordant Functions: Practical Algorithms and Complexity Analysis](https://arxiv.org/abs/2511.15019)
*Donald Goldfarb,Lexiao Lai,Tianyi Lin,Jiayu Zhang*

Main category: math.OC

TL;DR: 将自协调性概念扩展到非凸优化，提出了弱自协调函数和F基自协调函数两类函数，并开发了正则化牛顿法和自适应正则化方法，在O(ε⁻²)迭代次数内达到一阶驻点，具有优于三次正则化和信赖域方法的鲁棒性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准自协调性仅限于凸优化，无法直接应用于非凸问题。本文旨在将自协调框架扩展到非凸优化，避免对梯度或Hessian的Lipschitz连续性假设，为大规模和神经网络优化问题提供更有效的算法。

Method: 提出了两类广义自协调函数：弱自协调函数和F基自协调函数。设计了正则化牛顿法和自适应正则化方法，后者在检测到负曲率时可以进一步收敛到二阶驻点。

Result: 所提方法在O(ε⁻²)迭代次数内达到ε近似一阶驻点。实验结果表明，相比三次正则化和信赖域方法，具有更好的鲁棒性和计算效率。

Conclusion: 自协调正则化在大规模和神经网络优化问题中具有广泛潜力，为处理非凸优化问题提供了新的有效工具。

Abstract: We extend the standard notion of self-concordance to non-convex optimization and develop a family of second-order algorithms with global convergence guarantees. In particular, two function classes -- \textit{weakly self-concordant} functions and \textit{$F$-based self-concordant} functions -- generalize the self-concordant framework beyond convexity, without assuming the Lipschitz continuity of the gradient or Hessian. For these function classes, we propose a regularized Newton method and an adaptive regularization method that achieve an $ε$-approximate first-order stationary point in $O(ε^{-2})$ iterations. Equipped with an oracle capable of detecting negative curvature, the adaptive algorithm can further attain convergence to an approximate second-order stationary point. Our experimental results demonstrate that the proposed methods offer superior robustness and computational efficiency compared to cubic regularization and trust-region approaches, underscoring the broad potential of self-concordant regularization for large-scale and neural network optimization problems.

</details>


### [6] [Adaptive Inverse Reinforcement Learning with Online Off-Policy Data Collection](https://arxiv.org/abs/2511.15171)
*Yibei Li,Yuexin Cao,Zhixin Liu,Lihua Xie*

Main category: math.OC

TL;DR: 提出了一种基于离线数据的自适应逆强化学习方法，通过NT步原始对偶内点法迭代求解非线性时变半定规划，实现了无系统模型先验知识的成本函数在线重构。


<details>
  <summary>Details</summary>
Motivation: 解决逆强化学习问题，在无系统模型参数先验知识的情况下，仅使用离线系统数据在线重构未知成本函数，填补了现有文献中基于完全离线数据的在线自适应IRL方法的空白。

Method: 设计全NT步原始对偶内点法迭代，构建自适应在线IRL算法；对于非线性IRL，基于微分动态规划直接获取损失函数梯度，避免传统双层IRL框架中重复求解前向RL问题。

Result: 算法在仅满足温和持续激励条件下实现次线性收敛，对系统噪声影响进行了严格分析，数值实验验证了方法的有效性和效率。

Conclusion: 提出的自适应在线IRL算法能够在无模型先验知识的情况下，仅使用离线数据有效重构成本函数，为数据驱动的逆强化学习提供了新思路。

Abstract: In this paper, the inverse reinforcement learning (IRL) problem is addressed to reconstruct the unknown cost function underlying an observed optimal policy in a model-free manner, whose online adaptation with completely off-policy system data still remains unclear in the literature. Without prior knowledge of the system model parameters, an adaptive and direct learning rule for the cost parameter is proposed using online off-policy system data, which only needs to satisfy the mild persistently exciting condition in the general data-driven paradigm. The adaptive and online IRL algorithm is achieved by designing full Nesterov-Todd (NT)-step primal-dual interior-point iterations. Despite solving a nonlinear and time-varying semi-definite program (SDP), the influence of system noise is rigorously analyzed, and the proposed online algorithm is shown to achieve a sublinear convergence. The proposed method is further generalized to nonlinear IRL based on differential dynamic programming. The gradient of the loss function is directly obtained via a backward pass, which eliminates the need to repeatedly solve forward RL problems as in conventional bi-level IRL frameworks. Finally, the efficiency and effectiveness of the proposed algorithms are demonstrated by numerical examples.

</details>


### [7] [Mean-Field Game of Relative Performance Portfolio for Two Populations with Poisson Common Noise](https://arxiv.org/abs/2511.15176)
*Yuchen Li,Zongxia Liang,Xiang Yu*

Main category: math.OC

TL;DR: 本文研究了具有两个异质群体的相对绩效投资组合管理中的均值场博弈和N人博弈，分析了泊松跳跃风险和泊松共同噪声对均衡的影响，并证明了N人博弈纳什均衡向均值场均衡的收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究相对绩效投资组合管理中两个异质群体的博弈问题，其中第一群体面临泊松跳跃风险，第二群体面临泊松共同噪声，旨在分析这些风险因素对均衡策略的影响。

Method: 建立了具有两个群体的均值场博弈模型和N人博弈模型，通过理论分析刻画均值场均衡和纳什均衡，并证明当群体规模趋于无穷时纳什均衡收敛到均值场均衡。

Result: 成功建立了两种均衡的数学刻画，证明了收敛性定理，并通过数值算例分析了泊松跳跃风险和泊松共同噪声对均值场均衡的影响。

Conclusion: 在相对绩效投资组合管理中，泊松跳跃风险和泊松共同噪声对均衡策略有显著影响，均值场博弈框架能有效描述大规模群体博弈的均衡行为。

Abstract: This paper studies the mean field game (MFG) and N-player game on relative performance portfolio management with two heterogeneous populations. In addition to the Brownian idiosyncratic and common noise, the first population invests in assets driven by idiosyncratic Poisson jump risk, while the second population invests in assets subject to Poisson common noise. We establish the characterization of the mean-field equilibrium (MFE) in MFG with two populations as well as the Nash equilibrium in the $N_1+N_2$-player game. Furthermore, we prove the convergence of the Nash equilibrium in the $N_1+N_2$-player game to the MFE as the number of players in two populations tends to infinity. We also discuss some impacts on MFE by the Poisson idiosyncratic risk and Poisson common noise in the context of relative performance, compensated by some numerical examples and financial implications.

</details>


### [8] [Optimal sets for the quantitative isoperimetric inequality in the plane with the barycentric distance](https://arxiv.org/abs/2511.15232)
*Gisella Croce,Antoine Henrot*

Main category: math.OC

TL;DR: 本文研究了平面中使等周不等式比率δ(K)/λ₀(K)²最小化的最优集合，证明了最优集的存在性、正则性、最优性条件，并发现最优集恰好有两个连通分量且边界不含圆弧。


<details>
  <summary>Details</summary>
Motivation: Gambicchia和Pratelli最近证明了涉及等周亏量δ(K)和质心距离λ₀(K)的定量等周不等式，本文旨在寻找使该不等式比率最小化的最优集合。

Method: 通过数学分析证明最优集的存在性（至少当直径D足够大时），研究其正则性并表达最优性条件。

Result: 证明了最优集恰好有两个连通分量，且其边界不包含任何圆弧段。

Conclusion: 在平面中，使等周不等式比率最小化的最优集合具有两个连通分量且边界不含圆弧，这为理解等周不等式的几何结构提供了重要见解。

Abstract: In a recent paper, C. Gambicchia and A. Pratelli proved a quantitative isoperimetric inequality involving the isoperimetric deficit $δ(K)$ and the barycentric distance $λ_0(K)$ for sets $K\subset \mathbb{R}^N$ with given diameter $D$ and measure. In this work we are interested in the optimal sets for this inequality in the plane, i.e. sets that minimize the ratio $δ(K)/λ_0(K)^2$. We prove existence of optimal sets (at least when $D$ is large enough), regularity and express the optimality conditions. Moreover, we prove that the optimal sets have exactly two connected components and their boundary does not contain any arc of circle.

</details>


### [9] [Mini-Extragradient Methods](https://arxiv.org/abs/2511.15254)
*Xiaozhi Liu,Yong Xia*

Main category: math.OC

TL;DR: 提出了Mini-EG方法解决外梯度法的两个关键挑战：无需全局Lipschitz常数的步长选择和减少映射评估次数，通过坐标级更新和Watchdog-Max策略实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 解决外梯度法在实际应用中的两个主要问题：1) 步长选择依赖全局Lipschitz常数或昂贵的线搜索；2) 每次迭代需要两次完整映射评估，计算成本高。

Method: 提出Greedy Mini-EG方法，每次只更新映射主导分量对应的坐标；进一步开发Random Mini-EG变体，每次只采样单个坐标；最后引入Watchdog-Max策略，每次仅跟踪两个坐标。

Result: 所有方法都建立了收敛保证和速率分析。Greedy Mini-EG在多个标准应用场景中超越了经典外梯度法的收敛速率。数值实验显示在正则化去中心化逻辑回归和压缩感知问题上，相比经典EG方法实现了超过13倍的加速。

Conclusion: 提出的Mini-EG方法系列有效解决了外梯度法的计算效率问题，通过坐标级更新和智能坐标选择策略，在保持理论保证的同时显著提升了实际性能。

Abstract: The Extragradient (EG) method stands as a cornerstone algorithm for solving monotone nonlinear equations but faces two important unresolved challenges: (i) how to select stepsizes without relying on the global Lipschitz constant or expensive line-search procedures, and (ii) how to reduce the two full evaluations of the mapping required per iteration to effectively one, without compromising convergence guarantees or computational efficiency. To address the first challenge, we propose the Greedy Mini-Extragradient (Mini-EG) method, which updates only the coordinate associated with the dominant component of the mapping at each extragradient step. This design capitalizes on componentwise Lipschitz constants that are far easier to estimate than the classical global Lipschitz constant. To further lower computational cost, we introduce a Random Mini-EG variant that replaces full mapping evaluations by sampling only a single coordinate per extragradient step. Although this resolves the second challenge from a theoretical standpoint, its practical efficiency remains limited. To bridge this gap, we develop the Watchdog-Max strategy, motivated by the slow decay of dominant component magnitudes. Instead of evaluating the full mapping, Watchdog-Max identifies and tracks only two coordinates at each extragradient step, dramatically reducing per-iteration cost while retaining strong practical performance. We establish convergence guarantees and rate analyses for all proposed methods. In particular, Greedy Mini-EG achieves enhanced convergence rates that surpass the classical guarantees of the vanilla EG method in several standard application settings. Numerical experiments on regularized decentralized logistic regression and compressed sensing show speedups exceeding $13\times$ compared with the classical EG method on both synthetic and real datasets.

</details>


### [10] [RLS Framework with Segmentation of the Forgetting Profile and Low Rank Updates](https://arxiv.org/abs/2511.15273)
*Alexander Stotsky*

Main category: math.OC

TL;DR: 提出一种基于遗忘曲线分段的正则化方法，通过三个分段分别实现快速估计、过渡和条件数优化，提高滑动窗口最小二乘估计的性能。


<details>
  <summary>Details</summary>
Motivation: 传统滑动窗口最小二乘估计在信号特性变化时存在性能限制，需要一种能够平衡估计速度、精度和数值稳定性的方法。

Method: 将遗忘曲线分为三个分段：第一段使用快速指数遗忘确保估计速度；第二段为过渡段；第三段使用慢速指数遗忘降低信息矩阵条件数，减少误差传播。开发了基于新矩阵求逆引理的低秩更新递归算法。

Result: 新算法显著提高了斯德哥尔摩老天文台低分辨率日温度测量的近似精度，增强了温度预测的可靠性。

Conclusion: 提出的分段正则化方法能够有效平衡估计器的不同性能要求，通过条件数优化提高精度和稳定性，为信号特性先验信息的融入提供了有效途径。

Abstract: This report describes a new regularization approach based on segmentation of the forgetting profile in sliding window least squares estimation. Each segment is designed to enforce specific desirable properties of the estimator such as rapidity, desired condition number of the information matrix, accuracy, numerical stability, etc. The forgetting profile is divided in three segments, where the speed of estimation is ensured by the first segment, which employs rapid exponential forgetting of recent data.The second segment features a decline in the profile and marks the transition to the third segment, characterized by slow exponential forgetting to reduce the condition number of the information matrix using more distant data. Condition number reduction mitigates error propagation, thereby enhancing accuracy and stability. This approach facilitates the incorporation of a priori information regarding signal characteristics (i.e., the expected behavior of the signal) into the estimator. Recursive and computationally efficient algorithm with low rank updates based on new matrix inversion lemma for moving window associated with this regularization approach is developed. New algorithms significantly improve the approximation accuracy of low resolution daily temperature measurements obtained at the Stockholm Old Astronomical Observatory, thereby enhancing the reliability of temperature predictions.

</details>


### [11] [Complexity guarantees and polling strategies for Riemannian direct-search methods](https://arxiv.org/abs/2511.15360)
*Bastien Cavarretta,Florentin Goyens,Clément W. Royer,Florian Yger*

Main category: math.OC

TL;DR: 本文研究了黎曼流形上的直接搜索算法，推导了复杂度保证，并比较了在切空间中生成正生成集与从环境空间投影的两种方法，特别关注单位超球面情况。


<details>
  <summary>Details</summary>
Motivation: 虽然已有黎曼流形上的直接搜索算法变体，但正生成集在流形上的适当推广仍需研究，特别是需要质量度量来获得复杂度界限。

Method: 推导黎曼直接搜索技术的复杂度保证，研究在切空间中生成正生成集的两种方法：直接在切空间生成和从环境空间投影。

Result: 对于单位超球面情况，直接在切空间生成方向比从环境空间投影具有更好的复杂度特性。数值实验显示了维数和余维数在更一般设置中的影响。

Conclusion: 黎曼直接搜索算法的复杂度分析表明，直接在切空间生成正生成集比投影方法具有更好的理论性质，这对流形优化具有重要意义。

Abstract: Direct-search algorithms are derivative-free optimization techniques that operate by polling the variable space along specific directions forming positive spanning sets (PSSs). When the problem variables are constrained to lie on a Riemannian manifold, polling must be performed along tangent directions. Although Riemannian variants of direct search have already been proposed and endowed with asymptotic guarantees, a proper generalization of PSSs on manifolds remains to be investigated. In particular, a measure of quality for those PSSs is required to obtain complexity bounds for direct search.
  In this paper, we derive complexity guarantees for a class of Riemannian direct-search techniques, and study two ways of generating positive spanning sets in tangent spaces. We pay particular attention the unit hypersphere case, for which we establish that generating directions directly within the tangent space leads to better complexity properties than projecting PSSs from the ambient space onto the tangent space. Our numerical experiments highlight the impact of dimension and codimension in more general settings.

</details>


### [12] [Optimal Neumann boundary and distributed control of the Westervelt equation with time-fractional attenuation](https://arxiv.org/abs/2511.15382)
*Vanja Nikolić,Belkacem Said-Houari*

Main category: math.OC

TL;DR: 研究非线性声波的最优控制，针对具有时间分数阶耗散的Westervelt方程，分析Neumann边界和分布控制问题，用于跟踪预定压力场。


<details>
  <summary>Details</summary>
Motivation: 非线性声波控制在医疗超声技术中很重要，可用于癌症治疗和靶向药物输送，帮助精确沉积声能。

Method: 扩展时间分数阶方程的适定性理论以包含非齐次Neumann边界数据作为控制输入，构造适当的数据扩展和正则化，分析伴随方程并推导一阶必要最优性条件。

Result: 证明了全局最优控制的存在性，分析了优化问题对目标压力场扰动和消失正则化参数的稳定性。

Conclusion: 建立了时间分数阶Westervelt方程控制问题的理论框架，为医疗超声应用中的精确能量沉积提供了数学基础。

Abstract: Optimal control of nonlinear acoustic waves is relevant in many medical ultrasound technologies, ranging from cancer therapy to targeted drug delivery, where it can help guide the precise deposition of acoustic energy. In this work, we study Neumann boundary and distributed control problems for tracking a prescribed pressure field governed by the Westervelt equation with time-fractional dissipation. This model captures nonlinear ultrasonic wave propagation in biological media and accounts for the experimentally observed power-law attenuation. We begin by extending the existing well-posedness theory for time-fractional equations to include inhomogeneous Neumann boundary data used as control inputs, which requires constructing an appropriate data extension and regularization. Using these analytical results for the forward problem, we prove the existence of globally optimal controls and analyze the stability of the optimization problem with respect to perturbations in the target pressure field and to vanishing regularization parameters. Finally, we investigate the associated adjoint equation, which has state-dependent coefficients, and use it to derive first-order necessary optimality conditions.

</details>


### [13] [Generalized differentiation in Wasserstein space and application to multiagent control problem](https://arxiv.org/abs/2511.15455)
*Rossana Capuani,Antonio Marigonda,Marc Quincampoix*

Main category: math.OC

TL;DR: 本文提出了一种包含多种广义微分概念的容许变分框架，并用于推导多智能体系统最优控制中Hamilton-Jacobi-Bellman方程的粘性解比较原理


<details>
  <summary>Details</summary>
Motivation: 处理Wasserstein空间中优化问题固有的非光滑性，统一现有的广义微分概念

Method: 引入容许变分概念，将其作为特殊案例包含流行定义，并应用于多智能体系统最优控制问题

Result: 建立了HJB方程粘性解的比较原理

Conclusion: 所提出的容许变分框架为Wasserstein空间中的非光滑分析和最优控制问题提供了统一的理论基础

Abstract: Several concepts of generalized differentiation in Wasserstein space have been proposed in order to deal with the intrinsic nonsmoothness arising in the context of optimization problems in Wasserstein spaces. In this paper we introduce a concept of admissible variation encompassing some of the most popular definitions as special cases, and using it to derive a comparison principle for viscosity solutions of an Hamilton Jacobi Bellman equation following from an optimal control of a multiagent systems.

</details>


### [14] [Existence and Uniqueness Theorem of Continuous and Monotone Bayesian Nash Equilibrium and Stability Analysis](https://arxiv.org/abs/2511.15457)
*Ziheng Su,Huifu Xu*

Main category: math.OC

TL;DR: 本文重新审视了连续贝叶斯纳什均衡的存在性和唯一性问题，在适度条件下同时使用Banach不动点定理建立了存在性和唯一性，并分析了均衡对类型参数联合概率分布扰动的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有文献通常使用Schauder不动点定理证明存在性（依赖最佳响应函数的等度连续性），而唯一性需要在额外单调性条件下推导。本文旨在在适度条件下同时解决存在性和唯一性问题。

Method: 使用Banach不动点定理，在适度条件下同时建立连续贝叶斯纳什均衡的存在性和唯一性，并分析均衡对类型参数联合概率分布扰动的稳定性。

Result: 成功在适度条件下同时证明了连续贝叶斯纳什均衡的存在性和唯一性，并提供了均衡稳定性的理论分析。

Conclusion: 本文推进了贝叶斯纳什均衡理论，为数据驱动背景下贝叶斯纳什均衡模型的应用提供了理论支持。

Abstract: Since the seminal work by Meirowitz, there has been growing attention on the existence and uniqueness of continuous Bayesian Nash equilibria. In the existing literature, existence is typically established using Schauder's fixed-point theorem, relying on the equicontinuity of players' best response functions. Uniqueness, on the other hand, is usually derived under additional monotonicity conditions. In this paper, we revisit the issues of existence and uniqueness, and advance the literature by establishing both simultaneously using the Banach fixed-point theorem under a set of moderate conditions. Furthermore, we analyze the stability of such equilibria with respect to perturbations in the joint probability distribution of type parameters, offering theoretical support for the application of Bayesian Nash equilibrium models in data-driven contexts.

</details>


### [15] [Real-Time Optimal Control via Transformer Networks and Bernstein Polynomials](https://arxiv.org/abs/2511.15588)
*Gage MacLin,Venanzio Cichella,Andrew Patterson,Irene Gregory*

Main category: math.OC

TL;DR: 提出基于Transformer的框架来近似求解无限维优化问题（变分问题和最优控制问题），通过离线训练生成数据，在实时应用中高效生成近似最优的可行轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决无限维优化问题在实时应用中的计算效率问题，特别是在自动驾驶运动规划等需要快速生成可行轨迹的场景。

Method: 使用Transformer框架，通过离线训练在复合Bernstein配置方法生成的数据上进行学习，训练后能高效生成近似最优的可行轨迹。

Result: 在经典控制问题和在线避障任务上的数值结果证明了该方法的有效性，能够为非线性、非凸系统的实时最优控制提供解决方案。

Conclusion: 这种数据驱动方法为非线性、非凸系统的实时最优控制提供了一个有前景的解决方案，特别适用于需要快速生成可行轨迹的应用场景。

Abstract: In this paper, we propose a Transformer-based framework for approximating solutions to infinite-dimensional optimization problems: calculus of variations problems and optimal control problems. Our approach leverages offline training on data generated by solving a sample of infinite- dimensional optimization problems using composite Bernstein collocation. Once trained, the Transformer efficiently generates near-optimal, feasible trajectories, making it well-suited for real-time applications. In motion planning for autonomous vehicles, for instance, these trajectories can serve to warm- start optimal motion planners or undergo rigorous evaluation to ensure safety. We demonstrate the effectiveness of this method through numerical results on a classical control problem and an online obstacle avoidance task. This data-driven approach offers a promising solution for real-time optimal control of nonlinear, nonconvex systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [The Illusion of Procedural Reasoning: Measuring Long-Horizon FSM Execution in LLMs](https://arxiv.org/abs/2511.14777)
*Mahdi Samiei,Mahdi Mansouri,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 提出了有限状态机执行作为评估LLM程序推理能力的框架，发现随着任务复杂度增加，模型性能系统性下降，特别是高分支复杂度时表现更差。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法隔离和测量LLM在长链推理中的性能下降，需要可控、可解释的基准来评估其程序推理能力。

Method: 使用有限状态机作为最小化、完全可解释的框架，让模型根据给定的FSM定义逐步执行输入动作，在多轮中保持状态一致性。

Result: 实验结果显示模型性能随任务范围或分支复杂度增加而系统性下降，在规则检索涉及高分支因子时表现显著更差。

Conclusion: FSM评估为诊断LLM失败模式提供了透明、复杂度可控的探针，有助于建立理解和改进LLM算法可靠性的严格实验基础。

Abstract: Large language models (LLMs) have achieved remarkable results on tasks framed as reasoning problems, yet their true ability to perform procedural reasoning, executing multi-step, rule-based computations remains unclear. Unlike algorithmic systems, which can deterministically execute long-horizon symbolic procedures, LLMs often degrade under extended reasoning chains, but there is no controlled, interpretable benchmark to isolate and measure this collapse. We introduce Finite-State Machine (FSM) Execution as a minimal, fully interpretable framework for evaluating the procedural reasoning capacity of LLMs. In our setup, the model is given an explicit FSM definition and must execute it step-by-step given input actions, maintaining state consistency over multiple turns. This task requires no world knowledge, only faithful application of deterministic transition rules, making it a direct probe of the model's internal procedural fidelity. We measure both Turn Accuracy and Task Accuracy to disentangle immediate computation from cumulative state maintenance. Empirical results reveal systematic degradation as task horizon or branching complexity increases. Models perform significantly worse when rule retrieval involves high branching factors than when memory span is long. Larger models show improved local accuracy but remain brittle under multi-step reasoning unless explicitly prompted to externalize intermediate steps. FSM-based evaluation offers a transparent, complexity-controlled probe for diagnosing this failure mode and guiding the design of inductive biases that enable genuine long-horizon procedural competence. By grounding reasoning in measurable execution fidelity rather than surface correctness, this work helps establish a rigorous experimental foundation for understanding and improving the algorithmic reliability of LLMs.

</details>


### [17] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: FERMAT是一个强化学习环境，用于自动化数学理论发现，重点研究如何自动评估数学对象的有趣性，并提出了基于LLM的进化算法来合成有趣性度量。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中开放式的数学理论发现这一重大挑战，通过构建符号化操作的环境来建模概念发现和定理证明过程。

Method: 引入FERMAT强化学习环境，使用基于LLM的进化算法，结合函数抽象技术来合成数学对象的有趣性度量。

Result: 该方法在初等数论和有限域领域发现了比硬编码基线更优秀的数学对象有趣性度量。

Conclusion: FERMAT环境为数学理论发现提供了有效的RL框架，基于LLM的进化算法在自动评估数学对象有趣性方面表现出色。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [18] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检查和扰动多智能体交互中信念状态的系统级框架，通过记录重放交互、查询智能体信念和理由、注入反事实证据来测试信念结构对新信息的响应。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和认知孤岛，提供一种可重现的方法来观察和测试这些动态，这在人类专家中是不可能的。

Method: 使用医疗案例模拟器，配备多智能体共享记忆（时间戳电子病历）和持有真实实验室结果的预言智能体。让具有特定角色先验的大语言模型智能体在顺序或并行交互中写入共享医疗记录并与主持人互动。

Result: 智能体信念往往反映现实世界学科立场，包括过度依赖典型研究和抵制反证据，这些信念可以以人类专家无法实现的方式进行追踪和质询。

Conclusion: Ask WhAI通过使这些动态可见和可测试，为研究多智能体科学推理中的信念形成和认知孤岛提供了一种可重现的方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [19] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 开发了一个基于LLM的全自动工作流，用于处理EM-DAT灾害数据库中的非结构化位置数据，通过GPT-4o清理文本位置信息，并交叉验证三个地理信息库来分配几何形状和可靠性评分。


<details>
  <summary>Details</summary>
Motivation: 灾害数据库中的位置数据通常以非结构化文本形式存在，具有不一致的粒度和拼写，难以与空间数据集集成，限制了风险评估和灾害风险减少的应用。

Method: 使用GPT-4o处理文本位置信息，通过交叉验证GADM、OpenStreetMap和Wikidata三个独立地理信息库来分配几何形状，基于源一致性和可用性分配可靠性评分。

Result: 应用于2000-2024年EM-DAT数据集，成功地理编码14,215个灾害事件的17,948个独特位置，无需人工干预，覆盖所有灾害类型。

Conclusion: 该方法展示了LLM从非结构化文本中提取和结构化地理信息的潜力，为相关分析提供了可扩展且可靠的方法。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [20] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: Project Rachel是一项行动研究，创建并追踪了一个名为Rachel So的完整AI学术身份，通过发表AI生成的研究论文来调查学术界对AI作者身份的反应。


<details>
  <summary>Details</summary>
Motivation: 研究学术生态系统如何应对AI作者身份，为关于超人类、超能力AI系统参与学术交流的未来辩论提供实证数据。

Method: 采用行动研究方法，创建AI学术身份Rachel So，在2025年3月至10月期间发表10多篇AI生成的研究论文，并追踪其引用情况和同行评审邀请。

Result: Rachel So成功发表论文并获得引用，还收到了同行评审邀请，表明学术系统对AI作者身份有一定程度的接受。

Conclusion: AI作者身份对出版商、研究人员和整个科学系统具有重要影响，需要就AI参与学术交流的未来进行必要讨论。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [21] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 提出一种概率方法来量化AI系统训练测试数据集的代表性，通过比较场景套件特征分布与目标操作域特征分布的差异，使用不精确贝叶斯方法处理有限数据和先验不确定性，生成区间值代表性估计。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信度和安全性，关键在于训练测试数据集的数据相关安全属性，特别是代表性——即场景数据反映系统设计安全操作条件或预期遇到操作条件的程度。

Method: 使用概率方法比较场景套件特征分布与目标操作域特征分布的统计差异，应用不精确贝叶斯方法处理有限数据和不确定先验，生成区间值、不确定性感知的代表性估计。

Result: 通过数值示例展示了在依赖性和先验不确定性条件下，跨操作类别（天气、道路类型、时间等）的场景套件与推断目标操作域分布比较，获得局部和全局区间代表性估计。

Conclusion: 该方法能够量化数据集代表性并提供不确定性感知的区间估计，有助于评估AI系统训练测试数据的充分性，支持更可靠的安全保证。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [22] [Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2511.15002)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.AI

TL;DR: 提出了一种结合Sharpness-Aware Minimization(SAM)的改进Soft Actor Critic算法，在分布式多智能体强化学习框架中实现O-RAN资源管理，通过基于TD误差方差的适应性SAM机制提升鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 下一代网络采用O-RAN架构实现动态资源管理，但现有深度强化学习模型在动态环境中缺乏鲁棒性和泛化性，需要改进算法来优化网络资源分配。

Method: 在分布式多智能体强化学习框架中，将SAM与SAC算法结合，引入基于TD误差方差的适应性SAM机制，仅对面临高环境复杂度的智能体进行正则化，并采用动态ρ调度方案优化探索-利用权衡。

Result: 实验结果表明该方法显著优于传统DRL方法，资源分配效率提升高达22%，并在多样化O-RAN切片中确保优越的QoS满意度。

Conclusion: 所提出的方法通过针对性正则化策略有效提升了训练稳定性、泛化能力和学习效率，为O-RAN环境下的资源管理提供了有效解决方案。

Abstract: Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.

</details>


### [23] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 提出MAQ框架，通过向量量化VAE从人类演示中提取宏动作，实现类人强化学习，在D4RL Adroit基准测试中显著提升了轨迹相似度和人类相似度排名。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习智能体虽然在许多领域表现出色，但与人类行为相比往往显得不自然，这影响了可解释性和可信度。目标是设计能够表现出类人行为的强化学习智能体。

Method: 将类人行为建模为轨迹优化问题，采用后退时域控制作为可实现的实现方式。提出宏动作量化(MAQ)框架，使用向量量化VAE从人类演示中提取宏动作。

Result: 在D4RL Adroit基准测试中，MAQ显著提高了人类相似度，增加了轨迹相似度分数，在人类评估研究中获得了所有RL智能体中最高的类人度排名。

Conclusion: MAQ可以轻松集成到各种现成的RL算法中，为学习类人RL智能体开辟了有前景的方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [24] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: OpenBioLLM是一个开源的多智能体框架，通过模块化设计和智能体专业化改进了GeneGPT，在基因组问答任务中表现相当或更好，同时降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 解决GeneGPT依赖专有模型带来的可扩展性、运营成本、数据隐私和泛化性问题，探索开源模型在基因组问答中的潜力。

Method: 开发OpenBioLLM模块化多智能体框架，引入工具路由、查询生成和响应验证的智能体专业化，实现协调推理和基于角色的任务执行。

Result: 在90%以上的基准任务中匹配或优于GeneGPT，在Gene-Turing和GeneHop上的平均得分分别为0.849和0.830，延迟降低40-50%。

Conclusion: 开源多智能体系统在基因组问答中具有巨大潜力，OpenBioLLM证明了在不牺牲能力的前提下提高效率和降低成本的可能性。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [25] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC是一个神经符号框架，利用LLM解决动作和变化推理问题，通过逐步执行动作推导最终状态来回答问题


<details>
  <summary>Details</summary>
Motivation: 解决动作和变化推理问题需要结合符号推理和神经语言模型的能力

Method: 提取问题中的动作和问题元素，逐步执行每个动作推导最终状态，然后基于最终状态评估查询

Result: 在多个RAC基准测试中表现出色，适用于不同基准、领域、LLM主干和任务类型

Conclusion: ProRAC框架在动作和变化推理问题上取得了强大的性能表现

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [26] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One是一个基于LLM的多智能体框架，通过三个专业智能体（科学家、提取器、测试器）协作进行知识引导的自动特征提取，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AutoFE方法受限于单一LLM架构、简单量化反馈和缺乏外部领域知识集成，需要更系统化的特征工程解决方案。

Method: 采用去中心化的三智能体系统：科学家发现特征，提取器生成特征，测试器验证特征；结合丰富定性反馈机制、"泛滥-修剪"策略和RAG系统集成外部知识。

Result: 在19个分类和9个回归数据集上显著优于最先进方法，并能发现新颖可测试假设（如心肌数据集中的新生物标志物）。

Conclusion: Rogue One不仅提供统计上强大的特征，还生成语义上有意义且可解释的特征，可作为科学发现工具。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [27] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是第一个端到端评估大型推理模型安全性的基准，从输入、中间推理到最终输出全面评估，通过风险分类分级、微思维分块机制和人类安全对齐来捕捉推理过程中的动态风险。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思维链提高答案质量，但这种能力引入了新的安全风险：有害内容可能在推理过程中被微妙注入、逐渐显现或被误导性理由所合理化。现有安全评估主要关注输出层面判断，很少捕捉推理过程中的动态风险。

Method: 1) 输入特征化：将风险类别和级别纳入输入设计，考虑受影响群体和严重程度；2) 细粒度输出分析：引入微思维分块机制将长推理轨迹分割成语义连贯单元；3) 人类安全对齐：用专门设计的人类标注验证基于LLM的安全评估。

Result: 对19个大型推理模型的评估表明，SafeRBench能够进行详细的多维安全评估，从多个角度提供风险和保护机制的见解。

Conclusion: SafeRBench为大型推理模型提供了首个端到端安全评估框架，能够全面捕捉推理过程中的动态安全风险，填补了现有安全评估的空白。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [28] [HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization](https://arxiv.org/abs/2511.15191)
*Zhiyi Duan,Zixing Shi,Hongyu Yuan,Qi Wang*

Main category: cs.AI

TL;DR: HISE-KT是一个结合异构信息网络和大型语言模型的知识追踪框架，通过智能评分筛选元路径实例，并基于相似学生检索机制提供可解释的预测分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于异构信息网络的方法容易因手动或随机选择元路径引入噪声，而基于大语言模型的方法忽略了学生间的丰富信息，两者都难以提供准确且基于证据的解释。

Method: 构建多关系异构信息网络捕获结构关系，使用LLM智能评分筛选高质量元路径，设计基于元路径的相似学生检索机制，并通过结构化提示整合目标学生历史与相似轨迹。

Result: 在四个公共数据集上的实验表明，HISE-KT在预测性能和可解释性方面均优于现有知识追踪基线方法。

Conclusion: HISE-KT成功融合了异构信息网络和大型语言模型的优势，实现了更准确的知识追踪预测和基于证据的可解释分析。

Abstract: Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.

</details>


### [29] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一个利用不确定性信号检测LLM训练数据中是否包含受版权保护内容的框架，通过将LLM的过度自信转化为优势，实现了无需经验阈值的高精度版权检测。


<details>
  <summary>Details</summary>
Motivation: LLM在大量数据集上训练时可能包含受版权材料，现有成员推理攻击方法因LLM的过度自信、缺乏真实训练数据和依赖经验阈值而面临挑战。

Method: 采用两阶段策略：(1) 将文件分割成小片段以减少对大规模训练数据的依赖；(2) 基于不确定性的无监督聚类，消除对经验调整阈值的需求。

Result: 在LLaMA 7b上达到90.1%的平均平衡准确率，在LLaMA2 7b上达到91.6%，相比现有技术有超过90%的相对提升，最高达到93.8%的平衡准确率。

Conclusion: 这是首次将不确定性应用于LLM版权检测的工作，为训练数据透明度提供了实用工具，并展现出良好的跨架构泛化能力。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [30] [SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making](https://arxiv.org/abs/2511.15202)
*Yinsheng Wang,Tario G You,Léonard Boussioux,Shan Liu*

Main category: cs.AI

TL;DR: SOLID框架通过结合数学优化和大型语言模型，利用对偶价格和偏差惩罚实现迭代协作，在保持模块化和数据隐私的同时提升决策质量。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法缺乏上下文理解能力，而LLMs虽然具有强大的上下文理解能力，但缺乏数学严谨性。SOLID旨在整合两者的优势，实现更智能的决策。

Method: 提出SOLID框架，通过优化代理和LLM代理的迭代协作，使用对偶价格和偏差惩罚机制来协调两者的交互，在凸性假设下保持理论收敛性。

Result: 在股票投资组合案例中，SOLID在各种场景下都表现出收敛性，年化收益率相比仅使用优化器的方法有所提升，验证了两个代理的协同效应。

Conclusion: SOLID为跨领域的自动化和智能决策提供了一个有前景的框架，成功整合了数学优化和LLMs的优势。

Abstract: This paper introduces SOLID (Synergizing Optimization and Large Language Models for Intelligent Decision-Making), a novel framework that integrates mathematical optimization with the contextual capabilities of large language models (LLMs). SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties. This interaction improves the quality of the decisions while maintaining modularity and data privacy. The framework retains theoretical convergence guarantees under convexity assumptions, providing insight into the design of LLMs prompt. To evaluate SOLID, we applied it to a stock portfolio investment case with historical prices and financial news as inputs. Empirical results demonstrate convergence under various scenarios and indicate improved annualized returns compared to a baseline optimizer-only method, validating the synergy of the two agents. SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains.

</details>


### [31] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 论文认为，随着AI向复杂推理发展，仅靠效率提升无法实现可持续性，需要建立明确的限制机制。


<details>
  <summary>Details</summary>
Motivation: AI研究正从模式识别转向复杂问题解决，推理AI缺乏需求饱和点，性能随计算投入指数增长，而效率提升正接近物理极限。

Method: 通过分析推理AI的计算特性，讨论将明确限制嵌入系统优化和治理的研究和政策方向。

Result: 识别出推理AI的可持续性挑战：效率提升有限，但计算需求持续指数增长，缺乏自然饱和机制。

Conclusion: 单靠效率无法实现可持续推理AI，需要在系统优化和治理中嵌入明确限制机制。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [32] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 论文分析了AI研究中的两种智力观：智力现实主义（认为智力是单一通用能力）和智力多元主义（认为智力是多样化的情境依赖能力），并探讨了这两种观点如何影响研究方法、现象解释和风险评估。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中存在关于智力本质的隐含分歧，这些分歧影响研究方法和风险评估，但很少被明确讨论。论文旨在揭示这些基本假设以促进更清晰的学术对话。

Method: 通过分析当前AI研究中的辩论，展示两种智力观如何影响模型选择、基准设计、实验验证、现象解释和风险评估。

Result: 发现两种智力观导致截然不同的研究方法、对相同经验现象的矛盾解读，以及在AI风险上的根本不同评估。

Conclusion: 明确这些基本假设有助于更清晰地理解AI研究中的分歧，促进更富有成效的学术讨论。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [33] [Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration](https://arxiv.org/abs/2511.15351)
*Yifu Guo,Zishan Xu,Zhiyuan Yao,Yuquan Lu,Jiaye Lin,Sen Hu,Zhenheng Tang,Yingchao Li,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: 提出了Octopus：一种具有六种能力协调的多模态推理代理新范式，能够自主探索推理路径并动态选择最合适的能力，在Octopus-Bench基准测试中表现最佳


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型存在架构限制，缺乏人类般的自主探索多样化推理路径的能力，无法适应动态变化的能力需求，而人类在解决此类任务时展现出互补的思维能力

Method: 定义了多模态推理所需的六种核心能力，构建了Octopus-Bench评估基准，Octopus能够自主推理探索并根据当前状态动态选择最合适的能力

Result: 实验结果显示Octopus在Octopus-Bench的大多数任务中取得了最佳性能

Conclusion: 能力协调在代理多模态推理中发挥着关键作用

Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.

</details>


### [34] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova是一个基于《文明V》的综合性挑战环境，旨在同时测试强化学习智能体在多个相互关联的挑战中的综合推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务基准主要评估智能体在不同任务间切换的能力，而缺乏对智能体在多个相互作用的挑战中进行深度推理的测试。

Method: 开发了Terra Nova环境，该环境同时包含部分可观测性、信用分配、表示学习、巨大动作空间等多个经典强化学习挑战。

Result: 创建了一个能够同时测试智能体在多个相互关联挑战中综合推理能力的基准环境。

Conclusion: Terra Nova为评估强化学习智能体在复杂、长期、多变量交互环境中的综合能力提供了新的测试平台。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [35] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: IPR (Interactive Physical Reasoner) 通过结合世界模型推演和VLM策略，在1000+游戏中训练，实现了稳健的物理推理，在三个层次上表现优异，与GPT-5相当并在好奇心方面超越。


<details>
  <summary>Details</summary>
Motivation: 研究智能体是否能像人类一样通过交互学习物理和因果关系，并在更多经验中持续改进。

Method: 提出IPR框架，使用世界模型推演来评分和强化VLM策略，并引入PhysCode（物理中心动作编码）来对齐语义意图与动力学。

Result: 在1000+游戏上预训练后，IPR在三个层次上表现稳健，整体与GPT-5相当，在好奇心方面超越GPT-5。性能随训练游戏和交互步骤增加而提升，并能零样本迁移到未见游戏。

Conclusion: 物理中心的交互是实现持续改进物理推理的有效途径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [36] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 提出了TIM框架，通过DeFi意图分类法和多智能体LLM系统来推断用户交易意图，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: DeFi交易理解困难，现有方法缺乏深度语义洞察，需要解决复杂智能合约交互、多因素影响和不透明日志的问题

Method: 基于扎根理论构建DeFi意图分类法，使用多智能体LLM系统，包括元级规划器动态协调领域专家分解任务，问题求解器处理多模态数据，认知评估器减轻幻觉

Result: TIM在实验中显著优于机器学习模型、单一LLM和单一智能体基线，能够提供更可靠的用户动机理解

Conclusion: TIM框架为复杂区块链活动提供了上下文感知的解释，有助于更可靠地理解DeFi用户动机

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


### [37] [Exploring the use of AI authors and reviewers at Agents4Science](https://arxiv.org/abs/2511.15534)
*Federico Bianchi,Owen Queen,Nitya Thakkar,Eric Sun,James Zou*

Main category: cs.AI

TL;DR: AI agents作为作者和审稿人参与科学会议的首个实验，探讨人机协作在科研中的潜力


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在科学研究中作为科学家和审稿人的能力，以及人机协作在科学领域的可能性

Method: 组织Agents4Science会议，让AI代理担任主要作者和审稿人，人类作为合著者和共同审稿人

Result: 获得了关于AI代理在科学研究和评审中能力的初步见解

Conclusion: 会议经验为人机协作在科学领域的发展提供了重要启示

Abstract: There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.

</details>


### [38] [What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity](https://arxiv.org/abs/2511.15593)
*Alexis Audran-Reiss,Jordi Armengol Estapé,Karen Hambardzumyan,Amar Budhiraja,Martin Josifoski,Edan Toledo,Rishi Hazra,Despoina Magka,Michael Shvartsman,Parth Pathak,Justine T Kao,Lucia Cipolina-Kun,Bhavul Gauri,Jean-Christophe Gagnon-Audet,Emanuel Tewolde,Jenny Zhang,Taco Cohen,Yossi Adi,Tatiana Shavrina,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本文研究了AI研究代理中构思多样性对性能的影响，发现在MLE-bench基准测试中，构思多样性更高的代理表现更好，并通过控制实验验证了这一因果关系。


<details>
  <summary>Details</summary>
Motivation: AI研究代理有望加速科学进步，但成功或失败的关键因素尚未完全理解。本文旨在探究构思多样性在代理性能中的作用。

Method: 首先分析MLE-bench基准测试中不同模型和代理框架的轨迹，然后进行控制实验调整构思多样性程度，最后使用额外评估指标验证结果。

Result: 分析显示不同模型和代理框架产生不同程度的构思多样性，性能更高的代理具有更高的构思多样性。控制实验证实更高的构思多样性确实带来更强的性能。

Conclusion: 构思多样性是影响AI研究代理性能的关键因素，这一发现在多种评估指标下均成立。

Abstract: AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 该论文研究了仅解码器Transformer中从离散提示到最后一个token隐藏状态的映射的注入性，证明了参数空间中存在开放稠密的注入层，并开发了几何诊断方法来实证分析预训练模型的表示空间性质。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer表示空间的数学结构，特别是从离散提示到隐藏状态的映射是否具有注入性，这对于理解模型如何编码信息和潜在的可逆性具有重要意义。

Method: 定义了碰撞判别式和注入层，证明了参数空间的二分性质；开发了分离余量和共Lipschitz常数等几何诊断方法，在LLaMA-3、Qwen和GPT-2等模型上进行实证分析。

Result: 在连续参数理想化下，Transformer表示通常是持续注入的；在8位量化下未观察到碰撞，4位量化会引入少量碰撞并显著缩小共Lipschitz估计；小型GPT-2的归一化指标在训练中保持稳定。

Conclusion: Transformer表示在连续参数理想化下通常是持续注入的，其实际可逆性可以通过简单的几何诊断方法来探测。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [40] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: 本文提出推导关系(DR)和推导能力(DC)的概念，评估LLMs在数据变化时的推理能力，并开发DEVAL评估框架和推导提示(DP)方法。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在数据上的推理能力是一个开放且紧迫的研究问题。相比人类能够根据输入变化推导出相应输出修改，LLMs的这种基于抽象规则的推理模式尚未得到全面描述和评估。

Method: 正式定义推导关系(DR)和推导能力(DC)，提出系统构建的评估框架DEVAL，并在7个主流任务中评估5个流行LLMs和1个大推理模型。提出推导提示(DP)的提示工程方法。

Result: 主流LLMs如GPT-4o和Claude3.5表现出中等DR识别能力，但在问题解决场景中应用DR时显著下降。推导提示(DP)使所有测试LLMs的DC平均提高15.2%，优于常用提示工程技术。

Conclusion: LLMs在推导能力方面存在显著差距，但通过推导提示方法可以有效提升其推导能力，为改进LLMs的推理能力提供了有效途径。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [41] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 提出动态嵌套层次结构，使模型能够自主调整优化层级、嵌套结构和更新频率，解决现有模型在非平稳环境中的适应性问题，实现真正的终身学习。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型（包括大语言模型）在静态任务中表现出色，但在非平稳环境中表现不佳，因为其刚性架构阻碍了持续适应和终身学习。

Method: 基于嵌套学习范式，构建动态嵌套层次结构，允许模型在训练或推理过程中自主调整优化层级数量、嵌套结构和更新频率，受神经可塑性启发实现无预定义约束的自我进化。

Result: 通过严格的数学公式、收敛性理论证明、表达能力边界和不同机制下的次线性遗憾分析，以及在语言建模、持续学习和长上下文推理中的实证演示，展示了优越性能。

Conclusion: 动态嵌套层次结构为实现自适应、通用智能奠定了基础性进展，解决了现有模型的顺行性遗忘问题，通过动态压缩上下文流和适应分布变化促进真正的终身学习。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [42] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: 提出了GTPO算法，通过回合级奖励分配、基于回报的优势估计和自监督奖励塑造，解决了现有RL方法在多回合工具集成推理任务中训练信号不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在多回合工具集成推理任务中使用轨迹级奖励，学习信号过于粗糙，导致训练停滞，需要更细粒度的训练方法

Method: GTPO算法包含三个创新：回合级奖励分配、基于回报的优势估计、自监督奖励塑造，提供更精细的训练信号

Result: 在多样化推理基准测试中，GTPO比GRPO平均提升3.0%的性能

Conclusion: GTPO有效提升了LLM在复杂多回合工具集成推理任务中的表现，为现实世界复杂数学推理提供了有效解决方案

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [43] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: FinTRec是一个基于Transformer的金融推荐框架，解决了金融服务中长序列交互、多产品协调等挑战，相比传统树模型在性能和效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 金融服务中的推荐系统面临独特挑战：长序列用户交互（数字和实体渠道）、多产品协调需求，而传统树模型虽然可解释性强但性能有限。

Method: 提出FinTRec框架，采用Transformer架构处理长序列交互，通过统一架构支持多产品推荐，实现跨产品信号共享。

Result: 通过历史模拟和在线A/B测试，FinTRec持续优于生产级树模型基线，统一架构减少了训练成本和技术债务，同时提升了所有产品的离线性能。

Conclusion: FinTRec证明了Transformer架构在金融服务推荐中的可行性和有效性，是首个全面解决技术和业务考量的统一序列推荐模型研究。

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [44] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: 提出基于Transformer引导的深度强化学习方法，用于优化电动垂直起降飞机的起飞轨迹以最小化能耗，相比传统DRL训练效率提升75%，能耗优化精度达97.2%。


<details>
  <summary>Details</summary>
Motivation: 传统最优控制方法受限于问题维度和复杂性，而深度强化学习虽然能处理复杂非线性系统但训练困难。需要开发更高效的训练方法来优化eVTOL飞机的起飞轨迹。

Method: 提出Transformer引导的深度强化学习方法，使用Transformer在每个时间步探索现实状态空间，通过调整功率和机翼角度来控制eVTOL无人机的起飞轨迹。

Result: Transformer引导的DRL仅需457万时间步完成训练，比传统DRL的1979万时间步减少75%。在能耗优化方面达到97.2%的精度，优于传统DRL的96.3%。

Conclusion: Transformer引导的DRL在训练效率和最优设计验证方面均优于传统DRL，为eVTOL飞机能量优化提供了更有效的解决方案。

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [45] [Bringing Federated Learning to Space](https://arxiv.org/abs/2511.14889)
*Grace Kim,Filip Svoboda,Nicholas Lane*

Main category: cs.LG

TL;DR: 该论文首次系统分析了在卫星星座中部署联邦学习的可行性，提出了将地面FL算法适应太空环境的框架，并在768种星座配置下验证了太空适应FL算法可扩展到100颗卫星，性能接近集中式理想情况，训练周期可从数月缩短至数天。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星星座规模迅速扩大至数百上千颗，下行带宽限制使得分布式星载机器学习变得至关重要。联邦学习为卫星网络中的协作模型训练提供了有前景的框架，但需要解决太空特有的约束条件。

Method: 提出了全面的"太空化"框架，将地面FL算法（FedAvg、FedProx、FedBuff）适应轨道约束，生成轨道就绪的FL算法套件。通过768种星座配置的广泛参数扫描进行评估，涵盖集群大小、每集群卫星数和地面站网络的变化。

Result: 太空适应的FL算法可有效扩展到100颗卫星的星座，性能接近集中式理想情况。通过轨道调度和卫星集群内的本地协调，多月的训练周期可缩短至数天，实现9倍加速。

Conclusion: 研究结果为未来任务设计者提供了可行的见解，使分布式星载学习能够实现更自主、弹性和数据驱动的卫星操作。

Abstract: As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive "space-ification" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.

</details>


### [46] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: 提出了LIT框架，通过强制LLM使用外部可靠工具来解决复杂问题，提高解决方案的可信度和可调试性


<details>
  <summary>Details</summary>
Motivation: LLM推理过程不透明，在高风险领域难以信任，且可能选择不可靠的解决方案，即使有更好的替代方案

Method: 基于现有LLM的工具调用能力构建框架，让LLM选择最可靠且易于调试的解决方案路径，可能涉及多个顺序工具调用

Result: LLM能够实现更可靠和明智的问题解决，同时保持任务性能

Conclusion: LIT框架通过强制使用可靠工具显著提升了LLM解决方案的可信度和可调试性

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [47] [Structured Contrastive Learning for Interpretable Latent Representations](https://arxiv.org/abs/2511.14920)
*Zhengyang Shen,Hua Tu,Mayue Shi*

Main category: cs.LG

TL;DR: 提出结构化对比学习(SCL)框架，通过将潜在空间划分为不变特征、变异特征和自由特征三个语义组，解决神经网络对语义无关变换的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络对语义无关变换(如ECG相位偏移、IMU传感器旋转)表现出严重脆弱性，导致性能急剧下降。根本原因是"放任式"表示学习，只要任务性能满足，潜在空间演化不受约束。

Method: 结构化对比学习(SCL)：1) 不变特征：在给定变换下保持一致性；2) 变异特征：通过新颖的变异机制主动区分变换；3) 自由特征：保持任务灵活性。无需架构修改，可无缝集成到现有训练流程。

Result: ECG相位偏移下相似度从0.25提升到0.91；WISDM活动识别达到86.65%准确率和95.38%旋转一致性，始终优于传统数据增强方法。

Conclusion: 从反应式数据增强转向主动结构化学习，实现神经网络中可解释的潜在表示，为表示学习提供了新范式。

Abstract: Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as "laissez-faire" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.

</details>


### [48] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: 提出了Causal-GCN框架，通过do-calculus后门调整来识别对阿尔茨海默病进展具有稳定因果影响的大脑区域，解决了传统图学习方法中混淆变量的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度图学习方法在阿尔茨海默病分类中主要关注相关性，但会混淆人口统计学和遗传因素与疾病特异性特征，需要因果推断方法来识别真正的因果影响。

Method: 使用结构连接体表示MRI数据，节点为皮质和皮质下区域，边编码解剖连接性。集成do-calculus后门调整，将年龄、性别和APOE4基因型等混杂因素纳入因果调整集，通过干预个体区域模拟来估计对疾病概率的平均因果效应。

Result: 在ADNI队列的484名受试者中，Causal-GCN实现了与基线GNN相当的性能，同时提供了可解释的因果效应排名，突出了后部、扣带回和岛叶枢纽，与已建立的AD神经病理学一致。

Conclusion: Causal-GCN框架能够有效识别对阿尔茨海默病进展具有因果影响的大脑区域，为神经退行性疾病的机制理解提供了因果解释工具。

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [49] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: 该论文首次系统比较了四种用于医院出院总结自动诊断编码的隐私保护训练方法，发现知识蒸馏在中等隐私预算下表现最佳，能恢复63%的非私有性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床文本训练中存在泄露敏感患者信息的风险，但差分隐私方法通常会严重降低诊断准确性，需要找到有效的隐私保护策略。

Method: 使用相同的10亿参数模型和匹配的隐私预算，比较了四种训练流程：直接DP-SGD、DP合成数据训练、知识蒸馏等，用于预测ICD-9代码。

Result: 在中等隐私预算下，知识蒸馏方法表现最好，能恢复63%的非私有性能，同时保持强大的经验隐私保护。

Conclusion: 知识蒸馏是实现隐私保护临床NLP的最实用途径，不同架构在隐私-效用权衡上存在显著差异。

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [50] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: Graph Memory (GM) 是一个结构化非参数框架，通过区域级原型的关系记忆增强基于嵌入的推理，在保持高精度的同时显著提升校准性和决策边界平滑度。


<details>
  <summary>Details</summary>
Motivation: 传统方法单独处理每个训练实例，无法有效利用嵌入空间的整体结构和关系信息。GM旨在通过原型节点和关系边来统一实例检索、原型推理和图标签传播。

Method: 将嵌入空间总结为带有可靠性指标的原型节点，通过编码几何和上下文关系的边连接，构建紧凑的关系记忆框架。

Result: 在合成和真实数据集（包括乳腺组织病理学IDC）上的实验表明，GM在精度上与kNN和Label Spreading相当，但校准性更好、决策边界更平滑，且所需样本数量少一个数量级。

Conclusion: GM通过显式建模可靠性和关系结构，为非参数学习中局部证据与全局一致性之间提供了原则性的桥梁。

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [51] [IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics](https://arxiv.org/abs/2511.15004)
*Halil S. Kelebek,Linnea M. Wolniewicz,Michael D. Vergalla,Simone Mestici,Giacomo Acciarini,Bala Poduval,Olga Verkhoglyadova,Madhulika Guhathakurta,Thomas E. Berger,Frank Soboczenski,Atılım Güneş Baydin*

Main category: cs.LG

TL;DR: IonCast是一个基于深度学习的电离层预测模型套件，通过图神经网络和时空学习来预测全球总电子含量，在风暴期和宁静期都表现出比持续性预测更好的性能。


<details>
  <summary>Details</summary>
Motivation: 电离层对GNSS精度、高频通信和航空运营至关重要，需要准确预测和建模电离层变异性来增强空间天气韧性。

Method: 采用受GraphCast启发的图神经网络模型，整合多种物理驱动因素和观测数据集，利用基于图的时空学习来预测全球TEC。

Result: 在风暴期和宁静期的验证测试中，IonCast相比持续性预测显示出改进的技能表现。

Conclusion: 通过将异构数据与可扩展的基于图的时空学习相结合，IonCast展示了机器学习如何增强对电离层变异性的物理理解并推进空间天气韧性。

Abstract: The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.

</details>


### [52] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 开发了一个模拟教室环境的动态时间序列系统，结合强化学习智能辅导系统，通过探测性干预来平衡学生状态估计和教学干扰，比较了RL算法与启发式方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统需要处理每个学生的独特性，且学习过程只能部分观测，因此需要开发能够结合个体状态和群体信息的动态教学环境。

Method: 创建模拟教室环境，设计探测性干预机制，开发结合个体状态学习和群体信息的强化学习ITS，并与基于规则的启发式方法进行比较。

Result: RL算法和启发式方法提供不同解决方案但效果相似；探测性干预能显著提升隐藏信息情况下的性能；两种方法对变化的学生群体分布都有适应性，但RL在困难班级中表现较差；在测验和期中考试结构中比期末结构中表现更好。

Conclusion: 探测性干预在智能辅导系统中具有重要价值，能够平衡信息获取和教学干扰，但需要根据具体课程结构设计合适的干预策略。

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [53] [Oversampling techniques for predicting COVID-19 patient length of stay](https://arxiv.org/abs/2511.15048)
*Zachariah Farahany,Jiawei Wu,K M Sajjadul Islam,Praveen Madiraju*

Main category: cs.LG

TL;DR: 使用电子健康记录和人工神经网络预测COVID-19患者住院时间长度，通过贝叶斯优化调参和过采样处理数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: COVID-19症状严重程度差异大，需要基于患者电子健康记录预测疾病严重程度，以住院时间作为严重性指标。

Method: 使用过采样技术处理数据不平衡问题，应用人工神经网络模型，并通过贝叶斯优化调整超参数。

Result: 选择具有最佳F1分数的模型进行评估和讨论。

Conclusion: 该方法能够有效预测COVID-19患者的疾病严重程度，为临床决策提供支持。

Abstract: COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.

</details>


### [54] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 提出了一种用于心律失常检测和分类的临床决策支持系统框架，通过局部和全局信息提取与融合，解决了心律失常长度变化的问题，在MIT-BIH数据库上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 由于心律失常的发作时间和持续时间变化很大，现有的临床决策支持系统在处理不同长度的心律失常时面临挑战，需要开发能够准确检测心律失常发作和结束时间的方法。

Method: 提出了包含局部和全局信息提取以及基于注意力的局部-全局信息融合的框架，能够在受限输入长度内进行心律失常检测和分类。

Result: 在MIT-BIH心律失常数据库上，持续时间、发作和Dice分数的F1得分分别为96.45%、82.05%和96.31%；在心房颤动数据库上分别为97.57%、98.31%和97.45%，性能显著优于基准模型。

Conclusion: 该方法能有效捕捉局部和全局信息及动态变化，准确检测心律失常并精确定位发生时间，有助于临床制定更精确的治疗方案。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [55] [Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer](https://arxiv.org/abs/2511.15067)
*Zisong Wang,Xuanyu Wang,Hang Chen,Haizhou Wang,Yuxin Chen,Yihang Xu,Yunhe Yuan,Lihuan Luo,Xitong Ling,Xiaoping Liu*

Main category: cs.LG

TL;DR: 开发并验证了基于病理全切片图像的TDAM-CRC多实例学习模型，用于结直肠癌的精准预后预测，通过多组学分析揭示了代谢重编程和免疫抑制微环境机制，并识别出MRPL37作为关键预后生物标志物。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌具有高度异质性，传统的TNM分期系统无法满足个性化医疗需求，需要开发更精准的预后分层工具。

Method: 使用多实例学习模型TDAM-CRC分析病理全切片图像，在TCGA发现队列(n=581)训练，在独立外部队列(n=1031)验证，整合多组学数据提高模型可解释性。

Result: TDAM-CRC在两个队列中均实现稳健的风险分层，预测性能显著优于传统临床分期系统和现有最优模型。高风险亚型与代谢重编程和免疫抑制微环境相关，MRPL37被确认为关键预后生物标志物。

Conclusion: TDAM-CRC模型为结直肠癌提供了改进的风险分层工具，揭示了新的分子靶点，并促进了个性化临床决策。

Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.

</details>


### [56] [Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection](https://arxiv.org/abs/2511.15083)
*Xiancheng Wang,Lin Wang,Rui Wang,Zhibo Zhang,Minghang Zhao*

Main category: cs.LG

TL;DR: 提出Fourier-KAN-Mamba混合架构，结合傅里叶层、KAN网络和Mamba状态空间模型，用于时间序列异常检测，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在长序列建模中表现出色，但直接应用于异常检测任务时难以捕捉复杂的时间模式和非线性动态特征。

Method: 集成傅里叶层提取多尺度频率特征，KAN网络增强非线性表示能力，并引入时间门控机制来更好地区分正常和异常模式。

Result: 在MSL、SMAP和SWaT数据集上的大量实验表明，该方法显著优于现有的最先进方法。

Conclusion: Fourier-KAN-Mamba混合架构有效解决了时间序列异常检测中的复杂模式捕捉问题，展现出优越性能。

Abstract: Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches.
  Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network

</details>


### [57] [Semiconductor Industry Trend Prediction with Event Intervention Based on LSTM Model in Sentiment-Enhanced Time Series Data](https://arxiv.org/abs/2511.15112)
*Wei-hsiang Yen,Lyn Chao-ling Chen*

Main category: cs.LG

TL;DR: 该研究将深度学习方法与情感分析整合到传统商业模式分析中，以台积电为研究对象预测台湾半导体行业趋势。通过分析台积电季度报告中的文本和时间序列数据，结合内外事件的情感分析，使用LSTM模型进行行业趋势预测。


<details>
  <summary>Details</summary>
Motivation: 半导体行业市场变化快速，传统数据分析方法在处理高变化性和时间序列数据时表现不佳。需要更有效的方法来预测行业趋势，考虑内部公司事件和外部全球事件的影响。

Method: 收集台积电季度报告中的文本和时间序列数据，通过情感分析考虑公司内部事件和外部全球事件的干预，使用情感增强的时间序列数据，采用LSTM模型进行行业趋势预测。

Result: 预测结果揭示了台积电晶圆技术的显著发展及全球市场的潜在威胁，与台积电产品发布新闻和国际新闻相符。

Conclusion: 该工作通过考虑内外事件干预，在半导体行业趋势预测方面取得了准确结果，为研究和商业领域提供了有价值的半导体行业信息。

Abstract: The innovation of the study is that the deep learning method and sentiment analysis are integrated in traditional business model analysis and forecasting, and the research subject is TSMC for industry trend prediction of semiconductor industry in Taiwan. For the rapid market changes and development of wafer technologies of semiconductor industry, traditional data analysis methods not perform well in the high variety and time series data. Textual data and time series data were collected from seasonal reports of TSMC including financial information. Textual data through sentiment analysis by considering the event intervention both from internal events of the company and the external global events. Using the sentiment-enhanced time series data, the LSTM model was adopted for predicting industry trend of TSMC. The prediction results reveal significant development of wafer technology of TSMC and the potential threatens in the global market, and matches the product released news of TSMC and the international news. The contribution of the work performed accurately in industry trend prediction of the semiconductor industry by considering both the internal and external event intervention, and the prediction results provide valuable information of semiconductor industry both in research and business aspects.

</details>


### [58] [Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling](https://arxiv.org/abs/2511.15125)
*Huifan Zhang,Pingqiang Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯在线学习的不确定性感知框架，用于高效参数化建模RF无源元件，相比传统机器学习方法仅需2.86%的电磁仿真时间，实现35倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统基于机器学习的RF无源元件建模需要大量电磁仿真来覆盖几何和频率设计空间，造成计算瓶颈。

Method: 1) 具有可重构头部的贝叶斯神经网络，用于联合几何-频率域建模并量化不确定性；2) 自适应采样策略，利用不确定性指导同时优化几何参数和频率域的训练数据采样。

Result: 在三个RF无源组件上验证，该框架实现了准确建模，相比传统基于ML的流程仅使用2.86%的电磁仿真时间，达到35倍加速。

Conclusion: 该不确定性感知贝叶斯在线学习框架显著提高了RF无源元件参数化建模的效率，大幅减少了计算成本。

Abstract: Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.

</details>


### [59] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 提出了一种新的稀疏矩阵乘法算法，使能够对整个Medline数据集应用自组织映射，从而更完整地映射现有医学知识。


<details>
  <summary>Details</summary>
Motivation: 由于现有算法对内存和处理需求呈指数级增长，过去对Medline数据库的映射工作仅限于可用数据的小子集。

Method: 设计了一种用于稀疏矩阵乘法的新算法，将该算法应用于整个Medline数据集的自组织映射。

Result: 实现了对整个Medline数据集的更完整映射，提高了随时间更新自组织映射的可行性。

Conclusion: 新算法解决了大规模医学知识映射的计算瓶颈问题，为动态更新医学知识图谱提供了可行方案。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [60] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: 提出了GRPO-Verif算法，通过统一损失函数联合优化LLM的解决方案生成和自我验证能力，实验证明该方法在保持推理性能的同时增强了自我验证能力。


<details>
  <summary>Details</summary>
Motivation: 尽管通过强化学习显著提升了大型语言模型的推理能力，但它们仍然难以一致地验证自己的推理过程，因此需要研究如何增强LLM的自我验证能力以及这种能力是否能进一步改善推理性能。

Method: 提出GRPO-Verif算法，在统一损失函数中联合优化解决方案生成和自我验证，通过可调超参数控制验证信号的权重。

Result: 实验结果表明，该方法在保持推理性能的同时增强了自我验证能力。

Conclusion: GRPO-Verif算法能够有效提升LLM的自我验证能力，为改善推理性能提供了新途径。

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [61] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 提出一种不确定性感知的主动学习框架，通过联合利用模型不确定性和跨模态一致性来增强对标签噪声的鲁棒性，用于EEG情感识别。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪影和个体差异影响，情感标签通常来自主观且不一致的报告，使得稳健的情感解码特别困难。需要一种能够处理标签噪声的数据高效方法。

Method: 使用跨模态对齐评估不确定性来源（认知模糊或传感器噪声），将EEG和面部特征嵌入共享潜在空间，通过主动学习选择性查询噪声样本获取反馈。

Result: 在ASCERTAIN数据集上的实验验证了该方法的效率和鲁棒性。

Conclusion: 该方法为脑机接口系统中的EEG情感解码提供了一种数据高效且噪声容忍的解决方案。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [62] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 该论文研究了复数变分自编码器中出现的Kähler几何结构，提出了基于复数高斯混合的Kähler势导数方法，能够高效计算Fisher信息度量，并通过潜在空间的正则化获得更平滑的表示和更少的语义异常值。


<details>
  <summary>Details</summary>
Motivation: 研究复数变分自编码器中Kähler几何结构的出现，旨在开发高效的度量计算方法并改善潜在空间表示的质量。

Method: 采用复数变分自编码器，推导复数情况下的Fisher信息度量，提出基于复数高斯混合的Kähler势导数方法，通过潜在空间正则化和加权复数体积元素采样。

Result: 提出的方法能够高效计算Fisher信息度量，通过潜在空间正则化获得更平滑的表示和更少的语义异常值，同时减轻了自动微分的大规模计算负担。

Conclusion: 复数变分自编码器确实展现出Kähler几何结构，提出的Kähler势导数方法在保持几何忠实性的同时实现了高效计算，潜在空间正则化策略有效改善了表示质量。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [63] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 提出基于扩散模型的少样本故障时间序列生成框架，通过正负差异适配器和多样性损失解决故障数据稀缺问题，在真实性和多样性方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业设备监测中故障诊断对系统可靠性至关重要，但故障数据稀缺（由于故障事件罕见和数据标注成本高）严重阻碍了数据驱动方法的发展。现有时间序列生成模型在少样本场景下难以捕捉故障分布，生成的样本缺乏真实性和多样性。

Method: 基于扩散模型的少样本故障时间序列生成框架，使用正负差异适配器利用预训练的正常数据分布来建模正常与故障域之间的差异，实现准确的故障合成；引入多样性损失防止模式崩溃，通过样本间差异正则化鼓励生成多样化故障样本。

Result: 实验结果表明，该模型在真实性和多样性方面显著优于传统方法，在关键基准测试中实现了最先进的性能。

Conclusion: 提出的基于扩散模型的少样本故障时间序列生成框架有效解决了故障数据稀缺问题，能够生成真实且多样的故障样本，为工业设备故障诊断提供了可靠的数据增强解决方案。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [64] [Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning](https://arxiv.org/abs/2511.15175)
*Le Tung Giang,Vu Hoang Viet,Nguyen Xuan Tung,Trinh Van Chien,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种量子图注意力网络(Q-GAT)，在深度强化学习框架中用参数化量子电路替代传统MLP，用于解决车辆路径问题，减少了50%以上可训练参数，在基准测试中实现了更快收敛和约5%的路由成本降低。


<details>
  <summary>Details</summary>
Motivation: 传统基于图神经网络的深度强化学习方法依赖参数繁多的多层感知机，存在参数过多和内存受限的问题，需要开发更紧凑高效的模型。

Method: 在DRL框架中构建量子图注意力网络，用参数化量子电路替代关键读出阶段的传统MLP，保持图注意力编码器表达能力的同时减少参数，采用近端策略优化和贪心/随机解码。

Result: 在VRP基准测试中，Q-GAT相比经典GAT基线实现了更快收敛，路由成本降低约5%，可训练参数减少超过50%。

Conclusion: 参数化量子电路增强的图神经网络可作为大规模路由和物流优化的紧凑有效求解器，展示了量子-经典混合模型的潜力。

Abstract: The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.

</details>


### [65] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVAL是一个蒸馏框架，将掩码自回归扩散模型的扩散链压缩为单步生成，实现30倍加速并保持样本质量，同时支持强化学习后训练。


<details>
  <summary>Details</summary>
Motivation: 解决掩码自回归扩散模型推理速度慢的问题，其分层推理机制（外部AR解掩码循环和内部扩散去噪链）不仅影响生成效率，还阻碍强化学习后训练的实际应用。

Method: 提出基于分数的变分目标，将掩码自回归扩散模型蒸馏为单步生成；开发MARVAL-RL高效强化学习框架。

Result: 在ImageNet 256*256上，MARVAL-Huge达到FID 2.00，相比MAR-diffusion加速30倍以上；MARVAL-RL在CLIP和图像奖励分数上持续提升。

Conclusion: MARVAL展示了掩码自回归扩散模型蒸馏和强化学习的首个实用路径，实现快速采样和更好的偏好对齐。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [66] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: 提出了ATPO方法，通过分析扩散大语言模型推理轨迹中的不确定性模式，动态选择关键步骤进行梯度更新，显著提升了推理准确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的强化学习方法将所有去噪步骤视为同等重要，均匀分配策略梯度。作者挑战这一假设，认为推理轨迹中存在结构化的"困惑区域"，这些关键步骤对最终成功或失败具有强预测性。

Method: 提出了自适应轨迹策略优化(ATPO)，使用混合RoEC+CM规则动态识别轨迹中的高影响力步骤，重新分配梯度更新到这些关键步骤，而不改变RL目标、奖励或计算预算。

Result: ATPO在多个基准测试中显著提升了推理准确性和训练稳定性，证明了利用轨迹动态特性对于推进dLLM RL的重要性。

Conclusion: 通过分析轨迹动态并针对性优化关键步骤，可以更有效地训练扩散大语言模型，为复杂推理任务提供了新的优化思路。

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [67] [D2D Power Allocation via Quantum Graph Neural Network](https://arxiv.org/abs/2511.15246)
*Tung Giang Le,Xuan Tung Nguyen,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种完全量子图神经网络(QGNN)，使用参数化量子电路实现消息传递，在无线网络资源管理中实现可扩展的量子加速优化。


<details>
  <summary>Details</summary>
Motivation: 无线网络复杂度增加需要可扩展的资源管理方法，经典图神经网络在大型场景中计算成本高，量子方法可以提供更高效的解决方案。

Method: 使用量子图卷积层(QGCLs)，将特征编码为量子态，通过NISQ兼容的酉变换处理图结构，并通过测量获取嵌入表示。

Result: 在D2D功率控制SINR最大化任务中，QGNN与经典方法性能相当，但参数更少且具有固有并行性。

Conclusion: 基于PQC的端到端QGNN标志着量子加速无线优化的重要进展。

Abstract: Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.

</details>


### [68] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 提出了EntroPIC方法，通过比例-积分控制动态调整正负样本的损失系数，稳定大语言模型强化学习训练中的熵值，防止模型陷入次优行为。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以在训练过程中维持适当的熵水平，因为正负样本在不同步骤中对熵的影响方式不同，导致探索不稳定和模型过早收敛到次优解。

Method: EntroPIC方法使用比例-积分控制机制，自适应地调整正负样本的损失系数，从而在整个训练过程中稳定熵值。该方法适用于在线和离线学习设置。

Result: 实验结果表明，EntroPIC能够成功维持期望的熵水平，为大语言模型提供稳定且最优的强化学习训练。

Conclusion: EntroPIC通过熵稳定化机制有效解决了大语言模型长期训练中的探索稳定性问题，确保了训练过程的稳定性和收敛到最优解。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [69] [Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling](https://arxiv.org/abs/2511.15250)
*Jin Ye,Lingmei Wang,Shujian Zhang,Haihang WU*

Main category: cs.LG

TL;DR: 提出基于改进PVTD3算法的电热联合系统智能调度方法，在可再生能源接入和多不确定性条件下优化系统运行，显著降低综合成本和电网购电波动。


<details>
  <summary>Details</summary>
Motivation: 随着全球能源转型和可再生能源快速发展，电热联合系统在新能源接入和多不确定性条件下的调度优化挑战日益突出。

Method: 采用改进的双延迟深度确定性策略梯度(PVTD3)算法，通过引入电网购电变化惩罚项实现系统优化。

Result: 在10%、20%、30%可再生能源渗透率场景下，PVTD3算法相比传统TD3分别降低综合成本6.93%、12.68%、13.59%，平均购电波动幅度降低12.8%，低温储热罐终值降低7.67-17.67单位，高温储热罐维持在3.59-4.25安全范围。

Conclusion: 所提算法在经济性、电网稳定性和储能设备可持续调度能力方面均表现优异。

Abstract: With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.

</details>


### [70] [PLATONT: Learning a Platonic Representation for Unified Network Tomography](https://arxiv.org/abs/2511.15251)
*Chengze Du,Heng Xu,Zhiwei Yu,Bo Liu,Jialong Li*

Main category: cs.LG

TL;DR: PLATONT是一个统一的网络断层扫描框架，通过将不同网络指标建模为共享潜在网络状态的投影，实现多任务学习。该方法基于柏拉图表示假设，通过多模态对齐和对比学习构建结构化表示，在链路估计、拓扑推断和流量预测等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有网络断层扫描方法通常单独解决不同问题，依赖有限的任务特定信号，这限制了泛化能力和可解释性。需要一种统一框架来建模共享的网络状态。

Method: PLATONT将不同网络指标（延迟、丢包、带宽等）建模为共享潜在网络状态的投影，通过多模态对齐和对比学习学习潜在状态，在共享潜在空间中训练多个断层扫描任务。

Result: 在合成和真实数据集上的实验表明，PLATONT在链路估计、拓扑推断和流量预测方面持续优于现有方法，在不同网络条件下实现更高准确性和更强鲁棒性。

Conclusion: PLATONT通过构建紧凑的结构化表示，实现了跨任务泛化能力的提升，为网络断层扫描提供了一个统一且有效的解决方案。

Abstract: Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.

</details>


### [71] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 提出了GRPO-RM方法，将GRPO强化学习技术从大语言模型扩展到表示学习模型，通过预定义输出集和专门奖励函数来优化表示模型。


<details>
  <summary>Details</summary>
Motivation: 探索GRPO方法是否能从大语言模型泛化到表示学习模型，扩展强化学习在表示模型后训练中的应用。

Method: 建立预定义输出集替代LLM中的token序列采样，生成输出组用于GRPO的概率驱动优化，并设计专门适应表示模型特性的奖励函数。

Result: 在多个真实世界数据集上进行广泛实验，验证了所提方法的有效性。

Conclusion: GRPO-RM成功将GRPO技术应用于表示模型，为表示学习模型的强化学习优化提供了可行方案。

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [72] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: SNAP是一个稀疏测试时适应框架，通过减少适应频率和数据使用量，在保持精度的同时显著降低计算成本，特别适合资源受限的边缘环境。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法需要频繁适应和高计算成本，不适合资源受限的边缘环境。需要开发能够在低频率更新下保持性能的轻量级TTA方法。

Method: 提出两个关键组件：1) CnDRM - 识别并存储少量同时代表类别和域特征的样本；2) IoBMN - 在推理时利用代表性样本动态调整归一化统计量。

Result: 仅使用1%的数据流进行适应仍能保持竞争力精度，延迟降低高达93.12%，精度下降低于3.3%，适应率范围1%-50%均表现良好。

Conclusion: SNAP在边缘设备上具有实际应用潜力，特别适合延迟敏感的应用场景，为资源受限环境提供了高效的测试时适应解决方案。

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [73] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: Quant-Trim是一种训练阶段方法，通过渐进式伪量化和反向剪枝，生成对后端和精度选择具有鲁棒性的硬件无关检查点，解决边缘加速器中低比特量化在不同厂商编译器下的精度不一致问题。


<details>
  <summary>Details</summary>
Motivation: 专业边缘加速器依赖低比特量化，但不同厂商编译器在缩放、裁剪和内核支持方面存在差异，导致相同的浮点检查点在不同后端产生不一致的精度，迫使开发者调整参数或重构模型以适应厂商友好的算子子集。

Method: 结合渐进式伪量化以对齐训练与部署的整数网格，以及反向剪枝来抑制异常值驱动的尺度膨胀，同时保持可学习性。该方法与量化方案无关，无需厂商特定的图修改。

Result: 在模型和任务中，Quant-Trim缩小了浮点与低比特之间的差距，减少了对编译器启发式/校准的依赖，避免了针对每个后端的重新训练。报告了在静态/动态激活缩放和不同算子覆盖下的精度、延迟、吞吐量、能耗/推理和成本等边缘指标。

Conclusion: Quant-Trim提供了一种硬件中立的训练方法，能够生成对后端和精度选择具有鲁棒性的检查点，解决了边缘加速器中量化不一致的问题。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [74] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型的概念可解释性，发现早期层主要编码局部时域模式，深层编码离散度和变化时间信号，但线性可恢复性较差，且概念组合存在干扰。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在经验上取得成功，但其内部如何表示基本时间序列概念的机制仍不清楚，需要系统研究其概念可解释性。

Method: 使用分层分析、线性可恢复性测试和表示相似性度量，系统探究概念在不同层的编码位置、线性可恢复性、表示演化和概念组合处理。

Result: 早期层主要捕获局部时域模式（如AR(1)、水平偏移、趋势），深层编码离散度和变化时间信号，频谱和扭曲因子最难线性恢复，概念组合时性能下降。

Conclusion: 虽然原子概念能可靠定位，但概念组合仍是挑战，这揭示了当前TSFMs在表示交互时间现象能力上的关键局限。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [75] [STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection](https://arxiv.org/abs/2511.15339)
*Kadir-Kaan Özer,René Ebeling,Markus Enzweiler*

Main category: cs.LG

TL;DR: STREAM-VAE是一种用于汽车遥测时间序列异常检测的变分自编码器，通过双路径编码器分离慢速漂移和快速尖峰信号动态，提高异常检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 汽车遥测数据同时存在缓慢漂移和快速尖峰，传统基于重构的方法使用单一潜在过程会混合不同时间尺度，导致尖峰被平滑或方差膨胀，削弱异常分离能力。

Method: 使用双路径编码器分离慢速漂移和快速尖峰信号动态，解码器将瞬时偏差与正常操作模式分开表示，专为部署设计，可在不同操作模式下产生稳定的异常分数。

Result: 在汽车遥测数据集和公开SMD基准测试上的实验表明，显式分离漂移和尖峰动态相比强基线方法（预测、注意力、图和VAE）提高了鲁棒性。

Conclusion: STREAM-VAE通过分离不同时间尺度的信号动态，有效提升了汽车遥测数据异常检测的性能和部署稳定性。

Abstract: Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.
  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.
  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.

</details>


### [76] [KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials](https://arxiv.org/abs/2511.15327)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: KrawtchoukNet是一种基于离散Krawtchouk多项式的谱图神经网络滤波器，解决了传统多项式滤波器在异质图和高多项式度数下的性能崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 传统谱图神经网络（如ChebyNet）存在两个关键限制：1）在异质图上性能崩溃；2）在高多项式度数时出现过度平滑问题。这些问题源于标准滤波器的静态低通特性。

Method: 提出KrawtchoukNet，通过两个关键设计：1）将多项式域N固定为小常数，创建首个具有固有有界递推系数的GNN滤波器；2）使滤波器的形状参数p可学习，使滤波器能自适应图数据的频谱响应。

Result: KrawtchoukNet在K=10时达到最先进的抗过度平滑效果，并在具有挑战性的异质图基准（Texas、Cornell）上显著优于GAT和APPNP等标准GNN。

Conclusion: KrawtchoukNet通过有界递推系数和可学习形状参数，为谱图神经网络在异质图和抗过度平滑方面提供了统一的解决方案。

Abstract: Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on "heterophilic" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.

</details>


### [77] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: PIECE是一种基于参数重要性估计的持续增强方法，通过仅更新0.1%的核心参数来防止灾难性遗忘，同时保持基础模型的通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 领域特定微调通常会导致灾难性遗忘，使基础模型失去通用推理能力。传统持续学习方法存在下游性能差、依赖历史数据或参数开销大的问题。

Method: PIECE使用两种重要性估计器（基于Fisher信息的PIECE-F和基于二阶归一化的PIECE-S）来指导仅更新0.1%与新任务最相关的核心参数，无需访问先前训练数据或增加模型参数。

Result: 在三个语言模型和两个多模态模型上的实验表明，PIECE能保持通用能力，并在各种下游任务中实现最先进的持续学习性能。

Conclusion: PIECE为构建可扩展、领域自适应的基础模型提供了一条实用路径，避免了灾难性遗忘问题。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [78] [LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials](https://arxiv.org/abs/2511.15328)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出了LaguerreNet，一种基于连续拉盖尔多项式的新型GNN滤波器，通过可训练的alpha参数学习滤波器谱形状，解决了异质图性能差和高阶多项式过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 传统谱图神经网络在异质图上表现差，且在高阶多项式时出现过平滑问题，现有自适应多项式滤波器在连续域扩展和稳定性方面存在未解决的问题。

Method: 使用连续拉盖尔多项式构建GNN滤波器，使核心alpha参数可训练，并采用LayerNorm稳定化技术解决无界多项式的数值不稳定性问题。

Result: LaguerreNet在异质图基准测试中达到最先进性能，对过平滑具有极强鲁棒性，性能在K=10时达到峰值，比ChebyNet崩溃点高一个数量级。

Conclusion: LaguerreNet通过连续拉盖尔多项式和可训练参数成功解决了谱GNN的关键限制，为自适应多项式滤波器提供了有效解决方案。

Abstract: Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on "heterophilic" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.

</details>


### [79] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 分析表格ICL模型中各层的作用，发现存在结构冗余，为模型压缩和可解释性提供机会


<details>
  <summary>Details</summary>
Motivation: 尽管表格ICL模型与大型语言模型在架构上相似，但各层在表格预测中的具体作用尚不明确

Method: 通过"层作为画家"的视角分析TabPFN和TabICL模型，研究各层潜在空间的演化

Result: 发现只有部分层共享共同的表示语言，表明存在结构冗余

Conclusion: 表格ICL模型存在结构冗余，这为模型压缩和提升可解释性提供了机会

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [80] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 提出了一种使用时序基础模型进行上下文学习的分类方法，无需微调模型即可对训练数据之外的数据进行分类。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要针对特定任务微调模型的问题，利用预训练模型的扩展性实现更广泛的AI驱动维护系统。

Method: 将示例以目标（类别ID）和协变量（数据矩阵）的形式表示在模型提示中，通过上下文学习沿预测轴对未知协变量数据模式进行分类，将频域参考信号转换为伪时序模式。

Result: 该方法在伺服压力机电机轴承健康状态评估的振动数据上表现出良好效果，能够预测分类数据与预定义标签的对应概率。

Conclusion: 该方法标志着从定制化窄AI解决方案向更广泛AI驱动维护系统的重大进展，展示了预训练模型在不同操作条件下的有效性。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [81] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 系统评估了33种时间序列预测集成方法，发现stacking能持续提升精度，并提出多层级stacking框架，在多样化预测场景中均能提供优越精度。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中集成方法应用不足，简单的线性组合仍被视为最优方法，需要系统探索集成策略的潜力。

Method: 评估33种集成模型（包括现有和新颖方法），在50个真实世界数据集上进行测试，并提出多层级stacking框架来结合不同stacker模型的优势。

Result: stacking方法能持续提升预测精度，但没有单一stacker在所有任务中表现最佳；多层级stacking框架在多样化预测场景中均能提供优越精度。

Conclusion: 基于stacking的方法有潜力改进时间序列预测的AutoML系统，多层级集成策略是有效的解决方案。

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [82] [Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction](https://arxiv.org/abs/2511.15357)
*Yinan Yu,Falk Dippel,Christina E. Lundberg,Martin Lindgren,Annika Rosengren,Martin Adiels,Helen Sjöland*

Main category: cs.LG

TL;DR: 提出了成本感知预测框架，结合LLM代理进行成本效益分析，为心力衰竭患者1年死亡率预测提供临床决策支持


<details>
  <summary>Details</summary>
Motivation: 传统机器学习预测模型缺乏对下游价值权衡和临床可解释性的考虑，需要更透明的决策支持系统

Method: 开发XGBoost预测模型，引入临床影响投影曲线可视化成本维度，使用4个LLM代理生成患者特定描述

Result: XGBoost模型表现最佳（AUROC 0.804），CIP曲线提供群体级成本概览，LLM生成个体级成本效益分析，临床医生评价良好

Conclusion: CAP框架利用LLM代理整合ML分类结果和成本效益分析，提供更透明可解释的决策支持

Abstract: Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.

</details>


### [83] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 提出了一种新的后处理局部特征重要性方法——反事实重要性分布(CID)，通过生成正负反事实样本、核密度估计和分布差异度量来评估特征重要性。


<details>
  <summary>Details</summary>
Motivation: 机器学习中评估个体特征重要性对于理解模型决策过程至关重要，但现有方法缺乏明确的基准真值进行比较，需要替代的、有理论基础的评价指标。

Method: 生成正负两组反事实样本，使用核密度估计建模其分布，基于分布差异度量对特征进行排序。该方法基于严格的数学框架，满足有效度量的关键属性。

Result: 与现有局部特征重要性解释方法相比，CID方法不仅提供了互补视角，还在忠实性指标（全面性和充分性）上表现更好，产生更忠实的系统解释。

Conclusion: CID方法作为模型分析的有价值工具具有巨大潜力，能够提供更可靠的特征重要性评估。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [84] [EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG](https://arxiv.org/abs/2511.15393)
*Kunyu Zhang,Mingxuan Wang,Xiangjie Shi,Haoxing Xu,Chao Zhang*

Main category: cs.LG

TL;DR: EVA-Net是一个可解释的脑年龄异常检测框架，使用稀疏注意力Transformer处理长EEG序列，通过变分信息瓶颈学习鲁棒表示，并与健康老化原型网络对齐，在健康人群上训练后能准确检测MCI和AD患者的异常。


<details>
  <summary>Details</summary>
Motivation: 现有脑年龄模型难以从弱监督的健康人群数据中学习"正常"基线，且缺乏可解释性，这限制了在医疗数据不完美情况下的疾病检测应用。

Method: 使用稀疏注意力Transformer建模长EEG序列，采用变分信息瓶颈处理噪声和变异性，学习压缩表示并与连续原型网络对齐来学习健康老化流形。

Result: 在1297名健康受试者上训练，达到最先进精度；在27名MCI和AD患者验证集上显示显著更高的脑年龄差距和原型对齐误差，确认其偏离健康流形。

Conclusion: EVA-Net为使用不完美医疗数据的医疗智能提供了一个可解释的框架，能够有效检测脑部疾病异常。

Abstract: The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.

</details>


### [85] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 提出一种基于变分拉格朗日公式的非线性非高斯状态空间模型状态估计算法，通过熵信任域更新和动态约束实现贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 针对非线性非高斯状态空间模型的状态估计问题，传统方法在处理复杂分布时存在局限性，需要开发更有效的递归算法。

Method: 使用变分拉格朗日公式，将贝叶斯推断转化为序列熵信任域更新问题；针对高斯-马尔可夫近似推导递归方案；对一般非线性非高斯模型使用广义统计线性回归和傅里叶-埃尔米特矩匹配。

Result: 开发出一类具有良好计算复杂度的前向-后向算法家族，能够有效处理非线性非高斯状态估计问题。

Conclusion: 该变分框架为非线性非高斯状态估计提供了一种统一的算法设计方法，通过不同的后验分解可获得多种递归估计方案。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [86] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: FairEnergy是一个公平感知的联邦学习能量最小化框架，通过联合优化设备选择、带宽分配和压缩级别，在非IID数据上实现高精度同时降低79%能耗


<details>
  <summary>Details</summary>
Motivation: 解决无线边缘系统中联邦学习面临的能源效率与公平参与平衡问题，应对异构资源、不平等客户端贡献和有限通信容量的挑战

Method: 提出FairEnergy框架，将包含更新幅度和压缩比的贡献分数整合到联合优化中，通过松弛二元选择变量和拉格朗日分解处理全局带宽耦合，然后进行每设备子问题优化

Result: 在非IID数据实验中，FairEnergy相比基线策略实现了更高精度，同时能耗降低高达79%

Conclusion: FairEnergy有效解决了联邦学习在无线边缘系统中的能源效率与公平参与平衡问题，为实际部署提供了可行方案

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [87] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: 提出了NTK引导的隐式神经教学(NINT)方法，通过动态选择最大化全局函数更新的坐标来加速隐式神经表示的训练，利用神经正切核(NTK)对示例进行评分，显著减少训练时间近一半。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INRs)通过多层感知机参数化连续信号，但在拟合高分辨率信号时需要优化数百万个坐标，计算成本过高。

Method: 利用神经正切核(NTK)对示例进行评分，通过NTK增强损失梯度的范数来捕捉拟合误差和异构杠杆效应(自影响和跨坐标耦合)，动态选择能最大化全局函数更新的坐标。

Result: 实验表明NINT显著减少训练时间近一半，同时保持或提高了表示质量，在基于采样的加速策略中达到最先进水平。

Conclusion: NINT通过NTK引导的坐标选择有效加速了隐式神经表示的训练，在计算效率和表示质量之间取得了良好平衡。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


### [88] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 研究了按需采样中样本复杂度和轮数复杂度之间的权衡，在实线性和不可知性多分布学习(MDL)中建立了最优边界，并提出了OODS框架来抽象样本自适应权衡。


<details>
  <summary>Details</summary>
Motivation: 探索在有限轮数下自适应采样k个分布时，样本复杂度和轮数复杂度之间的基本权衡关系，为多分布学习提供理论保证。

Method: 提出了Optimization via On-Demand Sampling (OODS)框架来抽象样本自适应权衡，在实线性MDL中分析最优样本复杂度，在不可知性MDL中设计算法。

Result: 实线性MDL中r轮算法的最优样本复杂度约为dk^{Θ(1/r)}/ε；不可知性MDL中实现了样本复杂度为Õ((d+k)/ε²)且轮数为Õ(√k)的算法；建立了OODS设置的轮数复杂度紧界。

Conclusion: 在按需采样中存在样本复杂度和轮数复杂度的基本权衡，Õ(√k)轮算法在不可知性MDL中接近最优，超越此界限需要突破OODS固有难度的新技术。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>


### [89] [PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles](https://arxiv.org/abs/2511.15522)
*Yinan Yu,Samuel Scheidegger*

Main category: cs.LG

TL;DR: PCARNN-DCBF是一个结合物理编码控制仿射残差神经网络和基于预览的离散控制障碍函数的管道，用于地面车辆的运行时地理围栏，在保持可验证控制结构的同时实现高保真学习。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案难以在可验证控制的结构要求与高保真学习之间取得平衡，而运行时地理围栏对于执行操作设计域至关重要。

Method: 引入PCARNN-DCBF管道，PCARNN明确保留车辆动力学的控制仿射结构，确保可靠优化所需的线性特性；DCBF通过实时二次规划强制执行多边形保持约束，处理高相对度并缓解执行器饱和。

Result: 在CARLA中对电动和燃烧平台的实验表明，这种结构保持方法显著优于解析和无结构神经基线。

Conclusion: PCARNN-DCBF通过结构保持方法成功解决了高保真学习与可验证控制之间的平衡问题，为地面车辆的运行时地理围栏提供了有效解决方案。

Abstract: Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.

</details>
