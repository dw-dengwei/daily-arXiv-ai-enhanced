{"id": "2509.19459", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.19459", "abs": "https://arxiv.org/abs/2509.19459", "authors": ["Yutong Guo", "Weiyu Luo", "Brian Demsky"], "title": "Automated Insertion of Flushes and Fences for Persistency", "comment": null, "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations.", "AI": {"tldr": "PMRobust\u662f\u4e00\u4e2a\u7f16\u8bd1\u5668\uff0c\u80fd\u81ea\u52a8\u63d2\u5165flush\u548cfence\u64cd\u4f5c\uff0c\u786e\u4fdd\u6301\u4e45\u5185\u5b58\u4ee3\u7801\u6ca1\u6709\u7f3a\u5931flush\u548cfence\u7684bug\uff0c\u5e73\u5747\u5f00\u9500\u4ec50.26%\u3002", "motivation": "\u6301\u4e45\u5185\u5b58\u548cCXL\u5171\u4eab\u5185\u5b58\u9700\u8981\u624b\u52a8flush\u7f13\u5b58\u884c\u6765\u786e\u4fdd\u6570\u636e\u6301\u4e45\u5316\uff0c\u6b63\u786e\u4f7f\u7528flush\u548cfence\u64cd\u4f5c\u5f88\u56f0\u96be\uff0c\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u786e\u4fdd\u6ca1\u6709\u7f3a\u5931flush\u7684bug\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u9759\u6001\u5206\u6790\u548c\u9488\u5bf9\u65b0\u5206\u914d\u5bf9\u8c61\u7684\u4f18\u5316\uff0c\u81ea\u52a8\u63d2\u5165\u5fc5\u8981\u7684flush\u548cfence\u64cd\u4f5c\u3002", "result": "\u5728\u6301\u4e45\u5185\u5b58\u5e93\u548c\u6570\u636e\u7ed3\u6784\u4e0a\u8bc4\u4f30\uff0c\u51e0\u4f55\u5e73\u5747\u5f00\u9500\u4ec5\u4e3a0.26%\uff0c\u76f8\u6bd4\u624b\u52a8\u653e\u7f6eflush\u548cfence\u64cd\u4f5c\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "PMRobust\u80fd\u6709\u6548\u81ea\u52a8\u786e\u4fdd\u6301\u4e45\u5185\u5b58\u4ee3\u7801\u7684\u6b63\u786e\u6027\uff0c\u4e14\u6027\u80fd\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2509.19533", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19533", "abs": "https://arxiv.org/abs/2509.19533", "authors": ["Mengdi Lu", "Steven Ding", "Furkan Alaca", "Philippe Charland"], "title": "Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation", "comment": null, "summary": "Security vulnerabilities in Internet-of-Things devices, mobile platforms, and\nautonomous systems remain critical. Traditional mutation-based fuzzers -- while\neffectively explore code paths -- primarily perform byte- or bit-level edits\nwithout semantic reasoning. Coverage-guided tools such as AFL++ use\ndictionaries, grammars, and splicing heuristics to impose shallow structural\nconstraints, leaving deeper protocol logic, inter-field dependencies, and\ndomain-specific semantics unaddressed. Conversely, reasoning-capable large\nlanguage models (LLMs) can leverage pretraining knowledge to understand input\nformats, respect complex constraints, and propose targeted mutations, much like\nan experienced reverse engineer or testing expert. However, lacking ground\ntruth for \"correct\" mutation reasoning makes supervised fine-tuning\nimpractical, motivating explorations of off-the-shelf LLMs via prompt-based\nfew-shot learning. To bridge this gap, we present an open-source microservices\nframework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,\ntackling asynchronous execution and divergent hardware demands (GPU- vs.\nCPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)\nHow can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do\nfew-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt\nengineering with off-the-shelf models improve fuzzing directly? and (R4) Which\nopen-source reasoning LLMs perform best under prompt-only conditions?\nExperiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3\nhighlight Deepseek as the most promising. Mutation effectiveness depends more\non prompt complexity and model choice than shot count. Response latency and\nthroughput bottlenecks remain key obstacles, offering directions for future\nwork.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u63a8\u7406\u80fd\u529b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0eAFL++\u7ed3\u5408\u7684\u5fae\u670d\u52a1\u6846\u67b6\uff0c\u901a\u8fc7few-shot\u63d0\u793a\u5de5\u7a0b\u63d0\u5347\u6a21\u7cca\u6d4b\u8bd5\u7684\u7a81\u53d8\u8d28\u91cf\uff0c\u53d1\u73b0Deepseek\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u63d0\u793a\u590d\u6742\u5ea6\u548c\u6a21\u578b\u9009\u62e9\u6bd4shot\u6570\u91cf\u66f4\u91cd\u8981", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7a81\u53d8\u7684\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u7f3a\u4e4f\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u7684\u534f\u8bae\u903b\u8f91\u548c\u5b57\u6bb5\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u7406\u89e3\u8f93\u5165\u683c\u5f0f\u548c\u7ea6\u675f\u7684\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u76d1\u7763\u8bad\u7ec3\u7684ground truth\uff0c\u9700\u8981\u63a2\u7d22\u57fa\u4e8e\u63d0\u793a\u7684few-shot\u5b66\u4e60\u65b9\u6cd5", "method": "\u5f00\u53d1\u5f00\u6e90\u5fae\u670d\u52a1\u6846\u67b6\uff0c\u5c06\u63a8\u7406LLMs\u4e0eAFL++\u96c6\u6210\u5728Google FuzzBench\u4e0a\uff0c\u89e3\u51b3\u5f02\u6b65\u6267\u884c\u548c\u786c\u4ef6\u9700\u6c42\u5dee\u5f02\u95ee\u9898\uff0c\u8bc4\u4f30few-shot\u63d0\u793a\u4e0ezero-shot\u7684\u6548\u679c\u5dee\u5f02", "result": "\u5b9e\u9a8c\u663e\u793aDeepseek\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u7a81\u53d8\u6709\u6548\u6027\u66f4\u591a\u53d6\u51b3\u4e8e\u63d0\u793a\u590d\u6742\u5ea6\u548c\u6a21\u578b\u9009\u62e9\u800c\u975eshot\u6570\u91cf\uff0c\u54cd\u5e94\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u74f6\u9888\u662f\u4e3b\u8981\u6311\u6218", "conclusion": "\u63a8\u7406LLMs\u53ef\u4ee5\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6709\u6548\u63d0\u5347\u6a21\u7cca\u6d4b\u8bd5\u8d28\u91cf\uff0cDeepseek\u662f\u6700\u6709\u524d\u666f\u7684\u6a21\u578b\uff0c\u672a\u6765\u5de5\u4f5c\u9700\u8981\u89e3\u51b3\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u95ee\u9898"}}
{"id": "2509.19587", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19587", "abs": "https://arxiv.org/abs/2509.19587", "authors": ["Mohamed Ouf", "Haoyu Li", "Michael Zhang", "Mariam Guizani"], "title": "Reverse Engineering User Stories from Code using Large Language Models", "comment": null, "summary": "User stories are essential in agile development, yet often missing or\noutdated in legacy and poorly documented systems. We investigate whether large\nlanguage models (LLMs) can automatically recover user stories directly from\nsource code and how prompt design impacts output quality. Using 1,750 annotated\nC++ snippets of varying complexity, we evaluate five state-of-the-art LLMs\nacross six prompting strategies. Results show that all models achieve, on\naverage, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a\nsingle illustrative example enables the smallest model (8B) to match the\nperformance of a much larger 70B model. In contrast, structured reasoning via\nChain-of-Thought offers only marginal gains, primarily for larger models.", "AI": {"tldr": "LLMs\u80fd\u591f\u4ece\u6e90\u4ee3\u7801\u4e2d\u81ea\u52a8\u6062\u590d\u7528\u6237\u6545\u4e8b\uff0c\u5728200\u884c\u4ee3\u7801\u5185\u5e73\u5747F1\u5206\u6570\u8fbe\u52300.8\uff0c\u5c0f\u6a21\u578b\u901a\u8fc7\u5355\u793a\u4f8b\u63d0\u793a\u53ef\u5ab2\u7f8e\u5927\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u654f\u6377\u5f00\u53d1\u4e2d\u7528\u6237\u6545\u4e8b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u9057\u7559\u7cfb\u7edf\u548c\u6587\u6863\u4e0d\u5b8c\u5584\u7684\u7cfb\u7edf\u4e2d\u5e38\u5e38\u7f3a\u5931\u6216\u8fc7\u65f6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u6062\u590d\u8fd9\u4e9b\u7528\u6237\u6545\u4e8b\u3002", "method": "\u4f7f\u75281,750\u4e2a\u6807\u6ce8\u7684C++\u4ee3\u7801\u7247\u6bb5\uff0c\u8bc4\u4f305\u4e2a\u6700\u5148\u8fdb\u7684LLM\u57286\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\uff0c\u5206\u6790\u4ee3\u7801\u590d\u6742\u5ea6\u548c\u63d0\u793a\u8bbe\u8ba1\u5bf9\u8f93\u51fa\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728200NLOC\u4ee5\u5185\u7684\u4ee3\u7801\u4e0a\u5e73\u5747F1\u5206\u6570\u8fbe\u52300.8\uff1b\u5c0f\u6a21\u578b\uff088B\uff09\u901a\u8fc7\u5355\u793a\u4f8b\u63d0\u793a\u5c31\u80fd\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\uff0870B\uff09\u76f8\u5f53\u7684\u6027\u80fd\uff1bChain-of-Thought\u63a8\u7406\u4ec5\u5bf9\u5927\u578b\u6a21\u578b\u6709\u8fb9\u9645\u6539\u5584\u3002", "conclusion": "LLMs\u80fd\u591f\u6709\u6548\u4ece\u6e90\u4ee3\u7801\u4e2d\u6062\u590d\u7528\u6237\u6545\u4e8b\uff0c\u63d0\u793a\u8bbe\u8ba1\u6bd4\u6a21\u578b\u89c4\u6a21\u66f4\u91cd\u8981\uff0c\u7b80\u5355\u7684\u793a\u4f8b\u63d0\u793a\u5c31\u80fd\u8ba9\u5c0f\u6a21\u578b\u8fbe\u5230\u4f18\u79c0\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u5316\u6587\u6863\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2509.19673", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19673", "abs": "https://arxiv.org/abs/2509.19673", "authors": ["Ahmed Aljohani", "Anamul Haque Mollah", "Hyunsook Do"], "title": "Assertion Messages with Large Language Models (LLMs) for Code", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Assertion messages significantly enhance unit tests by clearly explaining the\nreasons behind test failures, yet they are frequently omitted by developers and\nautomated test-generation tools. Despite recent advancements, Large Language\nModels (LLMs) have not been systematically evaluated for their ability to\ngenerate informative assertion messages. In this paper, we introduce an\nevaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -\nQwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset\nof 216 Java test methods containing developer-written assertion messages. We\nfind that Codestral-22B achieves the highest quality score of 2.76 out of 5\nusing a human-like evaluation approach, compared to 3.24 for manually written\nmessages. Our ablation study shows that including descriptive test comments\nfurther improves Codestral's performance to 2.97, highlighting the critical\nrole of context in generating clear assertion messages. Structural analysis\ndemonstrates that all models frequently replicate developers' preferred\nlinguistic patterns. We discuss the limitations of the selected models and\nconventional text evaluation metrics in capturing diverse assertion message\nstructures. Our benchmark, evaluation results, and discussions provide an\nessential foundation for advancing automated, context-aware generation of\nassertion messages in test code. A replication package is available at\nhttps://doi.org/10.5281/zenodo.15293133", "AI": {"tldr": "\u8bc4\u4f30\u56db\u79cd\u5148\u8fdbLLM\u5728\u751f\u6210Java\u6d4b\u8bd5\u65ad\u8a00\u6d88\u606f\u65b9\u9762\u7684\u80fd\u529b\uff0cCodestral-22B\u8868\u73b0\u6700\u4f73\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u751f\u6210\u8d28\u91cf\u81f3\u5173\u91cd\u8981", "motivation": "\u65ad\u8a00\u6d88\u606f\u80fd\u663e\u8457\u63d0\u5347\u5355\u5143\u6d4b\u8bd5\u7684\u53ef\u7406\u89e3\u6027\uff0c\u4f46\u5f00\u53d1\u8005\u548c\u81ea\u52a8\u5316\u5de5\u5177\u7ecf\u5e38\u5ffd\u7565\u7f16\u5199\u3002LLM\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30", "method": "\u4f7f\u7528\u5305\u542b216\u4e2aJava\u6d4b\u8bd5\u65b9\u6cd5\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u56db\u79cdFIM LLM\uff08Qwen2.5-Coder-32B\u3001Codestral-22B\u3001CodeLlama-13B\u3001StarCoder\uff09\u751f\u6210\u65ad\u8a00\u6d88\u606f\u7684\u80fd\u529b\uff0c\u91c7\u7528\u7c7b\u4eba\u8bc4\u4f30\u65b9\u6cd5", "result": "Codestral-22B\u83b7\u5f97\u6700\u9ad8\u8d28\u91cf\u5206\u65702.76/5\uff08\u4eba\u5de5\u7f16\u5199\u4e3a3.24\uff09\uff0c\u5305\u542b\u63cf\u8ff0\u6027\u6d4b\u8bd5\u6ce8\u91ca\u540e\u6027\u80fd\u63d0\u5347\u81f32.97\u3002\u6240\u6709\u6a21\u578b\u90fd\u503e\u5411\u4e8e\u590d\u5236\u5f00\u53d1\u8005\u7684\u8bed\u8a00\u6a21\u5f0f", "conclusion": "LLM\u5728\u751f\u6210\u65ad\u8a00\u6d88\u606f\u65b9\u9762\u6709\u6f5c\u529b\u4f46\u4ecd\u6709\u5c40\u9650\uff0c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u751f\u6210\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u81ea\u52a8\u5316\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65ad\u8a00\u6d88\u606f\u751f\u6210\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840"}}
{"id": "2509.19512", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19512", "abs": "https://arxiv.org/abs/2509.19512", "authors": ["Charles Dansereau", "Junior-Samuel Lopez-Yepez", "Karthik Soma", "Antoine Fagette"], "title": "The Heterogeneous Multi-Agent Challenge", "comment": "7 pages. To Appear at ECAI 2025", "summary": "Multi-Agent Reinforcement Learning (MARL) is a growing research area which\ngained significant traction in recent years, extending Deep RL applications to\na much wider range of problems. A particularly challenging class of problems in\nthis domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where\nagents with different sensors, resources, or capabilities must cooperate based\non local information. The large number of real-world situations involving\nheterogeneous agents makes it an attractive research area, yet underexplored,\nas most MARL research focuses on homogeneous agents (e.g., a swarm of identical\nrobots). In MARL and single-agent RL, standardized environments such as ALE and\nSMAC have allowed to establish recognized benchmarks to measure progress.\nHowever, there is a clear lack of such standardized testbed for cooperative\nHeMARL. As a result, new research in this field often uses simple environments,\nwhere most algorithms perform near optimally, or uses weakly heterogeneous MARL\nenvironments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(HeMARL)\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u6d4b\u8bd5\u73af\u5883\uff0c\u5bfc\u81f4\u7814\u7a76\u8fdb\u5c55\u96be\u4ee5\u8861\u91cf\uff0c\u9700\u8981\u5efa\u7acb\u7c7b\u4f3cALE\u548cSMAC\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b58\u5728\u5927\u91cf\u5f02\u6784\u667a\u80fd\u4f53\u9700\u8981\u534f\u4f5c\u7684\u573a\u666f\uff0c\u4f46\u5f53\u524dMARL\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u540c\u6784\u667a\u80fd\u4f53\uff0c\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u6d4b\u8bd5\u73af\u5883\u6765\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524dHeMARL\u7814\u7a76\u73b0\u72b6\uff0c\u6307\u51fa\u5b58\u5728\u7684\u95ee\u9898\uff1a\u4f7f\u7528\u8fc7\u4e8e\u7b80\u5355\u7684\u73af\u5883\u6216\u5f31\u5f02\u6784\u73af\u5883\uff0c\u5bfc\u81f4\u7b97\u6cd5\u6027\u80fd\u96be\u4ee5\u533a\u5206\u4f18\u52a3\u3002", "result": "\u8bc6\u522b\u51faHeMARL\u9886\u57df\u6807\u51c6\u5316\u6d4b\u8bd5\u73af\u5883\u7684\u7f3a\u5931\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u5efa\u7acb\u516c\u8ba4\u57fa\u51c6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u9700\u8981\u4e3a\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5f00\u53d1\u6807\u51c6\u5316\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u548c\u7b97\u6cd5\u8bc4\u4f30\u3002"}}
{"id": "2509.19478", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19478", "abs": "https://arxiv.org/abs/2509.19478", "authors": ["Ziwei Wang", "Cong Wu", "Paolo Tasca"], "title": "Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera", "comment": null, "summary": "Sharding has emerged as a critical solution to address the scalability\nchallenges faced by blockchain networks, enabling them to achieve higher\ntransaction throughput, reduced latency, and optimized resource usage. This\npaper investigates the advancements, methodologies, and adoption potential of\nsharding in the context of Hedera, a distributed ledger technology known for\nits unique Gossip about Gossip protocol and asynchronous Byzantine Fault\nTolerance (ABFT). We explore various academic and industrial sharding\ntechniques, emphasizing their benefits and trade-offs. Building on these\ninsights, we propose a hybrid sharding solution for Hedera that partitions the\nnetwork into local and global committees, facilitating efficient cross-shard\ntransactions and ensuring robust security through dynamic reconfiguration. Our\nanalysis highlights significant reductions in storage and communication\noverhead, improved scalability, and enhanced fault tolerance, demonstrating the\nfeasibility and advantages of integrating sharding into Hedera's architecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eHedera\u533a\u5757\u94fe\u7684\u6df7\u5408\u5206\u7247\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u672c\u5730\u548c\u5168\u5c40\u59d4\u5458\u4f1a\u7684\u7f51\u7edc\u5206\u533a\u5b9e\u73b0\u9ad8\u6548\u8de8\u5206\u7247\u4ea4\u6613\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b58\u50a8\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5bb9\u9519\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u533a\u5757\u94fe\u7f51\u7edc\u9762\u4e34\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u901a\u8fc7\u5206\u7247\u6280\u672f\u5b9e\u73b0\u66f4\u9ad8\u7684\u4ea4\u6613\u541e\u5410\u91cf\u3001\u964d\u4f4e\u5ef6\u8fdf\u548c\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\uff0c\u7279\u522b\u662f\u5728Hedera\u8fd9\u79cd\u91c7\u7528Gossip about Gossip\u534f\u8bae\u548c\u5f02\u6b65\u62dc\u5360\u5ead\u5bb9\u9519\u6280\u672f\u7684\u5206\u5e03\u5f0f\u8d26\u672c\u4e2d\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u5206\u7247\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u7f51\u7edc\u5212\u5206\u4e3a\u672c\u5730\u548c\u5168\u5c40\u59d4\u5458\u4f1a\uff0c\u652f\u6301\u9ad8\u6548\u7684\u8de8\u5206\u7247\u4ea4\u6613\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u914d\u7f6e\u786e\u4fdd\u5f3a\u5927\u7684\u5b89\u5168\u6027\u3002", "result": "\u5206\u6790\u663e\u793a\u5b58\u50a8\u548c\u901a\u4fe1\u5f00\u9500\u663e\u8457\u964d\u4f4e\uff0c\u53ef\u6269\u5c55\u6027\u5f97\u5230\u6539\u5584\uff0c\u5bb9\u9519\u80fd\u529b\u589e\u5f3a\uff0c\u8bc1\u660e\u4e86\u5728Hedera\u67b6\u6784\u4e2d\u96c6\u6210\u5206\u7247\u6280\u672f\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002", "conclusion": "\u5206\u7247\u6280\u672f\u662f\u89e3\u51b3\u533a\u5757\u94fe\u53ef\u6269\u5c55\u6027\u95ee\u9898\u7684\u5173\u952e\u65b9\u6848\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6df7\u5408\u5206\u7247\u65b9\u6cd5\u4e3aHedera\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6269\u5c55\u8def\u5f84\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.19645", "categories": ["cs.PF", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19645", "abs": "https://arxiv.org/abs/2509.19645", "authors": ["Youpeng Zhao", "Jinpeng LV", "Di Wu", "Jun Wang", "Christopher Gooley"], "title": "Are We Scaling the Right Thing? A System Perspective on Test-Time Scaling", "comment": null, "summary": "Test-time scaling (TTS) has recently emerged as a promising direction to\nexploit the hidden reasoning capabilities of pre-trained large language models\n(LLMs). However, existing scaling methods narrowly focus on the compute-optimal\nPareto-frontier, ignoring the simple fact that compute-optimal is not always\nsystem-optimal. In this work, we propose a system-driven perspective on TTS,\nanalyzing how reasoning models scale against practical metrics, such as latency\nand cost-per-token. By evaluating the impact of popular optimizations such as\ntensor parallelism and speculative decoding, our preliminary analysis reveals\nthe limitations of current methods and calls for a paradigm shift toward\nholistic, system-aware evaluations that capture the true essence of scaling\nlaws at inference time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7cfb\u7edf\u9a71\u52a8\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u89c6\u89d2\uff0c\u5206\u6790\u63a8\u7406\u6a21\u578b\u5728\u5b9e\u9645\u6307\u6807\uff08\u5982\u5ef6\u8fdf\u548c\u6bcf\u4ee4\u724c\u6210\u672c\uff09\u4e0a\u7684\u6269\u5c55\u6548\u679c\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u5e76\u547c\u5401\u8f6c\u5411\u6574\u4f53\u7cfb\u7edf\u611f\u77e5\u8bc4\u4f30", "motivation": "\u73b0\u6709\u7f29\u653e\u65b9\u6cd5\u4ec5\u5173\u6ce8\u8ba1\u7b97\u6700\u4f18Pareto\u8fb9\u754c\uff0c\u5ffd\u7565\u4e86\u8ba1\u7b97\u6700\u4f18\u5e76\u4e0d\u603b\u662f\u7cfb\u7edf\u6700\u4f18\u7684\u7b80\u5355\u4e8b\u5b9e\uff0c\u9700\u8981\u4ece\u7cfb\u7edf\u89d2\u5ea6\u8bc4\u4f30\u63a8\u7406\u65f6\u7684\u6269\u5c55\u89c4\u5f8b", "method": "\u901a\u8fc7\u8bc4\u4f30\u5f20\u91cf\u5e76\u884c\u548c\u63a8\u6d4b\u89e3\u7801\u7b49\u6d41\u884c\u4f18\u5316\u6280\u672f\u7684\u5f71\u54cd\uff0c\u8fdb\u884c\u521d\u6b65\u5206\u6790", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "conclusion": "\u9700\u8981\u8303\u5f0f\u8f6c\u53d8\uff0c\u91c7\u7528\u6574\u4f53\u7cfb\u7edf\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u6355\u6349\u63a8\u7406\u65f6\u6269\u5c55\u89c4\u5f8b\u7684\u771f\u5b9e\u672c\u8d28"}}
{"id": "2509.19708", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19708", "abs": "https://arxiv.org/abs/2509.19708", "authors": ["Anand Kumar", "Vishal Khare", "Deepak Sharma", "Satyam Kumar", "Vijay Saini", "Anshul Yadav", "Sachendra Jain", "Ankit Rana", "Pratham Verma", "Vaibhav Meena", "Avinash Edubilli"], "title": "Intuition to Evidence: Measuring AI's True Impact on Developer Productivity", "comment": "16 pages, 10 figures, 5 tables", "summary": "We present a comprehensive real-world evaluation of AI-assisted software\ndevelopment tools deployed at enterprise scale. Over one year, 300 engineers\nacross multiple teams integrated an in-house AI platform (DeputyDev) that\ncombines code generation and automated review capabilities into their daily\nworkflows. Through rigorous cohort analysis, our study demonstrates\nstatistically significant productivity improvements, including an overall 31.8%\nreduction in PR review cycle time.\n  Developer adoption was strong, with 85% satisfaction for code review features\nand 93% expressing a desire to continue using the platform. Adoption patterns\nshowed systematic scaling from 4% engagement in month 1 to 83% peak usage by\nmonth 6, stabilizing at 60% active engagement. Top adopters achieved a 61%\nincrease in code volume pushed to production, contributing to approximately 30\nto 40% of code shipped to production through this tool, accounting for an\noverall 28% increase in code shipment volume.\n  Unlike controlled benchmark evaluations, our longitudinal analysis provides\nempirical evidence from production environments, revealing both the\ntransformative potential and practical deployment challenges of integrating AI\ninto enterprise software development workflows.", "AI": {"tldr": "\u4f01\u4e1a\u7ea7AI\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u7684\u4e00\u5e74\u671f\u771f\u5b9e\u8bc4\u4f30\u663e\u793a\uff1a\u4ee3\u7801\u5ba1\u67e5\u5468\u671f\u51cf\u5c1131.8%\uff0c\u5f00\u53d1\u8005\u6ee1\u610f\u5ea6\u8fbe85%\uff0c\u6d3b\u8dc3\u4f7f\u7528\u7387\u7a33\u5b9a\u572860%\uff0c\u4ee3\u7801\u4ea7\u51fa\u91cf\u589e\u52a028%", "motivation": "\u8bc4\u4f30AI\u8f85\u52a9\u5f00\u53d1\u5de5\u5177\u5728\u4f01\u4e1a\u89c4\u6a21\u90e8\u7f72\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u586b\u8865\u5b9e\u9a8c\u5ba4\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u771f\u5b9e\u751f\u4ea7\u73af\u5883\u4e4b\u95f4\u7684\u7814\u7a76\u7a7a\u767d", "method": "\u5bf9300\u540d\u5de5\u7a0b\u5e08\u8fdb\u884c\u4e3a\u671f\u4e00\u5e74\u7684\u961f\u5217\u5206\u6790\uff0c\u4f7f\u7528\u5185\u90e8AI\u5e73\u53f0DeputyDev\uff08\u96c6\u4ee3\u7801\u751f\u6210\u548c\u81ea\u52a8\u5ba1\u67e5\u529f\u80fd\uff09\uff0c\u901a\u8fc7\u7eb5\u5411\u6570\u636e\u5206\u6790\u751f\u4ea7\u73af\u5883\u4f7f\u7528\u60c5\u51b5", "result": "PR\u5ba1\u67e5\u5468\u671f\u51cf\u5c1131.8%\uff1b85%\u5f00\u53d1\u8005\u6ee1\u610f\u4ee3\u7801\u5ba1\u67e5\u529f\u80fd\uff1b93%\u5e0c\u671b\u7ee7\u7eed\u4f7f\u7528\uff1b\u4f7f\u7528\u7387\u4ece4%\u589e\u957f\u523083%\u5cf0\u503c\u540e\u7a33\u5b9a\u572860%\uff1b\u9ad8\u4ea7\u7528\u6237\u4ee3\u7801\u4ea7\u51fa\u589e\u52a061%\uff1b\u7ea630-40%\u751f\u4ea7\u4ee3\u7801\u901a\u8fc7\u8be5\u5de5\u5177\u751f\u6210\uff1b\u6574\u4f53\u4ee3\u7801\u4ea4\u4ed8\u91cf\u589e\u52a028%", "conclusion": "AI\u8f85\u52a9\u5f00\u53d1\u5de5\u5177\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4ece\u5b9e\u9a8c\u5ba4\u57fa\u51c6\u6d4b\u8bd5\u8f6c\u5411\u751f\u4ea7\u73af\u5883\u5b9e\u8bc1\u7814\u7a76"}}
{"id": "2509.19599", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19599", "abs": "https://arxiv.org/abs/2509.19599", "authors": ["Danilo Trombino", "Vincenzo Pecorella", "Alessandro de Giulii", "Davide Tresoldi"], "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method for Multi-Agent Systems", "comment": null, "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing.", "AI": {"tldr": "KBA Orchestration\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u63cf\u8ff0\u548c\u4ece\u667a\u80fd\u4f53\u77e5\u8bc6\u5e93\u63d0\u53d6\u7684\u52a8\u6001\u9690\u79c1\u4fdd\u62a4\u76f8\u5173\u6027\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u8def\u7531\u7684\u51c6\u786e\u6027\u548c\u7cfb\u7edf\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u8c03\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u667a\u80fd\u4f53\u63cf\u8ff0\uff0c\u8fd9\u4e9b\u63cf\u8ff0\u5f80\u5f80\u8fc7\u65f6\u6216\u4e0d\u5b8c\u6574\uff0c\u5bfc\u81f4\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4efb\u52a1\u8def\u7531\u6548\u7387\u4f4e\u4e0b\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u80fd\u529b\u6301\u7eed\u6f14\u5316\u7684\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u5e93\u611f\u77e5\u534f\u8c03\u6846\u67b6\uff0c\u5f53\u9759\u6001\u63cf\u8ff0\u4e0d\u8db3\u4ee5\u505a\u51fa\u660e\u786e\u8def\u7531\u51b3\u7b56\u65f6\uff0c\u534f\u8c03\u5668\u5e76\u884c\u63d0\u793a\u5b50\u667a\u80fd\u4f53\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u6839\u636e\u5176\u79c1\u6709\u77e5\u8bc6\u5e93\u8bc4\u4f30\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u8fd4\u56de\u8f7b\u91cf\u7ea7ACK\u4fe1\u53f7\u800c\u4e0d\u66b4\u9732\u5e95\u5c42\u6570\u636e\uff0c\u6536\u96c6\u7684\u4fe1\u53f7\u586b\u5145\u5171\u4eab\u8bed\u4e49\u7f13\u5b58\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cKBA Orchestration\u5728\u8def\u7531\u7cbe\u5ea6\u548c\u6574\u4f53\u7cfb\u7edf\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u9759\u6001\u63cf\u8ff0\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u8981\u6bd4\u6807\u51c6\u63cf\u8ff0\u9a71\u52a8\u8def\u7531\u66f4\u9ad8\u7cbe\u5ea6\u7684\u5927\u89c4\u6a21\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u63cf\u8ff0\u548c\u52a8\u6001\u9690\u79c1\u4fdd\u62a4\u76f8\u5173\u6027\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u548c\u81ea\u9002\u5e94\u7684\u4efb\u52a1\u8def\u7531\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u667a\u80fd\u4f53\u81ea\u4e3b\u6027\u548c\u6570\u636e\u673a\u5bc6\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u534f\u8c03\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19532", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.19532", "abs": "https://arxiv.org/abs/2509.19532", "authors": ["Flavio Castro", "Weijian Zheng", "Joaquin Chung", "Ian Foster", "Rajkumar Kettimuthu"], "title": "To Stream or Not to Stream: Towards A Quantitative Model for Remote HPC Processing Decisions", "comment": null, "summary": "Modern scientific instruments generate data at rates that increasingly exceed\nlocal compute capabilities and, when paired with the staging and I/O overheads\nof file-based transfers, also render file-based use of remote HPC resources\nimpractical for time-sensitive analysis and experimental steering. Real-time\nstreaming frameworks promise to reduce latency and improve system efficiency,\nbut lack a principled way to assess their feasibility. In this work, we\nintroduce a quantitative framework and an accompanying Streaming Speed Score to\nevaluate whether remote high-performance computing (HPC) resources can provide\ntimely data processing compared to local alternatives. Our model incorporates\nkey parameters including data generation rate, transfer efficiency, remote\nprocessing power, and file input/output overhead to compute total processing\ncompletion time and identify operational regimes where streaming is beneficial.\nWe motivate our methodology with use cases from facilities such as APS, FRIB,\nLCLS-II, and the LHC, and validate our approach through an illustrative case\nstudy based on LCLS-II data. Our measurements show that streaming can achieve\nup to 97% lower end-to-end completion time than file-based methods under high\ndata rates, while worst-case congestion can increase transfer times by over an\norder of magnitude, underscoring the importance of tail latency in streaming\nfeasibility decisions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316\u6846\u67b6\u548c\u6d41\u5f0f\u901f\u5ea6\u8bc4\u5206\u6765\u8bc4\u4f30\u8fdc\u7a0bHPC\u8d44\u6e90\u662f\u5426\u80fd\u6bd4\u672c\u5730\u66ff\u4ee3\u65b9\u6848\u63d0\u4f9b\u53ca\u65f6\u7684\u6570\u636e\u5904\u7406\uff0c\u901a\u8fc7\u8003\u8651\u6570\u636e\u751f\u6210\u7387\u3001\u4f20\u8f93\u6548\u7387\u3001\u8fdc\u7a0b\u5904\u7406\u80fd\u529b\u548c\u6587\u4ef6I/O\u5f00\u9500\u7b49\u5173\u952e\u53c2\u6570\u6765\u8ba1\u7b97\u603b\u5904\u7406\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u4eea\u5668\u751f\u6210\u6570\u636e\u7684\u901f\u5ea6\u8d8a\u6765\u8d8a\u8d85\u8fc7\u672c\u5730\u8ba1\u7b97\u80fd\u529b\uff0c\u6587\u4ef6\u5f0f\u4f20\u8f93\u7684\u6682\u5b58\u548cI/O\u5f00\u9500\u4f7f\u5f97\u57fa\u4e8e\u6587\u4ef6\u7684\u8fdc\u7a0bHPC\u8d44\u6e90\u4f7f\u7528\u5bf9\u65f6\u95f4\u654f\u611f\u7684\u5206\u6790\u548c\u5b9e\u9a8c\u5f15\u5bfc\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u5f15\u5165\u5b9a\u91cf\u6846\u67b6\u548c\u6d41\u5f0f\u901f\u5ea6\u8bc4\u5206\uff0c\u6574\u5408\u6570\u636e\u751f\u6210\u7387\u3001\u4f20\u8f93\u6548\u7387\u3001\u8fdc\u7a0b\u5904\u7406\u80fd\u529b\u548c\u6587\u4ef6I/O\u5f00\u9500\u7b49\u5173\u952e\u53c2\u6570\uff0c\u8ba1\u7b97\u603b\u5904\u7406\u5b8c\u6210\u65f6\u95f4\u5e76\u8bc6\u522b\u6d41\u5f0f\u5904\u7406\u6709\u76ca\u7684\u64cd\u4f5c\u533a\u95f4\u3002", "result": "\u6d4b\u91cf\u663e\u793a\u5728\u9ad8\u6570\u636e\u901f\u7387\u4e0b\uff0c\u6d41\u5f0f\u5904\u7406\u53ef\u4ee5\u5b9e\u73b0\u6bd4\u57fa\u4e8e\u6587\u4ef6\u7684\u65b9\u6cd5\u4f4e97%\u7684\u7aef\u5230\u7aef\u5b8c\u6210\u65f6\u95f4\uff0c\u4f46\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u62e5\u585e\u53ef\u80fd\u4f7f\u4f20\u8f93\u65f6\u95f4\u589e\u52a0\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002", "conclusion": "\u6d41\u5f0f\u5904\u7406\u5728\u9ad8\u6570\u636e\u901f\u7387\u4e0b\u663e\u8457\u4f18\u4e8e\u6587\u4ef6\u5f0f\u65b9\u6cd5\uff0c\u4f46\u5c3e\u90e8\u5ef6\u8fdf\u5bf9\u6d41\u5f0f\u5904\u7406\u7684\u53ef\u884c\u6027\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u6d41\u5f0f\u5904\u7406\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.19701", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.19701", "abs": "https://arxiv.org/abs/2509.19701", "authors": ["Akash Poptani", "Alireza Khadem", "Scott Mahlke", "Jonah Miller", "Joshua Dolence", "Reetuparna Das"], "title": "Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE", "comment": "Accepted to appear at IISWC 2025", "summary": "Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce\ncompute and memory demands while maintaining accuracy. This work analyzes the\nperformance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems.\nWe show that smaller mesh blocks and deeper AMR levels degrade GPU performance\ndue to increased communication, serial overheads, and inefficient GPU\nutilization. Through detailed profiling, we identify inefficiencies, low\noccupancy, and memory access bottlenecks. We further analyze rank scalability\nand memory constraints, and propose optimizations to improve GPU throughput and\nreduce memory footprint. Our insights can inform future AMR deployments on\nDepartment of Energy's upcoming heterogeneous supercomputers.", "AI": {"tldr": "Parthenon AMR\u57fa\u51c6\u6d4b\u8bd5\u5728CPU-GPU\u7cfb\u7edf\u4e0a\u6027\u80fd\u5206\u6790\u663e\u793a\uff0c\u8f83\u5c0f\u7684\u7f51\u683c\u5757\u548c\u8f83\u6df1\u7684AMR\u5c42\u7ea7\u4f1a\u56e0\u901a\u4fe1\u589e\u52a0\u3001\u4e32\u884c\u5f00\u9500\u548cGPU\u5229\u7528\u7387\u4f4e\u4e0b\u800c\u964d\u4f4eGPU\u6027\u80fd", "motivation": "\u5206\u6790\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316(AMR)\u5728CPU-GPU\u5f02\u6784\u7cfb\u7edf\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u80fd\u6e90\u90e8\u5373\u5c06\u90e8\u7f72\u7684\u5f02\u6784\u8d85\u7ea7\u8ba1\u7b97\u673a\u63d0\u4f9b\u4f18\u5316\u6307\u5bfc", "method": "\u901a\u8fc7\u8be6\u7ec6\u6027\u80fd\u5206\u6790\u8bc6\u522bParthenon\u5757\u7ed3\u6784AMR\u57fa\u51c6\u6d4b\u8bd5\u5728CPU-GPU\u7cfb\u7edf\u4e2d\u7684\u6548\u7387\u4f4e\u4e0b\u3001\u4f4e\u5360\u7528\u7387\u548c\u5185\u5b58\u8bbf\u95ee\u74f6\u9888\u95ee\u9898", "result": "\u53d1\u73b0\u8f83\u5c0f\u7f51\u683c\u5757\u548c\u8f83\u6df1AMR\u5c42\u7ea7\u4f1a\u663e\u8457\u964d\u4f4eGPU\u6027\u80fd\uff0c\u8bc6\u522b\u4e86\u901a\u4fe1\u5f00\u9500\u3001\u4e32\u884c\u74f6\u9888\u548cGPU\u5229\u7528\u7387\u4e0d\u8db3\u7b49\u95ee\u9898", "conclusion": "\u63d0\u51fa\u4e86\u4f18\u5316\u5efa\u8bae\u4ee5\u63d0\u9ad8GPU\u541e\u5410\u91cf\u548c\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u8fd9\u4e9b\u89c1\u89e3\u53ef\u4e3a\u672a\u6765\u5728\u5f02\u6784\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u90e8\u7f72AMR\u63d0\u4f9b\u6307\u5bfc"}}
{"id": "2509.19918", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19918", "abs": "https://arxiv.org/abs/2509.19918", "authors": ["Micheline B\u00e9n\u00e9dicte Moumoula", "Serge Lionel Nikiema", "Alb\u00e9rick Euraste Djire", "Abdoul Kader Kabore", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation", "comment": null, "summary": "Producing high-quality code across multiple programming languages is\nincreasingly important as today's software systems are built on heterogeneous\nstacks. Large language models (LLMs) have advanced the state of automated\nprogramming, yet their proficiency varies sharply between languages, especially\nthose with limited training data such as Rust, Perl, OCaml, and Erlang. Many\ncurrent solutions including language-specific fine-tuning, multi-agent\norchestration, transfer learning, and intermediate-representation pipelines\nstill approach each target language in isolation, missing opportunities to\nshare knowledge or exploit recurring cross-language patterns.\n  XL-CoGen tackles this challenge with a coordinated multi-agent architecture\nthat integrates intermediate representation, code generation, translation, and\nautomated repair. Its distinguishing feature is a data-driven mechanism for\nselecting bridging languages: empirically derived transfer matrices identify\nthe best intermediate languages based on demonstrated translation success\nrather than raw generation accuracy. The system performs early output\nvalidation, iteratively corrects errors, and reuses intermediate artifacts as\ncontextual scaffolds for subsequent translations.\n  Extensive experiments show that XL-CoGen yields notable improvements with 13\npercentage-point gains over the strongest fine-tuned baseline and as much as 30\npercentage points over existing single-language multi-agent methods. Ablation\nstudies further demonstrate that compatibility-guided bridging significantly\noutperforms LLM-based heuristics, confirming the value of cumulative\ncross-language knowledge transfer.", "AI": {"tldr": "XL-CoGen\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u6865\u63a5\u8bed\u8a00\u9009\u62e9\u548c\u8de8\u8bed\u8a00\u77e5\u8bc6\u8f6c\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u7801\u751f\u6210\u8d28\u91cf", "motivation": "\u5f53\u524dLLM\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u95f4\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5dee\u5f02\u5f88\u5927\uff0c\u7279\u522b\u662f\u5bf9\u8bad\u7ec3\u6570\u636e\u8f83\u5c11\u7684\u8bed\u8a00\u5982Rust\u3001Perl\u7b49\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5b64\u7acb\u5904\u7406\u6bcf\u79cd\u8bed\u8a00\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8de8\u8bed\u8a00\u6a21\u5f0f\u548c\u77e5\u8bc6\u5171\u4eab\u7684\u673a\u4f1a", "method": "\u91c7\u7528\u534f\u8c03\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u6574\u5408\u4e2d\u95f4\u8868\u793a\u3001\u4ee3\u7801\u751f\u6210\u3001\u7ffb\u8bd1\u548c\u81ea\u52a8\u4fee\u590d\u3002\u6838\u5fc3\u7279\u70b9\u662f\u6570\u636e\u9a71\u52a8\u7684\u6865\u63a5\u8bed\u8a00\u9009\u62e9\u673a\u5236\uff0c\u57fa\u4e8e\u7ecf\u9a8c\u63a8\u5bfc\u7684\u8f6c\u79fb\u77e9\u9635\u8bc6\u522b\u6700\u4f73\u4e2d\u95f4\u8bed\u8a00", "result": "\u5b9e\u9a8c\u663e\u793aXL-CoGen\u76f8\u6bd4\u6700\u5f3a\u5fae\u8c03\u57fa\u7ebf\u63d0\u534713\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4\u73b0\u6709\u5355\u8bed\u8a00\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe30\u4e2a\u767e\u5206\u70b9\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u517c\u5bb9\u6027\u5f15\u5bfc\u7684\u6865\u63a5\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5", "conclusion": "\u8be5\u7cfb\u7edf\u8bc1\u660e\u4e86\u7d2f\u79ef\u8de8\u8bed\u8a00\u77e5\u8bc6\u8f6c\u79fb\u7684\u4ef7\u503c\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u6570\u636e\u9a71\u52a8\u7684\u6865\u63a5\u8bed\u8a00\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u5dee\u5f02\u95ee\u9898"}}
{"id": "2509.19539", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19539", "abs": "https://arxiv.org/abs/2509.19539", "authors": ["Raj Patel", "Umesh Biswas", "Surya Kodipaka", "Will Carroll", "Preston Peranich", "Maxwell Young"], "title": "A Survey of Recent Advancements in Secure Peer-to-Peer Networks", "comment": "30 pages, 4 figures, 2 tables", "summary": "Peer-to-peer (P2P) networks are a cornerstone of modern computing, and their\nsecurity is an active area of research. Many defenses with strong security\nguarantees have been proposed; however, the most-recent survey is over a decade\nold. This paper delivers an updated review of recent theoretical advances that\naddress classic threats, such as the Sybil and routing attacks, while\nhighlighting how emerging trends -- such as machine learning, social networks,\nand dynamic systems -- pose new challenges and drive novel solutions. We\nevaluate the strengths and weaknesses of these solutions and suggest directions\nfor future research.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8eP2P\u7f51\u7edc\u5b89\u5168\u6700\u65b0\u7406\u8bba\u8fdb\u5c55\u7684\u7efc\u8ff0\u8bba\u6587\uff0c\u66f4\u65b0\u4e86\u5341\u591a\u5e74\u524d\u7684\u65e7\u8c03\u67e5\uff0c\u91cd\u70b9\u5173\u6ce8\u7ecf\u5178\u5a01\u80c1\u548c\u65b0\u5174\u8d8b\u52bf\u5e26\u6765\u7684\u65b0\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "P2P\u7f51\u7edc\u662f\u73b0\u4ee3\u8ba1\u7b97\u7684\u57fa\u77f3\uff0c\u5176\u5b89\u5168\u6027\u662f\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\u3002\u867d\u7136\u5df2\u6709\u8bb8\u591a\u5177\u6709\u5f3a\u5b89\u5168\u4fdd\u8bc1\u7684\u9632\u5fa1\u65b9\u6848\u88ab\u63d0\u51fa\uff0c\u4f46\u6700\u8fd1\u7684\u8c03\u67e5\u5df2\u8d85\u8fc7\u5341\u5e74\uff0c\u9700\u8981\u66f4\u65b0\u7684\u7efc\u8ff0\u6765\u53cd\u6620\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u56de\u987e\u548c\u5206\u6790\u8fd1\u5e74\u6765\u9488\u5bf9P2P\u7f51\u7edc\u5b89\u5168\u7684\u7406\u8bba\u8fdb\u5c55\uff0c\u5305\u62ec\u5bf9\u7ecf\u5178\u5a01\u80c1\uff08\u5982Sybil\u653b\u51fb\u548c\u8def\u7531\u653b\u51fb\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u53ca\u65b0\u5174\u8d8b\u52bf\uff08\u5982\u673a\u5668\u5b66\u4e60\u3001\u793e\u4ea4\u7f51\u7edc\u548c\u52a8\u6001\u7cfb\u7edf\uff09\u5e26\u6765\u7684\u65b0\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5404\u79cd\u89e3\u51b3\u65b9\u6848\u7684\u4f18\u52bf\u548c\u52a3\u52bf\uff0c\u63d0\u4f9b\u4e86\u5bf9P2P\u7f51\u7edc\u5b89\u5168\u73b0\u72b6\u7684\u5168\u9762\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5f3a\u9879\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u5f53\u524dP2P\u7f51\u7edc\u5b89\u5168\u7814\u7a76\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u4e3a\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u548c\u6307\u5bfc\u3002"}}
{"id": "2509.20010", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20010", "abs": "https://arxiv.org/abs/2509.20010", "authors": ["Xiaoning Ren", "Yuhang Ye", "Xiongfei Wu", "Yueming Wu", "Yinxing Xue"], "title": "Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories", "comment": "11pages,8figures", "summary": "Neural networks have become integral to many fields due to their exceptional\nperformance. The open-source community has witnessed a rapid influx of neural\nnetwork (NN) repositories with fast-paced iterations, making it crucial for\npractitioners to analyze their evolution to guide development and stay ahead of\ntrends. While extensive research has explored traditional software evolution\nusing Software Bill of Materials (SBOMs), these are ill-suited for NN software,\nwhich relies on pre-defined modules and pre-trained models (PTMs) with distinct\ncomponent structures and reuse patterns. Conceptual AI Bills of Materials\n(AIBOMs) also lack practical implementations for large-scale evolutionary\nanalysis. To fill this gap, we introduce the Neural Network Bill of Material\n(NNBOM), a comprehensive dataset construct tailored for NN software. We create\na large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,\ncataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct\na comprehensive empirical study of neural network software evolution across\nsoftware scale, component reuse, and inter-domain dependency, providing\nmaintainers and developers with a holistic view of its long-term trends.\nBuilding on these findings, we develop two prototype applications,\n\\textit{Multi repository Evolution Analyzer} and \\textit{Single repository\nComponent Assessor and Recommender}, to demonstrate the practical value of our\nanalysis.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u7f51\u7edc\u7269\u6599\u6e05\u5355(NNBOM)\u6982\u5ff5\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u8f6f\u4ef6\u6f14\u5316\u8d8b\u52bf", "motivation": "\u4f20\u7edfSBOM\u4e0d\u9002\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u8f6f\u4ef6\uff0c\u73b0\u6709AIBOM\u7f3a\u4e4f\u5927\u89c4\u6a21\u6f14\u5316\u5206\u6790\u5b9e\u8df5\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u5206\u6790NN\u8f6f\u4ef6\u6f14\u5316", "method": "\u4ece55,997\u4e2aPyTorch GitHub\u4ed3\u5e93\u521b\u5efaNNBOM\u6570\u636e\u5e93\uff0c\u5206\u6790\u7b2c\u4e09\u65b9\u5e93\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6a21\u5757\u7684\u4f7f\u7528\u6a21\u5f0f", "result": "\u5efa\u7acb\u4e86\u5927\u89c4\u6a21NNBOM\u6570\u636e\u5e93\uff0c\u8fdb\u884c\u4e86\u8de8\u8f6f\u4ef6\u89c4\u6a21\u3001\u7ec4\u4ef6\u91cd\u7528\u548c\u8de8\u9886\u57df\u4f9d\u8d56\u7684\u5168\u9762\u5b9e\u8bc1\u7814\u7a76", "conclusion": "NNBOM\u4e3a\u795e\u7ecf\u7f51\u7edc\u8f6f\u4ef6\u6f14\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5f00\u53d1\u7684\u539f\u578b\u5e94\u7528\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2509.20136", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20136", "abs": "https://arxiv.org/abs/2509.20136", "authors": ["Wei Zhang", "Jack Yang", "Renshuai Tao", "Lingzheng Chai", "Shawn Guo", "Jiajun Wu", "Xiaoming Chen", "Ganqu Cui", "Ning Ding", "Xander Xu", "Hu Wei", "Bowen Zhou"], "title": "V-GameGym: Visual Game Generation for Code Large Language Models", "comment": null, "summary": "Code large language models have demonstrated remarkable capabilities in\nprogramming tasks, yet current benchmarks primarily focus on single modality\nrather than visual game development. Most existing code-related benchmarks\nevaluate syntax correctness and execution accuracy, overlooking critical\ngame-specific metrics such as playability, visual aesthetics, and user\nengagement that are essential for real-world deployment. To address the gap\nbetween current LLM capabilities in algorithmic problem-solving and competitive\nprogramming versus the comprehensive requirements of practical game\ndevelopment, we present V-GameGym, a comprehensive benchmark comprising 2,219\nhigh-quality samples across 100 thematic clusters derived from real-world\nrepositories, adopting a novel clustering-based curation methodology to ensure\nboth diversity and structural completeness. Further, we introduce a multimodal\nevaluation framework with an automated LLM-driven pipeline for visual code\nsynthesis using complete UI sandbox environments. Our extensive analysis\nreveals that V-GameGym effectively bridges the gap between code generation\naccuracy and practical game development workflows, providing quantifiable\nquality metrics for visual programming and interactive element generation.", "AI": {"tldr": "V-GameGym\u662f\u4e00\u4e2a\u9488\u5bf9\u89c6\u89c9\u6e38\u620f\u5f00\u53d1\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,219\u4e2a\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u586b\u8865\u4e86\u73b0\u6709\u4ee3\u7801LLM\u57fa\u51c6\u5728\u6e38\u620f\u5f00\u53d1\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u8bed\u6cd5\u6b63\u786e\u6027\u548c\u6267\u884c\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u6e38\u620f\u5f00\u53d1\u4e2d\u5173\u952e\u7684\u73a9\u6027\u3001\u89c6\u89c9\u7f8e\u5b66\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u7b49\u5b9e\u9645\u90e8\u7f72\u6240\u9700\u7684\u6307\u6807\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u7b5b\u9009\u65b9\u6cd5\u4ece\u771f\u5b9e\u4ed3\u5e93\u4e2d\u63d0\u53d6100\u4e2a\u4e3b\u9898\u96c6\u7fa4\u7684\u6837\u672c\uff0c\u6784\u5efa\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\u548c\u81ea\u52a8\u5316LLM\u9a71\u52a8\u7684\u89c6\u89c9\u4ee3\u7801\u5408\u6210\u6d41\u6c34\u7ebf\u3002", "result": "V-GameGym\u6709\u6548\u8fde\u63a5\u4e86\u4ee3\u7801\u751f\u6210\u51c6\u786e\u6027\u4e0e\u5b9e\u9645\u6e38\u620f\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e3a\u89c6\u89c9\u7f16\u7a0b\u548c\u4ea4\u4e92\u5143\u7d20\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u8d28\u91cf\u6307\u6807\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u4ee3\u7801LLM\u5728\u7b97\u6cd5\u95ee\u9898\u89e3\u51b3\u4e0e\u5b8c\u6574\u6e38\u620f\u5f00\u53d1\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2509.19729", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19729", "abs": "https://arxiv.org/abs/2509.19729", "authors": ["Haoyu Chen", "Xue Li", "Kun Qian", "Yu Guan", "Jin Zhao", "Xin Wang"], "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient LLM Inference", "comment": "12 pages, 15 figures", "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.", "AI": {"tldr": "Gyges\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u5e76\u884c\u7b56\u7565\u6765\u5904\u7406LLM\u670d\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d8\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u7684\u540c\u65f6\u652f\u6301\u66f4\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6", "motivation": "LLM\u670d\u52a1\u4e2d\u8bf7\u6c42\u7684\u52a8\u6001\u6027\uff08\u7279\u522b\u662f\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d8\u5316\uff09\u9700\u8981\u9ad8\u6548\u5904\u7406\uff0c\u4f46\u73b0\u6709\u5e76\u884c\u7b56\u7565\uff08\u5982Tensor Parallelism\uff09\u5728\u652f\u6301\u66f4\u5927\u4e0a\u4e0b\u6587\u65f6\u4f1a\u964d\u4f4e\u6574\u4f53\u541e\u5410\u91cf", "method": "\u63d0\u51faCross-Instance Parallelism Transformation (Gyges)\uff0c\u5305\u62ec\uff1a(1)\u9875\u9762\u53cb\u597d\u7684\u5934\u90e8\u4e2d\u5fc3\u5e03\u5c40\u52a0\u901fKV\u7f13\u5b58\u8f6c\u6362\uff1b(2)\u4e13\u7528\u6743\u91cd\u586b\u5145\u52a0\u901f\u6a21\u578b\u6743\u91cd\u8f6c\u6362\uff1b(3)\u8f6c\u6362\u611f\u77e5\u8c03\u5ea6\u5668\u534f\u540c\u8c03\u5ea6\u8bf7\u6c42\u548c\u5e76\u884c\u8f6c\u6362", "result": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754ctrace\u8bc4\u4f30\u663e\u793a\uff0cGyges\u76f8\u6bd4\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u5c06\u541e\u5410\u91cf\u63d0\u9ad8\u4e861.75\u500d\u52306.57\u500d", "conclusion": "Gyges\u901a\u8fc7\u81ea\u9002\u5e94\u5e76\u884c\u7b56\u7565\u8f6c\u6362\u6709\u6548\u89e3\u51b3\u4e86LLM\u670d\u52a1\u4e2d\u4e0a\u4e0b\u6587\u957f\u5ea6\u52a8\u6001\u53d8\u5316\u4e0e\u541e\u5410\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898"}}
{"id": "2509.20149", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20149", "abs": "https://arxiv.org/abs/2509.20149", "authors": ["Jianzhang Zhang", "Jialong Zhou", "Nan Niu", "Chuang Liu"], "title": "Enhancing Requirement Traceability through Data Augmentation Using Large Language Models", "comment": null, "summary": "Requirements traceability is crucial in software engineering to ensure\nconsistency between requirements and code. However, existing automated\ntraceability methods are constrained by the scarcity of training data and\nchallenges in bridging the semantic gap between artifacts. This study aims to\naddress the data scarcity problem in requirements traceability by employing\nlarge language models (LLMs) for data augmentation. We propose a novel approach\nthat utilizes prompt-based techniques with LLMs to generate augmented\nrequirement-to-code trace links, thereby enhancing the training dataset. Four\nLLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both\nzero-shot and few-shot templates. Moreover, we optimized the encoder component\nof the tracing model to improve its efficiency and adaptability to augmented\ndata. The key contributions of this paper are: (1) proposing and evaluating\nfour prompt templates for data augmentation; (2) providing a comparative\nanalysis of four LLMs for generating trace links; (3) enhancing the model's\nencoder for improved adaptability to augmented datasets. Experimental results\nshow that our approach significantly enhances model performance, achieving an\nF1 score improvement of up to 28.59%, thus demonstrating its effectiveness and\npotential for practical application.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u751f\u6210\u9700\u6c42\u5230\u4ee3\u7801\u7684\u8ffd\u8e2a\u94fe\u63a5\uff0c\u663e\u8457\u63d0\u5347\u9700\u6c42\u8ffd\u8e2a\u6a21\u578b\u7684\u6027\u80fd\uff0cF1\u5206\u6570\u6700\u9ad8\u63d0\u534728.59%\u3002", "motivation": "\u89e3\u51b3\u9700\u6c42\u8ffd\u8e2a\u4e2d\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u4eba\u5de5\u5236\u54c1\u95f4\u8bed\u4e49\u9e3f\u6c9f\u7684\u95ee\u9898\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684LLM\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4f7f\u75284\u79cdLLM\uff08Gemini 1.5 Pro\u3001Claude 3\u3001GPT-3.5\u3001GPT-4\uff09\u548c\u96f6\u6837\u672c/\u5c11\u6837\u672c\u6a21\u677f\u751f\u6210\u8ffd\u8e2a\u94fe\u63a5\uff0c\u5e76\u4f18\u5316\u8ffd\u8e2a\u6a21\u578b\u7684\u7f16\u7801\u5668\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cF1\u5206\u6570\u6700\u9ad8\u63d0\u534728.59%\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "LLM\u6570\u636e\u589e\u5f3a\u662f\u89e3\u51b3\u9700\u6c42\u8ffd\u8e2a\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u548c\u6a21\u578b\u4f18\u5316\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8ffd\u8e2a\u6027\u80fd\uff0c\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2509.19836", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19836", "abs": "https://arxiv.org/abs/2509.19836", "authors": ["Ao Sun", "Weilin Zhao", "Xu Han", "Cheng Yang", "Zhiyuan Liu", "Chuan Shi", "Maosong sun"], "title": "BurstEngine: an Efficient Distributed Framework for Training Transformers on Extremely Long Sequences of over 1M Tokens", "comment": null, "summary": "Existing methods for training LLMs on long-sequence data, such as Tensor\nParallelism and Context Parallelism, exhibit low Model FLOPs Utilization as\nsequence lengths and number of GPUs increase, especially when sequence lengths\nexceed 1M tokens. To address these challenges, we propose BurstEngine, an\nefficient framework designed to train LLMs on long-sequence data. BurstEngine\nintroduces BurstAttention, an optimized distributed attention with lower\ncommunication cost than RingAttention. BurstAttention leverages topology-aware\nring communication to fully utilize network bandwidth and incorporates\nfine-grained communication-computation overlap. Furthermore, BurstEngine\nintroduces sequence-level selective checkpointing and fuses the language\nmodeling head with the loss function to reduce memory cost. Additionally,\nBurstEngine introduces workload balance optimization for various types of\nattention masking. By integrating these optimizations, BurstEngine achieves a\n$1.2\\times$ speedup with much lower memory overhead than the state-of-the-art\nbaselines when training LLMs on extremely long sequences of over 1M tokens. We\nhave made our code publicly available on GitHub:\nhttps://github.com/thunlp/BurstEngine.", "AI": {"tldr": "BurstEngine\u662f\u4e00\u4e2a\u9ad8\u6548\u8bad\u7ec3LLM\u957f\u5e8f\u5217\u6570\u636e\u7684\u6846\u67b6\uff0c\u901a\u8fc7BurstAttention\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u5f15\u5165\u5e8f\u5217\u7ea7\u9009\u62e9\u6027\u68c0\u67e5\u70b9\u548c\u8d1f\u8f7d\u5e73\u8861\u4f18\u5316\uff0c\u5728\u8d85\u8fc7100\u4e07token\u7684\u8d85\u957f\u5e8f\u5217\u8bad\u7ec3\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb1.2\u500d\u4e14\u5185\u5b58\u5f00\u9500\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982Tensor Parallelism\u548cContext Parallelism\u5728\u5904\u7406\u8d85\u957f\u5e8f\u5217\uff08\u8d85\u8fc7100\u4e07token\uff09\u65f6\uff0c\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u548cGPU\u6570\u91cf\u589e\u52a0\uff0c\u6a21\u578bFLOPs\u5229\u7528\u7387\u8f83\u4f4e\u3002", "method": "\u63d0\u51faBurstEngine\u6846\u67b6\uff0c\u5305\u542b\uff1a1) BurstAttention - \u4f18\u5316\u7684\u5206\u5e03\u5f0f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u7528\u62d3\u6251\u611f\u77e5\u73af\u5f62\u901a\u4fe1\u5145\u5206\u5229\u7528\u7f51\u7edc\u5e26\u5bbd\uff1b2) \u7ec6\u7c92\u5ea6\u901a\u4fe1\u8ba1\u7b97\u91cd\u53e0\uff1b3) \u5e8f\u5217\u7ea7\u9009\u62e9\u6027\u68c0\u67e5\u70b9\uff1b4) \u8bed\u8a00\u5efa\u6a21\u5934\u4e0e\u635f\u5931\u51fd\u6570\u878d\u5408\uff1b5) \u5404\u79cd\u6ce8\u610f\u529b\u63a9\u7801\u7c7b\u578b\u7684\u8d1f\u8f7d\u5e73\u8861\u4f18\u5316\u3002", "result": "\u5728\u8d85\u8fc7100\u4e07token\u7684\u8d85\u957f\u5e8f\u5217\u8bad\u7ec3\u4e2d\uff0cBurstEngine\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e861.2\u500d\u7684\u52a0\u901f\uff0c\u5e76\u4e14\u5185\u5b58\u5f00\u9500\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "BurstEngine\u901a\u8fc7\u591a\u9879\u4f18\u5316\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u548c\u5185\u5b58\u95ee\u9898\uff0c\u4e3a\u8bad\u7ec3\u8d85\u957f\u5e8f\u5217LLM\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20172", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20172", "abs": "https://arxiv.org/abs/2509.20172", "authors": ["Daniel Maninger", "Leon Chemnitz", "Amir Molzam Sharifloo", "Jannis Brugger", "Mira Mezini"], "title": "Benchmarking Web API Integration Code Generation", "comment": "To be published in Proceedings of 2nd ACM International Conference on\n  AI-powered Software, Benchmark & Dataset Track (AIware '25)", "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models~(LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks.", "AI": {"tldr": "LLM\u5728\u751f\u6210Web API\u8c03\u7528\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5f00\u6e90\u6a21\u578b\u5728API\u96c6\u6210\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\u4f4e\u4e8e40%\uff0c\u5b58\u5728\u7aef\u70b9\u5e7b\u89c9\u548c\u53c2\u6570\u9519\u8bef\u7b49\u95ee\u9898", "motivation": "API\u96c6\u6210\u662f\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u7684\u6838\u5fc3\uff0c\u4f46\u7f16\u5199\u6b63\u786e\u7684API\u8c03\u7528\u4ee3\u7801\u5177\u6709\u6311\u6218\u6027\u3002\u867d\u7136LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8d8a\u6765\u8d8a\u6d41\u884c\uff0c\u4f46\u5176\u5728\u81ea\u52a8\u5316\u751f\u6210Web API\u96c6\u6210\u4ee3\u7801\u65b9\u9762\u7684\u6709\u6548\u6027\u5c1a\u672a\u88ab\u63a2\u7d22", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210Web API\u8c03\u7528\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u5e76\u5bf9\u591a\u4e2a\u5f00\u6e90LLM\u8fdb\u884c\u4e86\u5b9e\u9a8c", "result": "\u5b9e\u9a8c\u663e\u793a\u751f\u6210API\u8c03\u7528\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u5bfc\u81f4\u5e7b\u89c9\u7aef\u70b9\u3001\u9519\u8bef\u53c2\u6570\u4f7f\u7528\u7b49\u95ee\u9898\u3002\u6240\u6709\u8bc4\u4f30\u7684\u5f00\u6e90\u6a21\u578b\u90fd\u65e0\u6cd5\u89e3\u51b3\u8d85\u8fc740%\u7684\u4efb\u52a1", "conclusion": "\u5f53\u524d\u7684\u5f00\u6e90LLM\u5728\u751f\u6210Web API\u96c6\u6210\u4ee3\u7801\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u8868\u73b0"}}
{"id": "2509.20160", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20160", "abs": "https://arxiv.org/abs/2509.20160", "authors": ["Prashanthi S. K.", "Sai Anuroop Kesanapalli", "Yogesh Simmhan"], "title": "Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models", "comment": "Preprint of article in ACM SIGMETRICS 2023", "summary": "Deep Neural Networks (DNNs) have had a significant impact on domains like\nautonomous vehicles and smart cities through low-latency inferencing on edge\ncomputing devices close to the data source. However, DNN training on the edge\nis poorly explored. Techniques like federated learning and the growing capacity\nof GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a\nholistic characterization of DNN training on the edge. Training DNNs is\nresource-intensive and can stress an edge's GPU, CPU, memory and storage\ncapacities. Edge devices also have different resources compared to workstations\nand servers, such as slower shared memory and diverse storage media. Here, we\nperform a principled study of DNN training on individual devices of three\ncontemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three\ndiverse DNN model--dataset combinations. We vary device and training parameters\nsuch as I/O pipelining and parallelism, storage media, mini-batch sizes and\npower modes, and examine their effect on CPU and GPU utilization, fetch stalls,\ntraining time, energy usage, and variability. Our analysis exposes several\nresource inter-dependencies and counter-intuitive insights, while also helping\nquantify known wisdom. Our rigorous study can help tune the training\nperformance on the edge, trade-off time and energy usage on constrained\ndevices, and even select an ideal edge hardware for a DNN workload, and, in\nfuture, extend to federated learning too. As an illustration, we use these\nresults to build a simple model to predict the training time and energy per\nepoch for any given DNN across different power modes, with minimal additional\nprofiling.", "AI": {"tldr": "\u5bf9\u4e09\u79cdNVIDIA Jetson\u8fb9\u7f18\u8bbe\u5907\u8fdb\u884c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u5168\u9762\u6027\u80fd\u5206\u6790\uff0c\u7814\u7a76\u4e0d\u540c\u53c2\u6570\u5bf9\u8bad\u7ec3\u65f6\u95f4\u3001\u80fd\u8017\u548c\u8d44\u6e90\u5229\u7528\u7387\u7684\u5f71\u54cd", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684DNN\u8bad\u7ec3\u7814\u7a76\u4e0d\u8db3\uff0c\u800c\u8054\u90a6\u5b66\u4e60\u548cGPU\u52a0\u901f\u8fb9\u7f18\u8bbe\u5907\u7684\u53d1\u5c55\u9700\u8981\u7cfb\u7edf\u6027\u5730\u4e86\u89e3\u8fb9\u7f18\u8bad\u7ec3\u7684\u7279\u6027", "method": "\u5728\u4e09\u79cdJetson\u8bbe\u5907\u4e0a\u5bf9\u4e09\u79cdDNN\u6a21\u578b-\u6570\u636e\u96c6\u7ec4\u5408\u8fdb\u884c\u8bad\u7ec3\uff0c\u8c03\u6574I/O\u6d41\u6c34\u7ebf\u3001\u5b58\u50a8\u4ecb\u8d28\u3001\u6279\u5927\u5c0f\u548c\u529f\u7387\u6a21\u5f0f\u7b49\u53c2\u6570\uff0c\u5206\u6790CPU/GPU\u5229\u7528\u7387\u3001\u8bad\u7ec3\u65f6\u95f4\u3001\u80fd\u8017\u7b49\u6307\u6807", "result": "\u63ed\u793a\u4e86\u8d44\u6e90\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u548c\u53cd\u76f4\u89c9\u7684\u53d1\u73b0\uff0c\u5efa\u7acb\u4e86\u9884\u6d4b\u8bad\u7ec3\u65f6\u95f4\u548c\u80fd\u8017\u7684\u7b80\u5355\u6a21\u578b", "conclusion": "\u7814\u7a76\u6709\u52a9\u4e8e\u4f18\u5316\u8fb9\u7f18\u8bad\u7ec3\u6027\u80fd\uff0c\u5728\u53d7\u9650\u8bbe\u5907\u4e0a\u6743\u8861\u65f6\u95f4\u548c\u80fd\u8017\uff0c\u5e76\u4e3aDNN\u5de5\u4f5c\u8d1f\u8f7d\u9009\u62e9\u5408\u9002\u7684\u8fb9\u7f18\u786c\u4ef6"}}
{"id": "2509.20215", "categories": ["cs.SE", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.20215", "abs": "https://arxiv.org/abs/2509.20215", "authors": ["Guang Yang", "Wei Zheng", "Xiang Chen", "Yifan Sun", "Fengji Zhang", "Terry Yue Zhuo"], "title": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation", "comment": "Under review ICASSP 2026", "summary": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods.", "AI": {"tldr": "VCD-RNK\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8eVerilog\u4ee3\u7801\u91cd\u6392\u7684\u5224\u522b\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u62df\u4e13\u5bb6\u5728\u4ee3\u7801\u8bed\u4e49\u5206\u6790\u3001\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u548c\u529f\u80fd\u6b63\u786e\u6027\u8bc4\u4f30\u4e09\u4e2a\u7ef4\u5ea6\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u65e0\u9700\u6267\u884c\u8ba1\u7b97\u5bc6\u96c6\u7684\u6d4b\u8bd5\u5c31\u80fd\u6709\u6548\u63d0\u5347Verilog\u4ee3\u7801\u751f\u6210\u8d28\u91cf", "motivation": "\u89e3\u51b3LLMs\u5728Verilog\u751f\u6210\u4e2d\u7531\u4e8e\u9886\u57df\u77e5\u8bc6\u6709\u9650\u5bfc\u81f4\u7684\u4e0d\u53ef\u9760\u95ee\u9898\uff0c\u786c\u4ef6\u5de5\u7a0b\u5e08\u9700\u8981\u4e00\u4e2a\u53ef\u4fe1\u8d56\u7684\u89e3\u51b3\u65b9\u6848\u800c\u975e\u591a\u4e2a\u4e0d\u786e\u5b9a\u7684\u5019\u9009", "method": "\u5c06Verilog\u751f\u6210\u95ee\u9898\u5efa\u6a21\u4e3a\u9700\u6c42\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51faVCD-RNK\u5224\u522b\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6574\u5408Verilog\u4e13\u5bb6\u5728\u4e09\u4e2a\u7ef4\u5ea6\u7684\u63a8\u7406\u80fd\u529b", "result": "VCD-RNK\u80fd\u591f\u6709\u6548\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u4e2d\u8ba1\u7b97\u5bc6\u96c6\u7684\u6d4b\u8bd5\u6267\u884c\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684Verilog\u4ee3\u7801\u91cd\u6392", "conclusion": "VCD-RNK\u4e3a\u89e3\u51b3Verilog\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5224\u522b\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4e13\u5bb6\u63a8\u7406\u8fc7\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4ee3\u7801\u7684\u53ef\u4fe1\u5ea6"}}
{"id": "2509.20189", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20189", "abs": "https://arxiv.org/abs/2509.20189", "authors": ["Prashanthi S. K.", "Kunal Kumar Sahoo", "Amartya Ranjan Saikia", "Pranav Gupta", "Atharva Vinay Joshi", "Priyanshu Pansari", "Yogesh Simmhan"], "title": "Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge Accelerators", "comment": null, "summary": "Edge accelerators such as Nvidia Jetsons are becoming an integral part of the\ncomputing continuum, and are often used for DNN inferencing and training.\nNvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power\nenvelope and offer $1000$s of power modes to customize CPU, GPU and memory\nfrequencies. Their widely varying power--performance trade-offs can be\nexploited for energy and power-constrained deployments. While data-driven\nmethods to predict the power and latency of DNN workloads for edge devices\nexist, there is a lack of principled study to understand why edge accelerators\nand their power modes perform the way they do. We develop a time roofline and a\nnovel energy roofline model for the Jetson Orin AGX for diverse power modes,\nand couple it with an analytical model of the compute (FLOP) and memory access\n(bytes) for DNN inference workloads to analyze them from first principles.\nThese reveal unique, sometimes counter-intuitive, insights into the power and\nperformance behavior of DNN workloads on edge accelerators, e.g., the default\npower mode MAXN is not the most energy efficient and time efficiency implies\nenergy efficiency for all power modes. We also extend our analytical roofline\nmodels to DNN training. Finally, we apply these methods to tune the power mode\n(and hence the roofline) of the edge device to optimize the latency and energy\nfor DNN inference, with up to $15\\%$ lower energy and minimal degradation in\ninference time.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u65f6\u95f4\u5c4b\u9876\u7ebf\u548c\u80fd\u91cf\u5c4b\u9876\u7ebf\u6a21\u578b\u6765\u5206\u6790Nvidia Jetson\u8fb9\u7f18\u52a0\u901f\u5668\u5728\u4e0d\u540c\u529f\u8017\u6a21\u5f0f\u4e0b\u7684DNN\u63a8\u7406\u6027\u80fd\uff0c\u53d1\u73b0\u9ed8\u8ba4MAXN\u6a21\u5f0f\u5e76\u975e\u6700\u8282\u80fd\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u65b9\u6cd5\u53ef\u964d\u4f4e15%\u80fd\u8017\u4e14\u6700\u5c0f\u5316\u63a8\u7406\u65f6\u95f4\u635f\u5931\u3002", "motivation": "\u8fb9\u7f18\u52a0\u901f\u5668\u5982Nvidia Jetson\u5728\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u529f\u8017\u6a21\u5f0f\u6027\u80fd\u884c\u4e3a\u7684\u539f\u7406\u6027\u7814\u7a76\uff0c\u9700\u8981\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u5206\u6790DNN\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u65f6\u95f4\u5c4b\u9876\u7ebf\u548c\u80fd\u91cf\u5c4b\u9876\u7ebf\u6a21\u578b\uff0c\u7ed3\u5408DNN\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8ba1\u7b97(FLOP)\u548c\u5185\u5b58\u8bbf\u95ee(\u5b57\u8282)\u5206\u6790\u6a21\u578b\uff0c\u5bf9Jetson Orin AGX\u7684\u4e0d\u540c\u529f\u8017\u6a21\u5f0f\u8fdb\u884c\u539f\u7406\u6027\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86DNN\u5de5\u4f5c\u8d1f\u8f7d\u5728\u8fb9\u7f18\u52a0\u901f\u5668\u4e0a\u7684\u72ec\u7279\u89c1\u89e3\uff1a\u9ed8\u8ba4MAXN\u6a21\u5f0f\u5e76\u975e\u6700\u8282\u80fd\uff0c\u65f6\u95f4\u6548\u7387\u5728\u6240\u6709\u529f\u8017\u6a21\u5f0f\u4e0b\u90fd\u610f\u5473\u7740\u80fd\u91cf\u6548\u7387\u3002\u901a\u8fc7\u4f18\u5316\u529f\u8017\u6a21\u5f0f\u53ef\u5b9e\u73b015%\u7684\u80fd\u8017\u964d\u4f4e\u4e14\u63a8\u7406\u65f6\u95f4\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u5c4b\u9876\u7ebf\u6a21\u578b\u548c\u5206\u6790\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7406\u89e3\u548c\u4f18\u5316\u8fb9\u7f18\u52a0\u901f\u5668\u7684DNN\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u80fd\u91cf\u548c\u529f\u8017\u53d7\u9650\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2509.20300", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20300", "abs": "https://arxiv.org/abs/2509.20300", "authors": ["Jannis Kiesel", "Jonathan Heiss"], "title": "Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs", "comment": null, "summary": "Ensuring the integrity of business processes without disclosing confidential\nbusiness information is a major challenge in inter-organizational processes.\nThis paper introduces a zero-knowledge proof (ZKP)-based approach for the\nverifiable execution of business processes while preserving confidentiality. We\nintegrate ZK virtual machines (zkVMs) into business process management engines\nthrough a comprehensive system architecture and a prototypical implementation.\nOur approach supports chained verifiable computations through proof\ncompositions. On the example of product carbon footprinting, we model\nsequential footprinting activities and demonstrate how organizations can prove\nand verify the integrity of verifiable processes without exposing sensitive\ninformation. We assess different ZKP proving variants within process models for\ntheir efficiency in proving and verifying, and discuss the practical\nintegration of ZKPs throughout the Business Process Management (BPM) lifecycle.\nOur experiment-driven evaluation demonstrates the automation of process\nverification under given confidentiality constraints.", "AI": {"tldr": "\u57fa\u4e8e\u96f6\u77e5\u8bc6\u8bc1\u660e\u7684\u4e1a\u52a1\u6d41\u7a0b\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5728\u4fdd\u62a4\u5546\u4e1a\u673a\u5bc6\u7684\u540c\u65f6\u786e\u4fdd\u6d41\u7a0b\u6267\u884c\u5b8c\u6574\u6027\uff0c\u901a\u8fc7zkVM\u96c6\u6210\u548c\u539f\u578b\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u78b3\u8db3\u8ff9\u8ba1\u7b97\u7b49\u573a\u666f\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u89e3\u51b3\u8de8\u7ec4\u7ec7\u4e1a\u52a1\u6d41\u7a0b\u4e2d\u65e2\u8981\u786e\u4fdd\u6d41\u7a0b\u5b8c\u6574\u6027\u53c8\u4e0d\u80fd\u6cc4\u9732\u5546\u4e1a\u673a\u5bc6\u7684\u53cc\u91cd\u6311\u6218\uff0c\u4e3a\u53ef\u4fe1\u4e1a\u52a1\u6d41\u7a0b\u6267\u884c\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u96f6\u77e5\u8bc6\u8bc1\u660e\u865a\u62df\u673a(zkVM)\u96c6\u6210\u5230\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u5f15\u64ce\u4e2d\uff0c\u91c7\u7528\u7cfb\u7edf\u67b6\u6784\u548c\u539f\u578b\u5b9e\u73b0\uff0c\u652f\u6301\u901a\u8fc7\u8bc1\u660e\u7ec4\u5408\u5b9e\u73b0\u94fe\u5f0f\u53ef\u9a8c\u8bc1\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u7ed9\u5b9a\u4fdd\u5bc6\u7ea6\u675f\u4e0b\u81ea\u52a8\u5316\u6d41\u7a0b\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540cZKP\u8bc1\u660e\u53d8\u4f53\u5728\u6d41\u7a0b\u6a21\u578b\u4e2d\u7684\u6548\u7387\u548c\u9a8c\u8bc1\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684ZKP\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u4e1a\u52a1\u6d41\u7a0b\u7684\u53ef\u9a8c\u8bc1\u6267\u884c\u540c\u65f6\u4fdd\u62a4\u673a\u5bc6\u4fe1\u606f\uff0c\u4e3aBPM\u751f\u547d\u5468\u671f\u4e2d\u96f6\u77e5\u8bc6\u8bc1\u660e\u7684\u5b9e\u9645\u96c6\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.20205", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20205", "abs": "https://arxiv.org/abs/2509.20205", "authors": ["Prashanthi S. K.", "Saisamarth Taluri", "Pranav Gupta", "Amartya Ranjan Saikia", "Kunal Kumar Sahoo", "Atharva Vinay Joshi", "Lakshya Karwa", "Kedar Dhule", "Yogesh Simmhan"], "title": "Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge Accelerators", "comment": null, "summary": "The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the\nrise in privacy concerns are placing an emphasis on concurrent DNN training and\ninferencing on edge devices. Inference and training have different computing\nand QoS goals. But edge accelerators like Jetson do not support native GPU\nsharing and expose 1000s of power modes. This requires careful time-sharing of\nconcurrent workloads to meet power--performance goals, while limiting costly\nprofiling. In this paper, we design an intelligent time-slicing approach for\nconcurrent DNN training and inferencing on Jetsons. We formulate an\noptimization problem to interleave training and inferencing minibatches, and\ndecide the device power mode and inference minibatch size, while maximizing the\ntraining throughput and staying within latency and power budgets, with modest\nprofiling costs. We propose GMD, an efficient multi-dimensional gradient\ndescent search which profiles just $15$ power modes; and ALS, an Active\nLearning technique which identifies reusable Pareto-optimal power modes, but\nprofiles $50$--$150$ power modes. We evaluate these within our Fulcrum\nscheduler for $273,000+$ configurations across $15$ DNN workloads. We also\nevaluate our strategies on dynamic arrival inference and concurrent inferences.\nALS and GMD outperform simpler and more complex baselines with larger-scale\nprofiling. Their solutions satisfy the latency and power budget for $>97\\%$ of\nour runs, and on average are within $7\\%$ of the optimal throughput.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u65f6\u95f4\u5207\u7247\u65b9\u6cd5GMD\u548cALS\uff0c\u7528\u4e8e\u5728Jetson\u8fb9\u7f18\u8bbe\u5907\u4e0a\u540c\u65f6\u8fdb\u884cDNN\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u901a\u8fc7\u4f18\u5316\u529f\u7387\u6a21\u5f0f\u548c\u63a8\u7406\u6279\u6b21\u5927\u5c0f\u6765\u6700\u5927\u5316\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u540c\u65f6\u6ee1\u8db3\u5ef6\u8fdf\u548c\u529f\u8017\u9884\u7b97\u3002", "motivation": "\u968f\u7740GPU\u52a0\u901f\u8fb9\u7f18\u8bbe\u5907\u7684\u666e\u53ca\u548c\u9690\u79c1\u95ee\u9898\u7684\u589e\u52a0\uff0c\u9700\u8981\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u540c\u65f6\u8fdb\u884cDNN\u8bad\u7ec3\u548c\u63a8\u7406\u3002\u4f46Jetson\u7b49\u8bbe\u5907\u4e0d\u652f\u6301\u539f\u751fGPU\u5171\u4eab\u4e14\u5177\u6709\u6570\u5343\u79cd\u529f\u7387\u6a21\u5f0f\uff0c\u9700\u8981\u4ed4\u7ec6\u7684\u65f6\u95f4\u5171\u4eab\u6765\u6ee1\u8db3\u529f\u8017\u6027\u80fd\u76ee\u6807\u3002", "method": "\u8bbe\u8ba1\u4e86Fulcrum\u8c03\u5ea6\u5668\uff0c\u5305\u542bGMD\uff08\u9ad8\u6548\u591a\u7ef4\u68af\u5ea6\u4e0b\u964d\u641c\u7d22\uff0c\u4ec5\u5206\u679015\u79cd\u529f\u7387\u6a21\u5f0f\uff09\u548cALS\uff08\u4e3b\u52a8\u5b66\u4e60\u6280\u672f\uff0c\u8bc6\u522b\u53ef\u91cd\u7528\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u529f\u7387\u6a21\u5f0f\uff0c\u5206\u679050-150\u79cd\u529f\u7387\u6a21\u5f0f\uff09\u3002", "result": "\u572815\u4e2aDNN\u5de5\u4f5c\u8d1f\u8f7d\u7684273,000+\u914d\u7f6e\u4e2d\uff0cALS\u548cGMD\u4f18\u4e8e\u9700\u8981\u66f4\u5927\u89c4\u6a21\u5206\u6790\u7684\u57fa\u51c6\u65b9\u6cd5\u300297%\u4ee5\u4e0a\u7684\u8fd0\u884c\u6ee1\u8db3\u5ef6\u8fdf\u548c\u529f\u8017\u9884\u7b97\uff0c\u5e73\u5747\u541e\u5410\u91cf\u63a5\u8fd1\u6700\u4f18\u503c\u76847%\u4ee5\u5185\u3002", "conclusion": "\u63d0\u51fa\u7684\u667a\u80fd\u65f6\u95f4\u5207\u7247\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7ba1\u7406\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5e76\u53d1DNN\u8bad\u7ec3\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u4ee5\u8f83\u5c0f\u7684\u5206\u6790\u6210\u672c\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.20308", "categories": ["cs.SE", "68M15 (Primary), 68M12, 68Q42 (Secondary)", "D.2.5; C.2.2; F.4.2"], "pdf": "https://arxiv.org/pdf/2509.20308", "abs": "https://arxiv.org/abs/2509.20308", "authors": ["Alexander Liggesmeyer", "Jos\u00e9 Antonio Zamudio Amaya", "Andreas Zeller"], "title": "Protocol Testing with I/O Grammars", "comment": "20 pages", "summary": "Generating software tests faces two fundamental problems. First, one needs to\n_generate inputs_ that are syntactically and semantically correct, yet\nsufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check\noutputs_ whether a test case is correct or not. Both problems become apparent\nin _protocol testing_, where inputs are messages exchanged between parties, and\noutputs are the responses of these parties.\n  In this paper, we propose a novel approach to protocol testing that combines\ninput generation and output checking in a single framework. We introduce _I/O\ngrammars_ as the first means to _completely_ specify the syntax and semantics\nof protocols, including messages, states, and interactions. Our implementation,\nbased on the FANDANGO framework, takes a single I/O grammar, and can act as a\n_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a\n_server_, or both (or actually any number of parties), a versatility not found\nin any existing tool or formalism. User-defined _constraints}_can have the\ngenerator focus on arbitrary protocol features; $k$-path guidance\nsystematically covers states, messages, responses, and value alternatives in a\nunified fashion.\n  We evaluate the effectiveness of our approach by applying it to several\nprotocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can\nspecify advanced protocol features correctly and completely, while also\nenabling output validation of the programs under test. In its evaluation, we\nfind that systematic coverage of the I/O grammar results in much quicker\ncoverage of the input and response spaces (and thus functionality) compared to\nthe random-based state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eI/O\u8bed\u6cd5\u7684\u534f\u8bae\u6d4b\u8bd5\u65b0\u65b9\u6cd5\uff0c\u5c06\u8f93\u5165\u751f\u6210\u548c\u8f93\u51fa\u68c0\u67e5\u7edf\u4e00\u5728\u5355\u4e00\u6846\u67b6\u4e2d\uff0c\u80fd\u591f\u5b8c\u6574\u6307\u5b9a\u534f\u8bae\u8bed\u6cd5\u8bed\u4e49\u5e76\u5b9e\u73b0\u591a\u529f\u80fd\u6d4b\u8bd5\u3002", "motivation": "\u534f\u8bae\u6d4b\u8bd5\u9762\u4e34\u4e24\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u9700\u8981\u751f\u6210\u8bed\u6cd5\u8bed\u4e49\u6b63\u786e\u4e14\u591a\u6837\u5316\u7684\u8f93\u5165\uff0c\u4ee5\u53ca\u9700\u8981\u9a8c\u8bc1\u8f93\u51fa\u6b63\u786e\u6027\u7684\u9884\u8a00\u673a\u5236\u3002\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165I/O\u8bed\u6cd5\u4f5c\u4e3a\u5b8c\u6574\u6307\u5b9a\u534f\u8bae\u8bed\u6cd5\u8bed\u4e49\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8eFANDANGO\u6846\u67b6\u5b9e\u73b0\uff0c\u652f\u6301\u6d4b\u8bd5\u751f\u6210\u3001\u6a21\u62df\u5bf9\u8c61\u548c\u9884\u8a00\u529f\u80fd\uff0c\u5e76\u91c7\u7528k\u8def\u5f84\u5f15\u5bfc\u7cfb\u7edf\u5316\u8986\u76d6\u534f\u8bae\u72b6\u6001\u548c\u4ea4\u4e92\u3002", "result": "\u5728DNS\u3001FTP\u3001SMTP\u7b49\u534f\u8bae\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cI/O\u8bed\u6cd5\u80fd\u6b63\u786e\u5b8c\u6574\u5730\u6307\u5b9a\u9ad8\u7ea7\u534f\u8bae\u7279\u6027\uff0c\u7cfb\u7edf\u5316\u8986\u76d6\u76f8\u6bd4\u968f\u673a\u65b9\u6cd5\u80fd\u66f4\u5feb\u8986\u76d6\u8f93\u5165\u54cd\u5e94\u7a7a\u95f4\u548c\u529f\u80fd\u3002", "conclusion": "I/O\u8bed\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u534f\u8bae\u6d4b\u8bd5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8f93\u5165\u751f\u6210\u548c\u8f93\u51fa\u9a8c\u8bc1\u7684\u53cc\u91cd\u6311\u6218\uff0c\u5728\u534f\u8bae\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u968f\u673a\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2509.20223", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20223", "abs": "https://arxiv.org/abs/2509.20223", "authors": ["Md Jueal Mia", "M. Hadi Amini"], "title": "An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications", "comment": "i3CE 2024, 2024 ASCE International Conference on Computing in Civil\n  Engineering", "summary": "Federated Learning lends itself as a promising paradigm in enabling\ndistributed learning for autonomous vehicles applications and ensuring data\nprivacy while enhancing and refining predictive model performance through\ncollaborative training on edge client vehicles. However, it remains vulnerable\nto various categories of cyber-attacks, necessitating more robust security\nmeasures to effectively mitigate potential threats. Poisoning attacks and\ninference attacks are commonly initiated within the federated learning\nenvironment to compromise secure system performance. Secure aggregation can\nlimit the disclosure of sensitive information from outsider and insider\nattackers of the federated learning environment. In this study, our aim is to\nconduct an empirical analysis on the transportation image dataset (e.g., LISA\ntraffic light) using various secure aggregation techniques and multiparty\ncomputation in the presence of diverse categories of cyber-attacks. Multiparty\ncomputation serves as a state-of-the-art security mechanism, offering standard\nprivacy for secure aggregation of edge autonomous vehicles local model updates\nthrough various security protocols. The presence of adversaries can mislead the\nautonomous vehicle learning model, leading to the misclassification of traffic\nlights, and resulting in detrimental impacts. This empirical study explores the\nresilience of various secure federated learning aggregation techniques and\nmultiparty computation in safeguarding autonomous vehicle applications against\nvarious cyber threats during both training and inference times.", "AI": {"tldr": "\u672c\u6587\u5bf9\u8054\u90a6\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u91cd\u70b9\u7814\u7a76\u5b89\u5168\u805a\u5408\u6280\u672f\u548c\u591a\u65b9\u8ba1\u7b97\u5728\u5e94\u5bf9\u7f51\u7edc\u653b\u51fb\u65f6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u4ecd\u6613\u53d7\u6295\u6bd2\u653b\u51fb\u548c\u63a8\u7406\u653b\u51fb\u7b49\u7f51\u7edc\u5a01\u80c1\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u63aa\u65bd\u6765\u4fdd\u62a4\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u4ea4\u901a\u56fe\u50cf\u6570\u636e\u96c6\uff08\u5982LISA\u4ea4\u901a\u706f\u6570\u636e\u96c6\uff09\uff0c\u91c7\u7528\u591a\u79cd\u5b89\u5168\u805a\u5408\u6280\u672f\u548c\u591a\u65b9\u8ba1\u7b97\u534f\u8bae\uff0c\u5728\u5b58\u5728\u4e0d\u540c\u7c7b\u578b\u7f51\u7edc\u653b\u51fb\u7684\u73af\u5883\u4e0b\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5404\u79cd\u5b89\u5168\u8054\u90a6\u5b66\u4e60\u805a\u5408\u6280\u672f\u548c\u591a\u65b9\u8ba1\u7b97\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u4fdd\u62a4\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u514d\u53d7\u7f51\u7edc\u5a01\u80c1\u7684\u97e7\u6027\u3002", "conclusion": "\u591a\u65b9\u8ba1\u7b97\u4f5c\u4e3a\u5148\u8fdb\u7684\u5b89\u5168\u673a\u5236\uff0c\u80fd\u591f\u4e3a\u6807\u51c6\u5316\u7684\u5b89\u5168\u805a\u5408\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\uff0c\u6709\u52a9\u4e8e\u9632\u6b62\u81ea\u52a8\u9a7e\u9a76\u5b66\u4e60\u6a21\u578b\u88ab\u8bef\u5bfc\u800c\u5bfc\u81f4\u4ea4\u901a\u706f\u9519\u8bef\u5206\u7c7b\u7b49\u4e25\u91cd\u540e\u679c\u3002"}}
{"id": "2509.20353", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20353", "abs": "https://arxiv.org/abs/2509.20353", "authors": ["Viktoria Stray", "Elias Goldmann Brandtz\u00e6g", "Viggo Tellefsen Wivestad", "Astri Barbala", "Nils Brede Moe"], "title": "Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study", "comment": "Accepted for publication in the Proceedings of the 59th Hawaii\n  International Conference on System Sciences (HICSS 2026)", "summary": "This study investigates the real-world impact of the generative AI (GenAI)\ntool GitHub Copilot on developer activity and perceived productivity. We\nconducted a mixed-methods case study in NAV IT, a large public sector agile\norganization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's\nGitHub repositories over a two-year period, focusing on commit-based activity\nmetrics from 25 Copilot users and 14 non-users. The analysis was complemented\nby survey responses on their roles and perceived productivity, as well as 13\ninterviews. Our analysis of activity metrics revealed that individuals who used\nCopilot were consistently more active than non-users, even prior to Copilot's\nintroduction. We did not find any statistically significant changes in\ncommit-based activity for Copilot users after they adopted the tool, although\nminor increases were observed. This suggests a discrepancy between changes in\ncommit-based metrics and the subjective experience of productivity.", "AI": {"tldr": "GitHub Copilot\u5bf9\u5f00\u53d1\u8005\u6d3b\u52a8\u7684\u5f71\u54cd\u7814\u7a76\uff1aCopilot\u7528\u6237\u6bd4\u975e\u7528\u6237\u66f4\u6d3b\u8dc3\uff0c\u4f46\u91c7\u7528\u5de5\u5177\u540e\u63d0\u4ea4\u6d3b\u52a8\u65e0\u663e\u8457\u53d8\u5316\uff0c\u4e0e\u4e3b\u89c2\u751f\u4ea7\u529b\u611f\u77e5\u5b58\u5728\u5dee\u5f02", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u5de5\u5177GitHub Copilot\u5728\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\u4e2d\u5bf9\u5f00\u53d1\u8005\u6d3b\u52a8\u548c\u611f\u77e5\u751f\u4ea7\u529b\u7684\u5b9e\u9645\u5f71\u54cd", "method": "\u6df7\u5408\u65b9\u6cd5\u6848\u4f8b\u7814\u7a76\uff1a\u5206\u679026,317\u4e2a\u975e\u5408\u5e76\u63d0\u4ea4\u6570\u636e\uff08\u6765\u81ea703\u4e2aGitHub\u4ed3\u5e93\uff09\uff0c\u8c03\u67e5\u95ee\u5377\u548c13\u6b21\u8bbf\u8c08\uff0c\u6bd4\u8f8325\u540dCopilot\u7528\u6237\u548c14\u540d\u975e\u7528\u6237", "result": "Copilot\u7528\u6237\u59cb\u7ec8\u6bd4\u975e\u7528\u6237\u66f4\u6d3b\u8dc3\uff08\u5373\u4f7f\u5728\u91c7\u7528Copilot\u4e4b\u524d\uff09\uff0c\u91c7\u7528Copilot\u540e\u63d0\u4ea4\u6d3b\u52a8\u6307\u6807\u65e0\u7edf\u8ba1\u5b66\u663e\u8457\u53d8\u5316\uff0c\u4f46\u89c2\u5bdf\u5230\u8f7b\u5fae\u589e\u957f", "conclusion": "\u57fa\u4e8e\u63d0\u4ea4\u7684\u6307\u6807\u53d8\u5316\u4e0e\u4e3b\u89c2\u751f\u4ea7\u529b\u4f53\u9a8c\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0cCopilot\u53ef\u80fd\u901a\u8fc7\u5176\u4ed6\u65b9\u5f0f\u5f71\u54cd\u751f\u4ea7\u529b\u800c\u975e\u76f4\u63a5\u589e\u52a0\u63d0\u4ea4\u6d3b\u52a8"}}
{"id": "2509.20340", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20340", "abs": "https://arxiv.org/abs/2509.20340", "authors": ["Liubov Kurafeeva", "Alan Subedi", "Ryan Hartung", "Michael Fay", "Avhishek Biswas", "Shantenu Jha", "Ozgur O. Kilic", "Chandra Krintz", "Andre Merzky", "Douglas Thain", "Mehmet C. Vuran", "Rich Wolski"], "title": "xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G Wireless Networks for Real-Time Digital Agriculture", "comment": "8 pages with 7 figures followed by 3 pages of reproducibility\n  appendix. This paper will be published following the SC 2025 conference on\n  November 16-21, 2025 at St Louis, MO, USA. ISBN: 978-8-4007-1871-7/2025/11", "summary": "Advanced scientific applications require coupling distributed sensor networks\nwith centralized high-performance computing facilities. Citrus Under Protective\nScreening (CUPS) exemplifies this need in digital agriculture, where citrus\nresearch facilities are instrumented with numerous sensors monitoring\nenvironmental conditions and detecting protective screening damage. CUPS\ndemands access to computational fluid dynamics codes for modeling environmental\nconditions and guiding real-time interventions like water application or\nrobotic repairs. These computing domains have contrasting properties: sensor\nnetworks provide low-performance, limited-capacity, unreliable data access,\nwhile high-performance facilities offer enormous computing power through\nhigh-latency batch processing. Private 5G networks present novel capabilities\naddressing this challenge by providing low latency, high throughput, and\nreliability necessary for near-real-time coupling of edge sensor networks with\nHPC simulations. This work presents xGFabric, an end-to-end system coupling\nsensor networks with HPC facilities through Private 5G networks. The prototype\nconnects remote sensors via 5G network slicing to HPC systems, enabling\nreal-time digital agriculture simulation.", "AI": {"tldr": "xGFabric\u7cfb\u7edf\u901a\u8fc7\u79c1\u67095G\u7f51\u7edc\u5c06\u4f20\u611f\u5668\u7f51\u7edc\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u8bbe\u65bd\u8026\u5408\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6570\u5b57\u519c\u4e1a\u6a21\u62df", "motivation": "\u6570\u5b57\u519c\u4e1a\u4e2d\u7684\u67d1\u6a58\u4fdd\u62a4\u5c4f\u7814\u7a76\u9700\u8981\u5c06\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u7f51\u7edc\u4e0e\u96c6\u4e2d\u5f0f\u9ad8\u6027\u80fd\u8ba1\u7b97\u8bbe\u65bd\u8026\u5408\uff0c\u4ee5\u8fdb\u884c\u73af\u5883\u6761\u4ef6\u5efa\u6a21\u548c\u5b9e\u65f6\u5e72\u9884", "method": "\u5f00\u53d1xGFabric\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u901a\u8fc75G\u7f51\u7edc\u5207\u7247\u5c06\u8fdc\u7a0b\u4f20\u611f\u5668\u8fde\u63a5\u5230HPC\u7cfb\u7edf", "result": "\u539f\u578b\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u4f20\u611f\u5668\u7f51\u7edc\u4e0eHPC\u8bbe\u65bd\u7684\u5b9e\u65f6\u8026\u5408", "conclusion": "\u79c1\u67095G\u7f51\u7edc\u4e3a\u89e3\u51b3\u8fb9\u7f18\u4f20\u611f\u5668\u7f51\u7edc\u4e0eHPC\u6a21\u62df\u7684\u8fd1\u5b9e\u65f6\u8026\u5408\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u9896\u80fd\u529b"}}
