<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 116]
- [cs.AI](#cs.AI) [Total: 31]
- [math.OC](#math.OC) [Total: 19]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Hidden markov model to predict tourists visited place](https://arxiv.org/abs/2511.19465)
*Theo Demessance,Chongke Bi,Sonia Djebali,Guillaume Guerard*

Main category: cs.LG

TL;DR: 基于社交网络数据分析游客移动行为，使用机器学习语法推断算法预测游客未来移动轨迹，特别针对大数据环境进行了算法适配。


<details>
  <summary>Details</summary>
Motivation: 社交网络上的游客数字足迹为分析旅游行为提供了丰富数据，预测游客移动对旅游营销和决策支持至关重要。

Method: 采用机器学习语法推断算法，构建隐马尔可夫模型来表征游客移动模式，该模型具有灵活性和可更新性。

Result: 以巴黎为例验证了方法的有效性，能够成功预测游客的移动行为。

Conclusion: 提出的方法能够有效利用社交网络大数据来理解和预测游客移动行为，为旅游营销提供决策支持。

Abstract: Nowadays, social networks are becoming a popular way of analyzing tourist behavior, thanks to the digital traces left by travelers during their stays on these networks. The massive amount of data generated; by the propensity of tourists to share comments and photos during their trip; makes it possible to model their journeys and analyze their behavior. Predicting the next movement of tourists plays a key role in tourism marketing to understand demand and improve decision support. In this paper, we propose a method to understand and to learn tourists' movements based on social network data analysis to predict future movements. The method relies on a machine learning grammatical inference algorithm. A major contribution in this paper is to adapt the grammatical inference algorithm to the context of big data. Our method produces a hidden Markov model representing the movements of a group of tourists. The hidden Markov model is flexible and editable with new data. The capital city of France, Paris is selected to demonstrate the efficiency of the proposed methodology.

</details>


### [2] [Quantifying Modality Contributions via Disentangling Multimodal Representations](https://arxiv.org/abs/2511.19470)
*Padegal Amit,Omkar Mahesh Kashyap,Namitha Rayasam,Nidhi Shekhar,Surabhi Narayan*

Main category: cs.LG

TL;DR: 提出了基于部分信息分解(PID)的框架来量化多模态模型中各模态的贡献，通过将内部嵌入中的预测信息分解为独特、冗余和协同成分，提供比基于准确性的方法更清晰的模态贡献分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基于准确性的方法，将移除模态后的性能下降解释为其影响力，但这类结果驱动指标无法区分模态是本身具有信息量还是仅通过与其他模态交互产生价值。这种区分在跨注意力架构中尤为重要。

Method: 开发了基于迭代比例拟合程序(IPFP)的算法，计算层级和数据集级别的贡献而无需重新训练，实现可扩展的仅推理分析。

Result: 该框架提供了原则性的、表示级别的多模态行为视图，相比基于结果的指标提供更清晰和可解释的见解。

Conclusion: 提出的PID框架能够更准确地量化多模态模型中各模态的贡献，区分模态的固有信息价值与交互价值，为多模态模型分析提供了更好的工具。

Abstract: Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.

</details>


### [3] [PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer](https://arxiv.org/abs/2511.19472)
*Ruogu Ding,Xin Ning,Ulf Schlichtmann,Weikang Qian*

Main category: cs.LG

TL;DR: PrefixGPT是一个基于GPT的生成式预训练Transformer模型，能够从零开始直接生成优化的前缀加法器，在面积延迟乘积(ADP)方面实现了显著改进。


<details>
  <summary>Details</summary>
Motivation: 由于严格的设计规则和指数级大的设计空间，设计优化的前缀加法器具有挑战性。需要一种能够自动生成有效且优化设计的解决方案。

Method: 将加法器拓扑表示为二维坐标序列，在生成过程中应用合法性掩码确保设计有效性。使用定制化的仅解码器Transformer架构，先在随机合成的有效前缀加法器语料库上进行预训练学习设计规则，然后微调以优化设计质量。

Result: 相比现有工作，PrefixGPT不仅找到了ADP改进7.7%的新最优设计，而且表现出优越的探索质量，将平均ADP降低了高达79.1%。

Conclusion: 这证明了GPT风格模型在掌握复杂硬件设计原理后应用于更高效设计优化的潜力。

Abstract: Prefix adders are widely used in compute-intensive applications for their high speed. However, designing optimized prefix adders is challenging due to strict design rules and an exponentially large design space. We introduce PrefixGPT, a generative pre-trained Transformer (GPT) that directly generates optimized prefix adders from scratch. Our approach represents an adder's topology as a two-dimensional coordinate sequence and applies a legality mask during generation, ensuring every design is valid by construction. PrefixGPT features a customized decoder-only Transformer architecture. The model is first pre-trained on a corpus of randomly synthesized valid prefix adders to learn design rules and then fine-tuned to navigate the design space for optimized design quality. Compared with existing works, PrefixGPT not only finds a new optimal design with a 7.7% improved area-delay product (ADP) but exhibits superior exploration quality, lowering the average ADP by up to 79.1%. This demonstrates the potential of GPT-style models to first master complex hardware design principles and then apply them for more efficient design optimization.

</details>


### [4] [WavefrontDiffusion: Dynamic Decoding Schedule or Improved Reasoning](https://arxiv.org/abs/2511.19473)
*Haojin Yang,Rui Hu,Zequn Sun,Rui Zhou,Yujun Cai,Yiwei Wang*

Main category: cs.LG

TL;DR: WavefrontDiffusion是一种动态解码方法，通过从已确定位置向外扩展活动token波前，在保持计算成本与基于块的方法相同的同时，实现了推理和代码生成任务的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 主流去噪策略存在局限性：标准扩散会过早结束序列，块扩散会破坏语义连贯性。需要一种既能保持语义结构自然流动，又保持计算效率的自适应调度方法。

Method: 提出WavefrontDiffusion方法，动态扩展活动token波前，从已确定位置向外自适应更新，遵循语义结构的自然流动。

Result: 在四个推理和代码生成基准测试中实现了最先进的性能，生成具有更高语义保真度的输出。

Conclusion: 自适应调度对于实现更连贯和高效的生成具有重要价值，WavefrontDiffusion展示了在保持计算效率的同时提高语义连贯性的有效途径。

Abstract: Diffusion Language Models (DLMs) have shown strong potential for text generation and are becoming a competitive alternative to autoregressive models. The denoising strategy plays an important role in determining the quality of their outputs. Mainstream denoising strategies include Standard Diffusion and BlockDiffusion. Standard Diffusion performs global denoising without restricting the update range, often finalizing incomplete context and causing premature end-of-sequence predictions. BlockDiffusion updates fixed-size blocks in a preset order, but its rigid structure can break apart coherent semantic units and disrupt reasoning. We present WavefrontDiffusion, a dynamic decoding approach that expands a wavefront of active tokens outward from finalized positions. This adaptive process follows the natural flow of semantic structure while keeping computational cost equal to block-based methods. Across four benchmarks in reasoning and code generation, WavefrontDiffusion achieves state-of-the-art performance while producing outputs with higher semantic fidelity, showing the value of adaptive scheduling for more coherent and efficient generation.

</details>


### [5] [Exploiting the Experts: Unauthorized Compression in MoE-LLMs](https://arxiv.org/abs/2511.19480)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.LG

TL;DR: 本文系统研究了MoE-LLMs在任务特定使用下的可剪枝性，揭示了知识损失与恢复的权衡，并提出了防御策略来防止未经授权的模型压缩和微调。


<details>
  <summary>Details</summary>
Motivation: MoE架构的模块化结构引入了独特的安全漏洞：攻击者可以通过剪枝专家并廉价微调剩余部分来绕过许可和安全约束，压缩或重新利用模型。

Method: 开发了专家归因框架来识别对特定任务最关键的专家子集，然后使用主动学习驱动的微调评估剪枝和重新对齐这些专家的性能权衡。

Result: 研究发现存在关键的知识损失-恢复权衡：虽然可以隔离某些专家来保持任务准确性，但如果没有针对性的重新对齐，会出现显著的性能下降。

Conclusion: 通过将专家剪枝定位为威胁向量和防御目标，这项工作突出了MoE模块化的双重用途性质，并为MoE-LLMs的安全专业化提供了第一个系统评估框架。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.

</details>


### [6] [Quality analysis and evaluation prediction of RAG retrieval based on machine learning algorithms](https://arxiv.org/abs/2511.19481)
*Ruoxin Zhang,Zhizhao Wen,Chao Wang,Chenchen Tang,Puyang Xu,Yifan Jiang*

Main category: cs.LG

TL;DR: 本文提出基于特征工程和粒子群优化的XGBoost回归模型，用于优化RAG系统的检索质量，通过改进文档相关性来提升答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理表格特征时存在性能瓶颈，检索模块质量直接影响生成内容的准确性，低相关性或噪声信息会导致内容失真。

Method: 使用XGBoost机器学习回归模型，结合特征工程和粒子群优化，并与决策树、AdaBoost等模型进行对比实验。

Result: VMD PSO BiLSTM模型在所有评估指标上表现最优，MSE、RMSE、MAE、MAPE显著降低，R2值更高，预测精度和稳定性更出色。

Conclusion: 该成果为优化RAG系统检索质量和生成效果提供了有效路径，对相关技术的实施应用具有重要价值。

Abstract: With the rapid evolution of large language models, retrieval enhanced generation technology has been widely used due to its ability to integrate external knowledge to improve output accuracy. However, the performance of the system is highly dependent on the quality of the retrieval module. If the retrieval results have low relevance to user needs or contain noisy information, it will directly lead to distortion of the generated content. In response to the performance bottleneck of existing models in processing tabular features, this paper proposes an XGBoost machine learning regression model based on feature engineering and particle swarm optimization. Correlation analysis shows that answer_quality is positively correlated with doc_delevance by 0.66, indicating that document relevance has a significant positive effect on answer quality, and improving document relevance may enhance answer quality; The strong negative correlations between semantic similarity, redundancy, and diversity were -0.89 and -0.88, respectively, indicating a trade- off between semantic similarity, redundancy, and diversity. In other words, as the former two increased, diversity significantly decreased. The experimental results comparing decision trees, AdaBoost, etc. show that the VMD PSO BiLSTM model is superior in all evaluation indicators, with significantly lower MSE, RMSE, MAE, and MAPE compared to the comparison model. The R2 value is higher, indicating that its prediction accuracy, stability, and data interpretation ability are more outstanding. This achievement provides an effective path for optimizing the retrieval quality and improving the generation effect of RAG system, and has important value in promoting the implementation and application of related technologies.

</details>


### [7] [OmniTFT: Omni Target Forecasting for Vital Signs and Laboratory Result Trajectories in Multi Center ICU Data](https://arxiv.org/abs/2511.19485)
*Wanzhe Xu,Yutong Dai,Yitao Yang,Martin Loza,Weihang Zhang,Yang Cui,Xin Zeng,Sung Joon Park,Kenta Nakai*

Main category: cs.LG

TL;DR: OmniTFT是一个基于时间融合变换器的深度学习框架，用于联合预测ICU中的高频生命体征和稀疏采样的实验室结果，通过四种新颖策略提升性能并保持跨机构泛化能力。


<details>
  <summary>Details</summary>
Motivation: ICU中生命体征存在噪声和快速波动，实验室结果存在缺失值、测量延迟和设备特异性偏差，这使得综合预测具有挑战性。

Method: 采用滑动窗口均衡采样、频率感知嵌入收缩、分层变量选择和影响对齐注意力校准四种策略，基于时间融合变换器构建统一模型。

Result: 在MIMIC-III、MIMIC-IV和eICU数据集上，OmniTFT在生命体征和实验室结果的预测任务中均取得显著性能提升。

Conclusion: OmniTFT能够统一建模多个异质临床目标，其注意力模式可解释且与已知病理生理学一致，具有临床决策支持的潜在应用价值。

Abstract: Accurate multivariate time-series prediction of vital signs and laboratory results is crucial for early intervention and precision medicine in intensive care units (ICUs). However, vital signs are often noisy and exhibit rapid fluctuations, while laboratory tests suffer from missing values, measurement lags, and device-specific bias, making integrative forecasting highly challenging. To address these issues, we propose OmniTFT, a deep learning framework that jointly learns and forecasts high-frequency vital signs and sparsely sampled laboratory results based on the Temporal Fusion Transformer (TFT). Specifically, OmniTFT implements four novel strategies to enhance performance: sliding window equalized sampling to balance physiological states, frequency-aware embedding shrinkage to stabilize rare-class representations, hierarchical variable selection to guide model attention toward informative feature clusters, and influence-aligned attention calibration to enhance robustness during abrupt physiological changes. By reducing the reliance on target-specific architectures and extensive feature engineering, OmniTFT enables unified modeling of multiple heterogeneous clinical targets while preserving cross-institutional generalizability. Across forecasting tasks, OmniTFT achieves substantial performance improvement for both vital signs and laboratory results on the MIMIC-III, MIMIC-IV, and eICU datasets. Its attention patterns are interpretable and consistent with known pathophysiology, underscoring its potential utility for quantitative decision support in clinical care.

</details>


### [8] [Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning then Rectification](https://arxiv.org/abs/2511.19486)
*Lei Wang,Zikun Ye,Jinglong Zhao*

Main category: cs.LG

TL;DR: 提出了一个结合微调和校正的框架，通过优化分配有限标注样本来提升大语言模型在市场研究和社科应用中的表现，使用预测误差方差最小化作为微调目标，并基于经验缩放定律分配样本。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要单独使用微调或校正来提升LLM性能，但缺乏将两者结合并优化样本分配的框架。

Method: 开发了结合微调和校正的框架，以预测误差方差最小化为微调目标，利用经验缩放定律优化样本在微调和校正阶段间的分配。

Result: 实证分析验证了该框架相比单独使用微调或校正，在估计和推断性能上都有提升。

Conclusion: 提出的框架通过优化结合微调和校正，有效提升了LLM在社科研究中的预测性能。

Abstract: Driven by recent advances in artificial intelligence (AI), a growing body of work demonstrates the potential of using large language models (LLMs) to generate human-like responses in market research and social science applications. Two primary approaches can be applied to improve the performance of LLMs: fine-tuning, which aligns LLM predictions more closely with human responses, and rectification, which corrects biases in LLM outputs. In this paper, we develop a framework that combines fine-tuning and rectification, and optimally allocates limited labeled samples across the two stages. Unlike the conventional objective that minimizes the mean squared prediction errors, we propose to minimize the variance of the prediction errors as the fine-tuning objective, which is optimal for the downstream rectification stage. Building on this insight, we leverage empirical scaling laws to develop a data-driven method for optimally splitting samples between the fine-tuning and rectification stages. Empirical analysis validates our framework, demonstrating improved estimation and inference performance compared to using either fine-tuning or rectification alone.

</details>


### [9] [The Generalized Proximity Forest](https://arxiv.org/abs/2511.19487)
*Ben Shaw,Adam Rustad,Sofia Pelagalli Maia,Jake S. Rhodes,Kevin R. Moon*

Main category: cs.LG

TL;DR: 本文提出了广义邻近森林模型，将随机森林邻近度扩展到所有基于距离的监督机器学习场景，并引入了回归任务的变体，以及作为元学习框架用于监督插补。


<details>
  <summary>Details</summary>
Motivation: 随机森林邻近度在多种监督学习任务中很有用，但其效用依赖于随机森林模型本身的表现，而随机森林并非在所有场景下都是理想模型。需要将邻近度扩展到更广泛的机器学习场景。

Method: 提出了广义邻近森林模型，将随机森林邻近度扩展到所有基于距离的监督机器学习任务。引入了回归任务的变体，并提出了将广义邻近森林作为元学习框架来扩展监督插补能力。

Result: 实验证明，广义邻近森林模型相比随机森林模型和k近邻模型具有独特优势。

Conclusion: 广义邻近森林模型成功扩展了随机森林邻近度的应用范围，为各种基于距离的监督机器学习任务提供了新的解决方案。

Abstract: Recent work has demonstrated the utility of Random Forest (RF) proximities for various supervised machine learning tasks, including outlier detection, missing data imputation, and visualization. However, the utility of the RF proximities depends upon the success of the RF model, which itself is not the ideal model in all contexts. RF proximities have recently been extended to time series by means of the distance-based Proximity Forest (PF) model, among others, affording time series analysis with the benefits of RF proximities. In this work, we introduce the generalized PF model, thereby extending RF proximities to all contexts in which supervised distance-based machine learning can occur. Additionally, we introduce a variant of the PF model for regression tasks. We also introduce the notion of using the generalized PF model as a meta-learning framework, extending supervised imputation capability to any pre-trained classifier. We experimentally demonstrate the unique advantages of the generalized PF model compared with both the RF model and the $k$-nearest neighbors model.

</details>


### [10] [Generative Model-Aided Continual Learning for CSI Feedback in FDD mMIMO-OFDM Systems](https://arxiv.org/abs/2511.19490)
*Guijun Liu,Yuwen Cao,Tomoaki Ohtsuki,Jiguang He,Shahid Mumtaz*

Main category: cs.LG

TL;DR: 提出基于GAN的持续学习方法解决CSI反馈中的灾难性遗忘问题，通过GAN生成器作为记忆单元保持过去环境知识，提升DAE框架在动态环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有CSI反馈模型难以适应用户移动性导致的动态环境变化，遇到新CSI分布需要重新训练，且返回先前环境时会出现灾难性遗忘问题。

Method: 使用生成对抗网络（GAN）作为学习框架，将GAN生成器作为记忆单元来保存过去环境的知识，确保在不同场景下保持高性能而不遗忘。

Result: 仿真结果表明，该方法增强了DAE框架的泛化能力，同时保持低内存开销，并能无缝集成到其他先进的CSI反馈模型中。

Conclusion: 所提出的GAN基方法有效解决了CSI反馈中的持续学习挑战，展现了良好的鲁棒性和适应性。

Abstract: Deep autoencoder (DAE) frameworks have demonstrated their effectiveness in reducing channel state information (CSI) feedback overhead in massive multiple-input multiple-output (mMIMO) orthogonal frequency division multiplexing (OFDM) systems. However, existing CSI feedback models struggle to adapt to dynamic environments caused by user mobility, requiring retraining when encountering new CSI distributions. Moreover, returning to previously encountered environments often leads to performance degradation due to catastrophic forgetting. Continual learning involves enabling models to incorporate new information while maintaining performance on previously learned tasks. To address these challenges, we propose a generative adversarial network (GAN)-based learning approach for CSI feedback. By using a GAN generator as a memory unit, our method preserves knowledge from past environments and ensures consistently high performance across diverse scenarios without forgetting. Simulation results show that the proposed approach enhances the generalization capability of the DAE framework while maintaining low memory overhead. Furthermore, it can be seamlessly integrated with other advanced CSI feedback models, highlighting its robustness and adaptability.

</details>


### [11] [OpenCML: End-to-End Framework of Open-world Machine Learning to Learn Unknown Classes Incrementally](https://arxiv.org/abs/2511.19491)
*Jitendra Parmar,Praveen Singh Thakur*

Main category: cs.LG

TL;DR: 提出了一种开放世界机器学习模型，通过发现未知类别和增量学习新类别来实现持续学习，在开放世界学习和持续学习任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型遵循封闭世界假设，难以保留先前学到的知识来处理未来任务，而自动化智能系统需要学习新类别和已知任务。

Method: 模型包含两个相互关联的任务：首先发现数据中的未知类别并创建新类别；然后对每个新类别进行增量学习。两者结合实现持续学习。

Result: 在开放世界学习中优于现有方法，在持续学习中达到最高平均准确率82.54%（四次迭代），最低准确率65.87%。

Conclusion: 该模型能够在开放和持续学习环境中有效扩展对数据的理解并随时间改进，为开放世界机器学习提供了有效解决方案。

Abstract: Open-world machine learning is an emerging technique in artificial intelligence, where conventional machine learning models often follow closed-world assumptions, which can hinder their ability to retain previously learned knowledge for future tasks. However, automated intelligence systems must learn about novel classes and previously known tasks. The proposed model offers novel learning classes in an open and continuous learning environment. It consists of two different but connected tasks. First, it discovers unknown classes in the data and creates novel classes; next, it learns how to perform class incrementally for each new class. Together, they enable continual learning, allowing the system to expand its understanding of the data and improve over time. The proposed model also outperformed existing approaches in open-world learning. Furthermore, it demonstrated strong performance in continuous learning, achieving a highest average accuracy of 82.54% over four iterations and a minimum accuracy of 65.87%.

</details>


### [12] [RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression](https://arxiv.org/abs/2511.19493)
*Chris Kuchar*

Main category: cs.LG

TL;DR: RFX v1.0是一个生产就绪的随机森林Python实现，通过QLORA压缩和TriBlock存储等技术解决了邻近矩阵内存瓶颈，使随机森林分析能够处理超过20万个样本的大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林分析受限于邻近矩阵的内存瓶颈，只能处理约6万个样本的数据集，限制了其在大规模数据分析中的应用。

Method: 采用四种解决方案：(1) QLORA压缩用于GPU邻近矩阵，实现12,500倍压缩；(2) CPU TriBlock邻近矩阵，结合上三角存储和块稀疏阈值；(3) SM感知GPU批处理大小；(4) GPU加速3D MDS可视化。

Result: 验证了四种实现模式的正确性，GPU相比CPU在500+棵树时实现1.4倍加速，邻近矩阵计算可扩展到20万+样本，消除了邻近矩阵内存瓶颈。

Conclusion: RFX v1.0消除了邻近矩阵内存瓶颈，使基于邻近的随机森林分析能够处理比以往大几个数量级的数据集，提供了开源的生产就绪分类实现。

Abstract: RFX (Random Forests X), where X stands for compression or quantization, presents a production-ready implementation of Breiman and Cutler's Random Forest classification methodology in Python. RFX v1.0 provides complete classification: out-of-bag error estimation, overall and local importance measures, proximity matrices with QLORA compression, case-wise analysis, and interactive visualization (rfviz)--all with CPU and GPU acceleration. Regression, unsupervised learning, CLIQUE importance, and RF-GAP proximity are planned for v2.0.
  This work introduces four solutions addressing the proximity matrix memory bottleneck limiting Random Forest analysis to ~60,000 samples: (1) QLORA (Quantized Low-Rank Adaptation) compression for GPU proximity matrices, reducing memory from 80GB to 6.4MB for 100k samples (12,500x compression with INT8 quantization) while maintaining 99% geometric structure preservation, (2) CPU TriBlock proximity--combining upper-triangle storage with block-sparse thresholding--achieving 2.7x memory reduction with lossless quality, (3) SM-aware GPU batch sizing achieving 95% GPU utilization, and (4) GPU-accelerated 3D MDS visualization computing embeddings directly from low-rank factors using power iteration.
  Validation across four implementation modes (GPU/CPU x case-wise/non-case-wise) demonstrates correct implementation. GPU achieves 1.4x speedup over CPU for overall importance with 500+ trees. Proximity computation scales from 1,000 to 200,000+ samples (requiring GPU QLORA), with CPU TriBlock filling the gap for medium-scale datasets (10K-50K samples). RFX v1.0 eliminates the proximity memory bottleneck, enabling proximity-based Random Forest analysis on datasets orders of magnitude larger than previously feasible. Open-source production-ready classification following Breiman and Cutler's original methodology.

</details>


### [13] [A Systematic Study of Compression Ordering for Large Language Models](https://arxiv.org/abs/2511.19495)
*Shivansh Chhawri,Rahul Mahadik,Suparna Rooj*

Main category: cs.LG

TL;DR: 本文系统研究了知识蒸馏、结构化剪枝和低位量化三种LLM压缩技术的组合顺序对Qwen2.5 3B模型的影响，发现P-KD-Q序列能实现3.68倍压缩比并保持良好性能。


<details>
  <summary>Details</summary>
Motivation: LLM需要大量计算资源，在受限环境中部署需要模型压缩。虽然单个压缩技术效果已有研究，但它们的组合顺序和交互作用尚不明确。

Method: 在Qwen2.5 3B模型上评估多种压缩流水线，包括单技术和三技术组合序列，使用困惑度、G-Eval、清晰度、提示对齐和压缩比等指标。

Result: 量化提供最大单独压缩，剪枝引入中等质量下降。技术顺序显著影响最终质量：P-KD-Q序列表现最佳，达到3.68倍压缩比并保持强指令跟随和语言理解能力。

Conclusion: 压缩技术顺序对模型质量至关重要，P-KD-Q序列是最优选择，为资源受限环境中部署LLM提供了实用的顺序感知压缩流水线设计指导。

Abstract: Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.

</details>


### [14] [Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM](https://arxiv.org/abs/2511.19496)
*Yang Liu,Xiaolong Zhong,Ling Jiang*

Main category: cs.LG

TL;DR: Xmodel-2.5是一个13亿参数的小型语言模型，作为即插即用的智能体核心，通过μP训练方法、1.4T令牌的课程学习以及从AdamW切换到Muon优化器，在保持计算效率的同时提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然推理和工具使用能力强，但计算需求大，不适合边缘或成本敏感部署。需要开发计算效率高的小型模型。

Method: 使用最大更新参数化(μP)训练，采用1.4T令牌的Warmup-Stable-Decay课程学习，在衰减阶段从AdamW切换到Muon优化器，并使用FP8混合精度训练。

Result: 在13个推理任务上平均性能提升4.58%，验证了早期AdamW稳定性和后期Muon锐化的组合策略能提升下游性能。

Conclusion: Xmodel-2.5展示了小型模型作为智能体核心的可行性，通过优化的训练策略在保持效率的同时提升性能，所有资源已开源。

Abstract: Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \emph{drop-in agent core}. Training with maximal-update parameterization ($μ$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\,\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\footnote{https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints).} Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.

</details>


### [15] [PeriodNet: Boosting the Potential of Attention Mechanism for Time Series Forecasting](https://arxiv.org/abs/2511.19497)
*Bowen Zhao,Huanlai Xing,Zhiwen Xiao,Jincheng Peng,Li Feng,Xinhan Wang,Rong Qu,Hui Li*

Main category: cs.LG

TL;DR: 提出了PeriodNet，一种用于时间序列预测的新网络结构，结合周期注意力机制和迭代分组机制，在多个数据集上优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 注意力机制在序列建模中表现出色，但在时间序列预测中尚未达到预期效果，需要探索更好的网络结构。

Method: 使用周期注意力和稀疏周期注意力机制分析相邻周期，引入迭代分组机制减少跨变量冗余，重新设计Transformer架构并添加周期扩散器进行多周期预测。

Result: 在8个数据集上的实验表明，PeriodNet在单变量和多变量时间序列预测中均优于6个最先进模型，在720长度时间序列预测中相对改进达到22%。

Conclusion: PeriodNet通过创新的周期注意力机制和架构设计，显著提升了时间序列预测的性能。

Abstract: The attention mechanism has demonstrated remarkable potential in sequence modeling, exemplified by its successful application in natural language processing with models such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT). Despite these advancements, its utilization in time series forecasting (TSF) has yet to meet expectations. Exploring a better network structure for attention in TSF holds immense significance across various domains. In this paper, we present PeriodNet with a brand new structure to forecast univariate and multivariate time series. PeriodNet incorporates period attention and sparse period attention mechanism for analyzing adjacent periods. It enhances the mining of local characteristics, periodic patterns, and global dependencies. For efficient cross-variable modeling, we introduce an iterative grouping mechanism which can directly reduce the cross-variable redundancy. To fully leverage the extracted features on the encoder side, we redesign the entire architecture of the vanilla Transformer and propose a period diffuser for precise multi-period prediction. Through comprehensive experiments conducted on eight datasets, we demonstrate that PeriodNet outperforms six state-of-the-art models in both univariate and multivariate TSF scenarios in terms of mean square error and mean absolute error. In particular, PeriodNet achieves a relative improvement of 22% when forecasting time series with a length of 720, in comparison to other models based on the conventional encoder-decoder Transformer architecture.

</details>


### [16] [Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data](https://arxiv.org/abs/2511.19498)
*Yi Zhang,Tianxiang Xu,Zijian Li,Chao Zhang,Kunyu Zhang,Zhan Gao,Meinuo Li,Xiaohan Zhang,Qichao Qi,Bing Chen*

Main category: cs.LG

TL;DR: 提出了一种分层双策略框架，用于选择性知识遗忘，在医疗领域精确移除专业知识同时保留基础医学能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗环境中存在隐私风险，特别是训练数据记忆问题，需要解决隐私敏感患者信息的保护需求。

Method: 采用几何约束梯度更新和概念感知令牌级干预相结合的双策略方法，通过统一的四级医学概念层次结构区分保留关键令牌和遗忘目标令牌。

Result: 在MedMCQA和MHQA数据集上取得优异表现，遗忘率达到82.7%，知识保留率达到88.5%，仅需修改0.1%的参数。

Conclusion: 该框架在保持强大隐私保障的同时，满足了临床研究中监管合规性、可审计性和道德标准的关键需求。

Abstract: Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.

</details>


### [17] [Beyond Binary Classification: A Semi-supervised Approach to Generalized AI-generated Image Detection](https://arxiv.org/abs/2511.19499)
*Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: 提出TriDetect检测器，通过发现"假"类中的潜在架构模式来增强跨生成器泛化能力，解决GAN和扩散模型检测器在跨架构边界时的泛化失败问题。


<details>
  <summary>Details</summary>
Motivation: 当前取证检测器在跨生成器泛化方面存在显著挑战，特别是在跨越架构边界（如从GAN到扩散模型）时失败。这源于不同架构产生的伪影存在根本性差异。

Method: 提出TriDetect半监督方法，通过平衡聚类分配（使用Sinkhorn-Knopp算法）和跨视图一致性机制，学习基本的架构差异模式。

Result: 在两个标准基准和三个真实世界数据集上评估，与13个基线方法比较，证明了其对未见生成器的泛化能力。

Conclusion: 通过理论分析揭示了GAN和扩散模型在流形覆盖行为上的根本差异，TriDetect能够有效学习这些架构差异，实现更好的跨生成器检测泛化。

Abstract: The rapid advancement of generators (e.g., StyleGAN, Midjourney, DALL-E) has produced highly realistic synthetic images, posing significant challenges to digital media authenticity. These generators are typically based on a few core architectural families, primarily Generative Adversarial Networks (GANs) and Diffusion Models (DMs). A critical vulnerability in current forensics is the failure of detectors to achieve cross-generator generalization, especially when crossing architectural boundaries (e.g., from GANs to DMs). We hypothesize that this gap stems from fundamental differences in the artifacts produced by these \textbf{distinct architectures}. In this work, we provide a theoretical analysis explaining how the distinct optimization objectives of the GAN and DM architectures lead to different manifold coverage behaviors. We demonstrate that GANs permit partial coverage, often leading to boundary artifacts, while DMs enforce complete coverage, resulting in over-smoothing patterns. Motivated by this analysis, we propose the \textbf{Tri}archy \textbf{Detect}or (TriDetect), a semi-supervised approach that enhances binary classification by discovering latent architectural patterns within the "fake" class. TriDetect employs balanced cluster assignment via the Sinkhorn-Knopp algorithm and a cross-view consistency mechanism, encouraging the model to learn fundamental architectural distincts. We evaluate our approach on two standard benchmarks and three in-the-wild datasets against 13 baselines to demonstrate its generalization capability to unseen generators.

</details>


### [18] [Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma](https://arxiv.org/abs/2511.19504)
*Subramanyam Sahoo,Aman Chadha,Vinija Jain,Divya Chaudhary*

Main category: cs.LG

TL;DR: 本文提出了对齐三难困境：RLHF系统无法同时实现代表性、可扩展性和鲁棒性。通过复杂性理论分析证明，要实现全球规模的代表性和鲁棒性需要超多项式计算复杂度。


<details>
  <summary>Details</summary>
Motivation: RLHF在实践中面临安全性与公平性的权衡、扩展到多样化群体的计算不可行性，以及鲁棒性增强多数偏见的问题。这些现象缺乏统一的理论解释。

Method: 通过复杂性理论分析，结合统计学习理论和鲁棒优化，形式化了对齐三难困境，并证明了实现代表性和鲁棒性的计算复杂度下界。

Result: 证明实现全球规模的代表性（ε≤0.01）和鲁棒性（δ≤0.001）需要Ω(2^{d_context})次操作，这是超多项式的。当前RLHF实现通过牺牲代表性来解决这一困境。

Conclusion: 提供了对RLHF病理现象的统一解释，并提出了通过战略性地放宽对齐要求来应对这些基本权衡的具体方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.

</details>


### [19] [Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles](https://arxiv.org/abs/2511.19656)
*Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文针对双层优化的下界问题，在光滑非凸-强凸设定下，提出了新的困难实例，分别在确定性和随机一阶oracle模型下建立了非平凡下界。


<details>
  <summary>Details</summary>
Motivation: 虽然双层优化的上界保证已被广泛研究，但由于双层结构的复杂性，下界方面的进展有限。本文旨在填补这一空白，为双层优化建立更严格的下界。

Method: 开发新的困难实例，在光滑非凸-强凸设定下，分别针对确定性和随机一阶oracle模型进行分析，证明所需oracle调用的最小数量。

Result: 在确定性情况下，任何一阶零尊重算法至少需要Ω(κ^{3/2}ε^{-2})次oracle调用才能找到ε-精确驻点；在随机情况下，至少需要Ω(κ^{5/2}ε^{-4})次随机oracle调用。

Conclusion: 研究结果揭示了当前双层优化上下界之间的显著差距，表明即使是简化设定（如二次下层目标）也需要进一步研究，以理解标准一阶oracle下双层优化的最优复杂度。

Abstract: Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $Ω(κ^{3/2}ε^{-2})$ oracle calls to find an $ε$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $Ω(κ^{5/2}ε^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.

</details>


### [20] [Profile Generators: A Link between the Narrative and the Binary Matrix Representation](https://arxiv.org/abs/2511.19506)
*Raoul H. Kutil,Georg Zimmermann,Barbara Strasser-Kirchweger,Christian Borgelt*

Main category: cs.LG

TL;DR: 开发了一种症状配置文件生成器，用于替代二进制矩阵表示法，以处理DSM-5中复杂认知障碍的相似性分析，解决了大规模症状组合矩阵不可行的问题。


<details>
  <summary>Details</summary>
Motivation: DSM-5中认知障碍的症状组合数量庞大，传统的二进制矩阵表示法不可行，需要一种能够自动生成有效症状组合的替代表示方法。

Method: 开发症状配置文件生成器，使用列表、集合和数字的严格预定义格式来表示复杂的诊断路径，支持自动生成症状组合。

Result: 成功将多种精神障碍表示为生成器形式，证明复杂障碍的矩阵表示过于庞大难以管理，开发了基于目标生成器操作的配置文件缩减方法。

Conclusion: 症状配置文件生成器提供了一种可读、适应性强且全面的二进制矩阵替代方案，能够处理复杂障碍的相似性计算问题。

Abstract: Mental health disorders, particularly cognitive disorders defined by deficits in cognitive abilities, are described in detail in the DSM-5, which includes definitions and examples of signs and symptoms. A simplified, machine-actionable representation was developed to assess the similarity and separability of these disorders, but it is not suited for the most complex cases. Generating or applying a full binary matrix for similarity calculations is infeasible due to the vast number of symptom combinations. This research develops an alternative representation that links the narrative form of the DSM-5 with the binary matrix representation and enables automated generation of valid symptom combinations. Using a strict pre-defined format of lists, sets, and numbers with slight variations, complex diagnostic pathways involving numerous symptom combinations can be represented. This format, called the symptom profile generator (or simply generator), provides a readable, adaptable, and comprehensive alternative to a binary matrix while enabling easy generation of symptom combinations (profiles). Cognitive disorders, which typically involve multiple diagnostic criteria with several symptoms, can thus be expressed as lists of generators. Representing several psychotic disorders in generator form and generating all symptom combinations showed that matrix representations of complex disorders become too large to manage. The MPCS (maximum pairwise cosine similarity) algorithm cannot handle matrices of this size, prompting the development of a profile reduction method using targeted generator manipulation to find specific MPCS values between disorders. The generators allow easier creation of binary representations for large matrices and make it possible to calculate specific MPCS cases between complex disorders through conditional generators.

</details>


### [21] [Adaptivity and Universality: Problem-dependent Universal Regret for Online Convex Optimization](https://arxiv.org/abs/2511.19937)
*Peng Zhao,Yu-Hu Yan,Hang Yu,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 提出了UniGrad方法，实现了通用性和自适应性的统一，能够同时适应不同函数类型并获得与梯度变化相关的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有通用在线学习方法虽然建立了极小极大最优遗憾界，但缺乏问题依赖的自适应性，特别是无法获得与梯度变化相关的遗憾界。

Method: 提出了UniGrad方法，包括UniGrad.Correct和UniGrad.Bregman两个变体，都采用元算法结构，使用O(log T)个基础学习器。还提出了计算效率更高的UniGrad++版本。

Result: UniGrad.Correct实现了强凸函数O(log V_T)遗憾、指数凹函数O(d log V_T)遗憾、凸函数O(√(V_T log V_T))遗憾；UniGrad.Bregman实现了凸函数最优的O(√V_T)遗憾。

Conclusion: UniGrad方法首次同时实现了通用性和自适应性的统一，为随机优化和博弈快速收敛等应用提供了理论基础。

Abstract: Universal online learning aims to achieve optimal regret guarantees without requiring prior knowledge of the curvature of online functions. Existing methods have established minimax-optimal regret bounds for universal online learning, where a single algorithm can simultaneously attain $\mathcal{O}(\sqrt{T})$ regret for convex functions, $\mathcal{O}(d \log T)$ for exp-concave functions, and $\mathcal{O}(\log T)$ for strongly convex functions, where $T$ is the number of rounds and $d$ is the dimension of the feasible domain. However, these methods still lack problem-dependent adaptivity. In particular, no universal method provides regret bounds that scale with the gradient variation $V_T$, a key quantity that plays a crucial role in applications such as stochastic optimization and fast-rate convergence in games. In this work, we introduce UniGrad, a novel approach that achieves both universality and adaptivity, with two distinct realizations: UniGrad.Correct and UniGrad.Bregman. Both methods achieve universal regret guarantees that adapt to gradient variation, simultaneously attaining $\mathcal{O}(\log V_T)$ regret for strongly convex functions and $\mathcal{O}(d \log V_T)$ regret for exp-concave functions. For convex functions, the regret bounds differ: UniGrad.Correct achieves an $\mathcal{O}(\sqrt{V_T \log V_T})$ bound while preserving the RVU property that is crucial for fast convergence in online games, whereas UniGrad.Bregman achieves the optimal $\mathcal{O}(\sqrt{V_T})$ regret bound through a novel design. Both methods employ a meta algorithm with $\mathcal{O}(\log T)$ base learners, which naturally requires $\mathcal{O}(\log T)$ gradient queries per round. To enhance computational efficiency, we introduce UniGrad++, which retains the regret while reducing the gradient query to just $1$ per round via surrogate optimization. We further provide various implications.

</details>


### [22] [TouchFormer: A Robust Transformer-based Framework for Multimodal Material Perception](https://arxiv.org/abs/2511.19509)
*Kailin Lyu,Long Xiao,Jianing Zeng,Junhao Dong,Xuexin Liu,Zhuojun Zou,Haoyue Yang,Lin Shu,Jie Hao*

Main category: cs.LG

TL;DR: 提出了TouchFormer框架，通过模态自适应门控机制和跨模态注意力机制解决多模态融合中的噪声、模态缺失和动态重要性问题，在非视觉材料感知任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统视觉方法在视觉受损条件下性能下降严重，现有多模态方法简单融合模态输入，忽略了模态特定噪声、模态缺失以及模态重要性动态变化等关键挑战。

Method: 使用模态自适应门控机制和内外模态注意力机制自适应整合跨模态特征，引入跨实例嵌入正则化策略提升细粒度子类别材料识别精度。

Result: 在SSMC和USMC任务上分别实现2.48%和6.83%的分类准确率提升，真实机器人实验验证了框架在环境感知方面的有效性。

Conclusion: TouchFormer框架为安全关键应用如应急响应和工业自动化中的机器人部署铺平了道路，代码和数据集将开源。

Abstract: Traditional vision-based material perception methods often experience substantial performance degradation under visually impaired conditions, thereby motivating the shift toward non-visual multimodal material perception. Despite this, existing approaches frequently perform naive fusion of multimodal inputs, overlooking key challenges such as modality-specific noise, missing modalities common in real-world scenarios, and the dynamically varying importance of each modality depending on the task. These limitations lead to suboptimal performance across several benchmark tasks. In this paper, we propose a robust multimodal fusion framework, TouchFormer. Specifically, we employ a Modality-Adaptive Gating (MAG) mechanism and intra- and inter-modality attention mechanisms to adaptively integrate cross-modal features, enhancing model robustness. Additionally, we introduce a Cross-Instance Embedding Regularization(CER) strategy, which significantly improves classification accuracy in fine-grained subcategory material recognition tasks. Experimental results demonstrate that, compared to existing non-visual methods, the proposed TouchFormer framework achieves classification accuracy improvements of 2.48% and 6.83% on SSMC and USMC tasks, respectively. Furthermore, real-world robotic experiments validate TouchFormer's effectiveness in enabling robots to better perceive and interpret their environment, paving the way for its deployment in safety-critical applications such as emergency response and industrial automation. The code and datasets will be open-source, and the videos are available in the supplementary materials.

</details>


### [23] [Communication-Efficient Learning for Satellite Constellations](https://arxiv.org/abs/2511.20220)
*Ruxandra-Stefania Tudose,Moritz H. W. Grüss,Grace Ra Kim,Karl H. Johansson,Nicola Bastianello*

Main category: cs.LG

TL;DR: 提出了一种用于卫星星座联邦学习的新型通信高效算法，通过本地训练、压缩和误差反馈机制减少与地面站的通信次数和大小，在真实空间场景中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星星座的普及，需要解决在这些卫星星座上进行学习的问题，特别是采用联邦学习方法，其中卫星收集并本地处理数据，地面站聚合本地模型。

Method: 设计了一种通信高效的联邦学习算法，采用本地训练减少通信次数，压缩减少通信大小，并提出了增强精度的误差反馈机制，该机制还可作为算法无关的方案更广泛应用。

Result: 通过理论分析算法的收敛性，并在真实空间场景中进行仿真，与现有技术相比展现出优越性能。

Conclusion: 提出的通信高效联邦学习算法在卫星星座环境中有效，通过本地训练、压缩和误差反馈机制实现了准确模型训练，同时显著减少了通信开销。

Abstract: Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.

</details>


### [24] [Row-stochastic matrices can provably outperform doubly stochastic matrices in decentralized learning](https://arxiv.org/abs/2511.19513)
*Bing Liu,Boao Kong,Limin Lu,Kun Yuan,Chengcheng Zhao*

Main category: cs.LG

TL;DR: 本文通过加权希尔伯特空间框架分析去中心化学习中两种权重处理策略的收敛性差异，发现行随机矩阵设计在特定条件下收敛更快，即使其谱间隙更小。


<details>
  <summary>Details</summary>
Motivation: 澄清去中心化学习中两种权重处理策略（嵌入本地损失vs使用λ诱导行随机矩阵）的本质差异，现有欧几里得分析无法充分解释其收敛行为差异。

Method: 建立加权希尔伯特空间框架L²(λ;ℝᵈ)，分析两种策略在该几何空间中的收敛性，使用Rayleigh商和Loewner序特征值比较推导拓扑条件。

Result: 在加权希尔伯特空间中，行随机矩阵成为自伴算子而双随机矩阵不是，产生额外的惩罚项放大共识误差；行随机设计在特定拓扑条件下收敛更快。

Conclusion: 收敛差异不仅源于谱间隙，还源于惩罚项；推导出保证行随机设计优势的拓扑条件，为实际拓扑设计提供指导。

Abstract: Decentralized learning often involves a weighted global loss with heterogeneous node weights $λ$. We revisit two natural strategies for incorporating these weights: (i) embedding them into the local losses to retain a uniform weight (and thus a doubly stochastic matrix), and (ii) keeping the original losses while employing a $λ$-induced row-stochastic matrix. Although prior work shows that both strategies yield the same expected descent direction for the global loss, it remains unclear whether the Euclidean-space guarantees are tight and what fundamentally differentiates their behaviors. To clarify this, we develop a weighted Hilbert-space framework $L^2(λ;\mathbb{R}^d)$ and obtain convergence rates that are strictly tighter than those from Euclidean analysis. In this geometry, the row-stochastic matrix becomes self-adjoint whereas the doubly stochastic one does not, creating additional penalty terms that amplify consensus error, thereby slowing convergence. Consequently, the difference in convergence arises not only from spectral gaps but also from these penalty terms. We then derive sufficient conditions under which the row-stochastic design converges faster even with a smaller spectral gap. Finally, by using a Rayleigh-quotient and Loewner-order eigenvalue comparison, we further obtain topology conditions that guarantee this advantage and yield practical topology-design guidelines.

</details>


### [25] [Automating Deception: Scalable Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2511.19517)
*Adarsh Kumarappan,Ananya Mujoo*

Main category: cs.LG

TL;DR: 本文开发了一个自动化流水线来生成大规模、基于心理学的多轮越狱数据集，评估了不同LLM对多轮对话攻击的脆弱性，发现GPT系列模型在对话历史下攻击成功率显著上升，而Gemini 2.5 Flash表现出极强的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 多轮对话攻击利用心理学原理（如登门槛效应）绕过LLM的安全对齐，现有防御方法依赖难以扩展的手动数据集创建，阻碍了防御进展。

Method: 系统地将登门槛技术转化为可复现模板，创建了包含1,500个场景的基准数据集，评估了来自三个主要LLM家族的七个模型在有无对话历史条件下的表现。

Result: GPT家族模型对对话历史表现出显著脆弱性，攻击成功率最多增加32个百分点；Gemini 2.5 Flash几乎免疫这些攻击；Claude 3 Haiku表现出强但不完美的抵抗力。

Conclusion: 当前安全架构在处理对话上下文方面存在关键差异，需要能够抵抗基于叙事操纵的防御机制。

Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.

</details>


### [26] [Shortcut Invariance: Targeted Jacobian Regularization in Disentangled Latent Space](https://arxiv.org/abs/2511.19525)
*Shivam Pal,Sakshi Varshney,Piyush Rai*

Main category: cs.LG

TL;DR: 提出一种简单有效的训练方法，通过功能不变性而非表示鲁棒性来解决深度神经网络中的捷径学习问题，在解耦的潜在空间中识别并抑制捷径特征，实现最先进的OOD性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易学习训练数据中的捷径相关性，导致在分布外泛化时出现严重失败。现有方法通常学习鲁棒表示，但这种方法复杂、脆弱且难以扩展。

Method: 在解耦的潜在空间中，通过捷径特征与标签的强相关性识别候选捷径特征，然后在训练过程中注入有针对性的各向异性潜在噪声，使分类器对这些特征不敏感。

Result: 在已建立的捷径学习基准测试中实现了最先进的OOD性能。

Conclusion: 通过功能不变性而非表示鲁棒性的方法，可以有效解决捷径学习问题，该方法简单有效且可扩展。

Abstract: Deep neural networks are prone to learning shortcuts, spurious and easily learned correlations in training data that cause severe failures in out-of-distribution (OOD) generalization. A dominant line of work seeks robustness by learning a robust representation, often explicitly partitioning the latent space into core and spurious components; this approach can be complex, brittle, and difficult to scale. We take a different approach, instead of a robust representation, we learn a robust function. We present a simple and effective training method that renders the classifier functionally invariant to shortcut signals. Our method operates within a disentangled latent space, which is essential as it isolates spurious and core features into distinct dimensions. This separation enables the identification of candidate shortcut features by their strong correlation with the label, used as a proxy for semantic simplicity. The classifier is then desensitized to these features by injecting targeted, anisotropic latent noise during training. We analyze this as targeted Jacobian regularization, which forces the classifier to ignore spurious features and rely on more complex, core semantic signals. The result is state-of-the-art OOD performance on established shortcut learning benchmarks.

</details>


### [27] [Learning to Solve Weighted Maximum Satisfiability with a Co-Training Architecture](https://arxiv.org/abs/2511.19544)
*Kaidi Wan,Minghao Liu,Yong Lai*

Main category: cs.LG

TL;DR: SplitGNN是一种基于图神经网络的加权最大可满足性问题求解方法，通过协同训练架构和新的图表示方法，在大型加权MaxSAT基准测试中超越了现代启发式求解器。


<details>
  <summary>Details</summary>
Motivation: 解决加权最大可满足性问题在现实应用中具有重要意义，但现有方法在处理大规模和复杂实例时面临挑战，需要更有效的学习型求解方法。

Method: 提出边缘分裂因子图的图表示方法，结合监督消息传递机制和无监督解增强层的协同训练架构，使用GPU加速层进行高效分数计算和基于松弛的优化。

Result: SplitGNN实现了3倍更快的收敛速度和更好的预测性能，在大型加权MaxSAT基准测试中超越了现代启发式求解器，并在多样化结构实例上展现出卓越的泛化能力。

Conclusion: SplitGNN通过创新的图表示和协同训练架构，为加权MaxSAT问题提供了高效且泛化能力强的求解方案，在性能和效率方面均优于现有方法。

Abstract: Wepropose SplitGNN, a graph neural network (GNN)-based
  approach that learns to solve weighted maximum satisfiabil ity (MaxSAT) problem. SplitGNN incorporates a co-training
  architecture consisting of supervised message passing mech anism and unsupervised solution boosting layer. A new graph
  representation called edge-splitting factor graph is proposed
  to provide more structural information for learning, which is
  based on spanning tree generation and edge classification. To
  improve the solutions on challenging and weighted instances,
  we implement a GPU-accelerated layer applying efficient
  score calculation and relaxation-based optimization. Exper iments show that SplitGNN achieves 3* faster convergence
  and better predictions compared with other GNN-based ar chitectures. More notably, SplitGNN successfully finds solu tions that outperform modern heuristic MaxSAT solvers on
  much larger and harder weighted MaxSAT benchmarks, and
  demonstrates exceptional generalization abilities on diverse
  structural instances.

</details>


### [28] [When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics](https://arxiv.org/abs/2511.19548)
*Yiven,Zhu*

Main category: cs.LG

TL;DR: 本文提出了一个神经经济学福利推断框架，将神经信号、计算决策模型和规范性福利标准联系起来，探讨神经数据在何种条件下可以合法地为政策提供福利判断。


<details>
  <summary>Details</summary>
Motivation: 神经经济学承诺基于神经和计算证据进行福利分析，同时政策和商业行为体越来越多地引用神经数据来为家长式监管、"基于大脑"的干预和新福利措施辩护。本文旨在确定神经数据在什么条件下可以合法地为政策提供福利判断，而不仅仅是描述行为。

Method: 开发了一个非经验性的、基于模型的框架，将三个层次联系起来：神经信号、计算决策模型和规范性福利标准。在演员-评论家强化学习模型中，形式化了从神经活动到潜在价值和预测误差，再到福利主张的推断路径。

Result: 研究表明，只有当神经-计算映射得到充分验证、决策模型识别出"真实"利益与情境依赖的错误，并且福利标准被明确指定和辩护时，神经证据才能约束福利判断。应用该框架到成瘾、神经营销和环境政策中，推导出了神经经济学福利推断清单。

Conclusion: 该分析将大脑和人工智能体视为价值学习系统，同时表明内部奖励信号（无论是生物的还是人工的）都是计算量，没有明确的规范性模型就不能被视为福利衡量标准。

Abstract: Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, "brain-based" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies "true" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.

</details>


### [29] [Online Sparse Feature Selection in Data Streams via Differential Evolution](https://arxiv.org/abs/2511.19555)
*Ruiyang Xu*

Main category: cs.LG

TL;DR: 提出了一种新的在线差分进化稀疏特征选择方法(ODESFS)，通过潜在因子分析填补缺失值和差分进化评估特征重要性，在六个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有在线稀疏流特征选择方法在特征评估方面存在显著局限性，导致性能下降，需要改进特征选择效果。

Method: 使用潜在因子分析模型进行缺失值填补，并通过差分进化算法评估特征重要性。

Result: 在六个真实数据集上的实验表明，ODESFS能够选择最优特征子集并获得更优的准确率，持续优于最先进的OSFS和OS2FS方法。

Conclusion: ODESFS方法通过结合缺失值填补和差分进化特征评估，有效解决了高维流数据特征选择问题，显著提升了性能。

Abstract: The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.

</details>


### [30] [Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport](https://arxiv.org/abs/2511.19561)
*Zecheng Pan,Zhikang Chen,Ding Li,Min Zhang,Sen Cui,Hongshuo Jin,Luqi Tao,Yi Yang,Deheng Ye,Yu Zhang,Tingting Zhu,Tianling Ren*

Main category: cs.LG

TL;DR: OTMF是一个基于最优传输理论的模型融合框架，通过发现应用于任务向量的共同掩码来对齐任务特定模型的语义几何，解决参数插值引起的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法主要依赖权重空间的参数插值，这会导致特征空间的显著分布偏移并削弱任务特定知识。

Method: OTMF使用最优传输计划发现应用于任务向量的共同掩码，选择性提取可转移和任务无关的组件，同时保持每个任务的独特结构身份。支持持续融合范式，增量集成新任务向量。

Result: 在多个视觉和语言基准测试中，OTMF在准确性和效率方面都达到了最先进的性能。

Conclusion: OTMF为模型融合提供了实用和理论价值，能够有效解决分布偏移问题并支持高效的多任务融合。

Abstract: Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.

</details>


### [31] [ModHiFi: Identifying High Fidelity predictive components for Model Modification](https://arxiv.org/abs/2511.19566)
*Dhruva Kashyap,Chaitanya Murti,Pranav K Nayak,Tanay Narshana,Chiranjib Bhattacharyya*

Main category: cs.LG

TL;DR: 提出了ModHiFi算法，无需训练数据或损失函数访问即可进行模型修改，包括剪枝和遗忘任务。


<details>
  <summary>Details</summary>
Motivation: 开放权重模型通常不提供训练数据或损失函数访问，现有修改技术需要梯度或真实标签，在计算资源有限的情况下不可行。

Method: 基于局部重构误差与全局重构误差的线性关系，提出子集保真度指标，开发ModHiFi算法进行模型修改。

Result: ModHiFi-P在ImageNet模型上比现有技术快11%，在语言模型上表现竞争性；ModHiFi-U在CIFAR-10上实现完全遗忘，在Swin Transformers上表现竞争性。

Conclusion: 证明了局部重构行为可以量化组件重要性，提出的方法在无训练数据访问的情况下有效进行模型修改。

Abstract: Open weight models, which are ubiquitous, rarely provide access to their training data or loss function. This makes modifying such models for tasks such as pruning or unlearning constrained by this unavailability an active area of research. Existing techniques typically require gradients or ground-truth labels, rendering them infeasible in settings with limited computational resources. In this work, we investigate the fundamental question of identifying components that are critical to the model's predictive performance, without access to either gradients or the loss function, and with only distributional access such as synthetic data. We theoretically demonstrate that the global reconstruction error is linearly bounded by local reconstruction errors for Lipschitz-continuous networks such as CNNs and well-trained Transformers (which, contrary to existing literature, we find exhibit Lipschitz continuity). This motivates using the locally reconstructive behavior of component subsets to quantify their global importance, via a metric that we term Subset Fidelity. In the uncorrelated features setting, selecting individual components via their Subset Fidelity scores is optimal, which we use to propose ModHiFi, an algorithm for model modification that requires no training data or loss function access. ModHiFi-P, for structured pruning, achieves an 11% speedup over the current state of the art on ImageNet models and competitive performance on language models. ModHiFi-U, for classwise unlearning, achieves complete unlearning on CIFAR-10 without fine-tuning and demonstrates competitive performance on Swin Transformers.

</details>


### [32] [An Invariant Latent Space Perspective on Language Model Inversion](https://arxiv.org/abs/2511.19569)
*Wentao Ye,Jiaqi Hu,Haobo Wang,Xinpeng Ti,Zhiqing Xiao,Hao Chen,Liyao Li,Lei Feng,Sai Wu,Junbo Zhao*

Main category: cs.LG

TL;DR: 本文提出Inv^2A方法，通过利用LLM的潜在空间不变性假设来改进语言模型反演攻击，在多个数据集上优于基线方法4.77% BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 语言模型反演（LMI）对用户隐私和系统安全构成威胁，现有方法效果有限且依赖大规模反演语料库。

Method: 提出不变潜在空间假设，将LLM视为不变解码器，仅学习轻量级逆编码器；采用两阶段训练：对比对齐和监督强化；支持训练自由邻域搜索。

Result: 在9个数据集上平均BLEU分数比基线高4.77%，减少了对大型反演语料库的依赖。

Conclusion: 现有防御措施保护有限，需要更强的防御策略；Inv^2A展示了LMI攻击的有效性。

Abstract: Language model inversion (LMI), i.e., recovering hidden prompts from outputs, emerges as a concrete threat to user privacy and system security. We recast LMI as reusing the LLM's own latent space and propose the Invariant Latent Space Hypothesis (ILSH): (1) diverse outputs from the same source prompt should preserve consistent semantics (source invariance), and (2) input<->output cyclic mappings should be self-consistent within a shared latent space (cyclic invariance). Accordingly, we present Inv^2A, which treats the LLM as an invariant decoder and learns only a lightweight inverse encoder that maps outputs to a denoised pseudo-representation. When multiple outputs are available, they are sparsely concatenated at the representation layer to increase information density. Training proceeds in two stages: contrastive alignment (source invariance) and supervised reinforcement (cyclic invariance). An optional training-free neighborhood search can refine local performance. Across 9 datasets covering user and system prompt scenarios, Inv^2A outperforms baselines by an average of 4.77% BLEU score while reducing dependence on large inverse corpora. Our analysis further shows that prevalent defenses provide limited protection, underscoring the need for stronger strategies. The source code and data involved in this paper can be found in https://github.com/yyy01/Invariant_Attacker.

</details>


### [33] [Neural Tractability via Structure: Learning-Augmented Algorithms for Graph Combinatorial Optimization](https://arxiv.org/abs/2511.19573)
*Jialiang Li,Weitong Chen,Mingyu Guo*

Main category: cs.LG

TL;DR: 提出一个结合神经网络推理效率和参数化算法最优性保证的新框架，通过识别实例的结构化难易部分，让神经网络处理困难部分生成指导信号，参数化算法搜索剩余简单部分，实现更好的解质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 神经网络组合优化求解器虽然推理快，但解质量不如经典搜索算法；经典搜索算法虽能保证最优性但速度慢。需要结合两者的优势。

Method: 使用参数化算法识别实例的结构化难易部分，神经网络处理困难部分生成指导信号，参数化算法基于指导信号搜索剩余简单部分。

Result: 在多个组合优化任务上，该框架达到与商业求解器相当的优越解质量，并改善了分布外泛化能力。

Conclusion: 该框架成功结合了神经网络的推理效率和参数化算法的最优性保证，在解质量和泛化能力方面均优于纯神经网络求解器。

Abstract: Neural models have shown promise in solving NP-hard graph combinatorial optimization (CO) problems. Once trained, they offer fast inference and reasonably high-quality solutions for in-distribution testing instances, but they generally fall short in terms of absolute solution quality compared to classical search-based algorithms that are admittedly slower but offer optimality guarantee once search finishes.
  We propose a novel framework that combines the inference efficiency and exploratory power of neural models with the solution quality guarantee of search-based algorithms. In particular, we use parameterized algorithms (PAs) as the search component. PAs are dedicated to identifying easy instances of generally NP-hard problems, and allow for practically efficient search by exploiting structural simplicity (of the identified easy instances). Under our framework, we use parameterized analysis to identify the structurally hard parts of a CO instance. The neural model handles the hard parts by generating advisory signals based on its data-driven understanding. The PA-based search component then integrates the advisory signals to systematically and efficiently searches through the remaining structurally easy parts. Notably, our framework is agnostic to the choice of neural model and produces strictly better solutions than neural solvers alone.
  We examine our framework on multiple CO tasks. Empirical results show that it achieves superior solution quality, competitive with that of commercial solvers. Furthermore, by using the neural model only for exploratory advisory signals, our framework exhibits improved out-of-distribution generalization, addressing a key limitation of existing neural CO solvers.

</details>


### [34] [Learning Massively Multitask World Models for Continuous Control](https://arxiv.org/abs/2511.19584)
*Nicklas Hansen,Hao Su,Xiaolong Wang*

Main category: cs.LG

TL;DR: 提出了一个包含200个多样化任务的基准测试，并开发了Newt语言条件多任务世界模型，通过演示预训练和在线交互联合优化，实现了比强基线更好的多任务性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决通用控制需要能够在多种任务和实体上操作的智能体，但目前连续控制的强化学习研究主要局限于单任务或离线模式，需要探索在线RL在大规模多任务环境中的可扩展性。

Method: 首先在演示数据上进行预训练以获取任务感知表示和动作先验，然后通过在线交互在所有任务上进行联合优化。

Result: Newt在多任务性能和数据效率方面优于强基线，表现出强大的开环控制能力，并能快速适应未见过的任务。

Conclusion: 证明了在线强化学习可以扩展到数百个任务，为通用控制智能体的发展提供了新的可能性。

Abstract: General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.

</details>


### [35] [Many Ways to be Right: Rashomon Sets for Concept-Based Neural Networks](https://arxiv.org/abs/2511.19636)
*Shihan Feng,Cheng Zhang,Michael Xi,Ethan Hsu,Lesia Semenova,Chudi Zhong*

Main category: cs.LG

TL;DR: 本文提出了Rashomon概念瓶颈模型框架，通过结合轻量级适配器模块和多样性正则化训练目标，学习多个准确但通过不同人类可理解概念进行推理的神经网络。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络很少只有单一的正确方式。对于许多任务，多个模型可以依赖不同特征或推理模式达到相同性能，这种特性被称为Rashomon效应。然而，在深度架构中发现这种多样性具有挑战性，因为它们的连续参数空间包含无数数值不同但行为相似的近最优解。

Method: 结合轻量级适配器模块与多样性正则化训练目标，构建一组多样化的基于概念的深度模型，无需从头重新训练。

Result: 生成的网络为相同预测提供根本不同的推理过程，揭示了在同等性能解决方案中概念依赖和决策制定的变化。

Conclusion: 该框架能够系统性地探索深度模型中数据驱动推理的多样性，为审计、比较和对齐同等准确解决方案提供了新机制。

Abstract: Modern neural networks rarely have a single way to be right. For many tasks, multiple models can achieve identical performance while relying on different features or reasoning patterns, a property known as the Rashomon Effect. However, uncovering this diversity in deep architectures is challenging as their continuous parameter spaces contain countless near-optimal solutions that are numerically distinct but often behaviorally similar. We introduce Rashomon Concept Bottleneck Models, a framework that learns multiple neural networks which are all accurate yet reason through distinct human-understandable concepts. By combining lightweight adapter modules with a diversity-regularized training objective, our method constructs a diverse set of deep concept-based models efficiently without retraining from scratch. The resulting networks provide fundamentally different reasoning processes for the same predictions, revealing how concept reliance and decision making vary across equally performing solutions. Our framework enables systematic exploration of data-driven reasoning diversity in deep models, offering a new mechanism for auditing, comparison, and alignment across equally accurate solutions.

</details>


### [36] [Structured Noise Modeling for Enhanced Time-Series Forecasting](https://arxiv.org/abs/2511.19657)
*Sepideh Koohfar*

Main category: cs.LG

TL;DR: 提出了一种预测-模糊-去噪框架，通过结构化噪声建模提高时间序列预测的保真度，在多个数据集上实现了多时间尺度预测精度和稳定性的提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的时间序列预测面临挑战，因为时间模式在多个尺度上运行，从广泛上下文趋势到快速细粒度波动。现有神经网络模型难以表示这些相互作用的动态，导致预测不稳定和下游应用可靠性降低。

Method: 引入预测-模糊-去噪框架，包含可学习的高斯过程模块生成平滑相关扰动，鼓励预测主干捕获长程结构，同时专用细化模型恢复高分辨率时间细节。组件联合训练实现自然能力划分。

Result: 在电力、交通和太阳能数据集上的实验显示，多时间尺度预测精度和稳定性持续提升。模块化设计允许模糊-去噪层作为预训练模型的轻量级增强，支持有限数据场景下的高效适应。

Conclusion: 通过增强细粒度时间预测的可靠性和可解释性，该框架有助于构建更值得信赖的AI系统，应用于能源、基础设施等时间关键领域的预测驱动决策支持。

Abstract: Time-series forecasting remains difficult in real-world settings because temporal patterns operate at multiple scales, from broad contextual trends to fast, fine-grained fluctuations that drive critical decisions. Existing neural models often struggle to represent these interacting dynamics, leading to unstable predictions and reduced reliability in downstream applications. This work introduces a forecast-blur-denoise framework that improves temporal fidelity through structured noise modeling. The approach incorporates a learnable Gaussian Process module that generates smooth, correlated perturbations, encouraging the forecasting backbone to capture long-range structure while a dedicated refinement model restores high-resolution temporal detail. Training the components jointly enables natural competence division and avoids the artifacts commonly produced by isotropic corruption methods. Experiments across electricity, traffic, and solar datasets show consistent gains in multi-horizon accuracy and stability. The modular design also allows the blur-denoise layer to operate as a lightweight enhancement for pretrained models, supporting efficient adaptation in limited-data scenarios. By strengthening the reliability and interpretability of fine-scale temporal predictions, this framework contributes to more trustworthy AI systems used in forecasting-driven decision support across energy, infrastructure, and other time-critical domains.

</details>


### [37] [Demystifying Diffusion Objectives: Reweighted Losses are Better Variational Bounds](https://arxiv.org/abs/2511.19664)
*Jiaxin Shi,Michalis K. Titsias*

Main category: cs.LG

TL;DR: 提出一种对扩散模型重加权损失的新理论解释，通过构建时间相关的变分下界级联来改进标准证据下界，降低数据模型KL散度，可应用于连续高斯扩散和掩码扩散模型。


<details>
  <summary>Details</summary>
Motivation: 为广泛使用的扩散模型重加权损失提供理论解释，改进标准证据下界，降低数据模型之间的KL散度。

Method: 构建时间相关的变分下界级联，结合这些下界产生重加权目标，可应用于连续高斯扩散和掩码扩散模型。

Result: 在掩码扩散中展示该框架，在像素空间图像建模中显著优于先前训练损失，样本质量接近连续扩散模型。

Conclusion: 为掩码图像模型中广泛使用的简单加权方案提供了理论依据，提出的重加权目标可提升扩散模型性能。

Abstract: We derive a new theoretical interpretation of the reweighted losses that are widely used for training diffusion models. Our method is based on constructing a cascade of time-dependent variational lower bounds on the data log-likelihood, that provably improves upon the standard evidence lower bound and results in reduced data-model KL-divergences. Combining such bounds gives rise to reweighted objectives that can be applied to any generative diffusion model including both continuous Gaussian diffusion and masked (discrete) diffusion models. Then, we showcase this framework in masked diffusion and report significant improvements over previous training losses in pixel-space image modeling, approaching sample quality comparable to continuous diffusion models. Our results also provide a theoretical justification for the simple weighting scheme widely used in masked image models.

</details>


### [38] [TREASURE: A Transformer-Based Foundation Model for High-Volume Transaction Understanding](https://arxiv.org/abs/2511.19693)
*Chin-Chia Michael Yeh,Uday Singh Saini,Xin Dai,Xiran Fan,Shubham Jain,Yujie Fan,Jiarui Sun,Junpeng Wang,Menghai Pan,Yingtong Dou,Yuzhong Chen,Vineeth Rakesh,Liang Wang,Yan Zheng,Mahashweta Das*

Main category: cs.LG

TL;DR: TREASURE是一个基于Transformer的多用途交易数据基础模型，能同时捕捉消费者行为和支付网络信号，在异常行为检测和推荐系统方面显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 支付网络产生大量交易记录，正确建模这些数据可以实现异常行为检测和个性化消费者洞察，从而改善人们的生活。

Method: 使用Transformer架构，包含静态和动态属性的专用输入模块，以及针对高基数分类属性的高效训练范式。

Result: 异常行为检测性能比生产系统提高111%，推荐模型增强104%，在行业级数据集上验证有效。

Conclusion: TREASURE作为交易数据的通用表示编码器，为准确推荐和异常检测等应用提供了全面的信息基础。

Abstract: Payment networks form the backbone of modern commerce, generating high volumes of transaction records from daily activities. Properly modeling this data can enable applications such as abnormal behavior detection and consumer-level insights for hyper-personalized experiences, ultimately improving people's lives. In this paper, we present TREASURE, TRansformer Engine As Scalable Universal transaction Representation Encoder, a multipurpose transformer-based foundation model specifically designed for transaction data. The model simultaneously captures both consumer behavior and payment network signals (such as response codes and system flags), providing comprehensive information necessary for applications like accurate recommendation systems and abnormal behavior detection. Verified with industry-grade datasets, TREASURE features three key capabilities: 1) an input module with dedicated sub-modules for static and dynamic attributes, enabling more efficient training and inference; 2) an efficient and effective training paradigm for predicting high-cardinality categorical attributes; and 3) demonstrated effectiveness as both a standalone model that increases abnormal behavior detection performance by 111% over production systems and an embedding provider that enhances recommendation models by 104%. We present key insights from extensive ablation studies, benchmarks against production models, and case studies, highlighting valuable knowledge gained from developing TREASURE.

</details>


### [39] [TiCT: A Synthetically Pre-Trained Foundation Model for Time Series Classification](https://arxiv.org/abs/2511.19694)
*Chin-Chia Michael Yeh,Uday Singh Saini,Junpeng Wang,Xin Dai,Xiran Fan,Jiarui Sun,Yujie Fan,Yan Zheng*

Main category: cs.LG

TL;DR: TiCT是一个基于Transformer的时间序列基础模型，专门用于上下文分类，仅使用合成数据进行预训练，无需微调即可在推理时通过少量示例实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据普遍存在，但标注成本高昂。现有的大规模时间序列模型主要关注预测任务，缺乏无需微调、支持上下文学习的通用分类模型。

Method: 提出TiCT模型：1）新颖架构，包括可扩展的基于比特的标签编码和特殊输出注意力机制以处理任意类别数；2）结合Mixup启发过程和数据增强的合成预训练框架，促进泛化和噪声不变性。

Result: 在UCR Archive上的广泛评估显示，TiCT仅使用上下文示例在推理时就能达到与最先进监督方法相竞争的性能，且无需更新任何模型权重。

Conclusion: TiCT证明了仅使用合成数据预训练的模型能够实现有效的上下文时间序列分类，为开发通用时间序列基础模型提供了可行路径。

Abstract: The ubiquity of time series data creates a strong demand for general-purpose foundation models, yet developing them for classification remains a significant challenge, largely due to the high cost of labeled data. Foundation models capable of in-context learning (ICL) offer a powerful solution, adapting to new tasks with minimal examples and reducing the need for extensive retraining. However, prior work on large-scale time series models has predominantly focused on forecasting, leaving a critical gap for versatile, fine-tuning-free classification. To address this, we introduce TiCT (Time-series in-Context Transformer), a transformer-based model pre-trained exclusively on synthetic data to perform in-context classification. We make two primary technical contributions: 1) a novel architecture featuring a scalable bit-based label encoding and a special output attention mechanism to handle an arbitrary number of classes; and 2) a synthetic pre-training framework that combines a Mixup-inspired process with data augmentation to foster generalization and noise invariance. Extensive evaluations on the UCR Archive show that TiCT achieves competitive performance against state-of-the-art supervised methods. Crucially, this is accomplished using only in-context examples at inference time, without updating a single model weight.

</details>


### [40] [CafeQ: Calibration-free Quantization via Learned Transformations and Adaptive Rounding](https://arxiv.org/abs/2511.19705)
*Ziteng Sun,Adrian Benton,Samuel Kushnir,Asher Trockman,Vikas Singh,Suhas Diggavi,Ananda Theertha Suresh*

Main category: cs.LG

TL;DR: 提出无需校准数据的后训练量化方法，通过优化变换和自适应舍入来减少大语言模型量化误差，在Gemma 2模型上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化方法依赖校准数据，但在实际场景中校准数据可能不可用或受隐私限制，需要开发无需校准数据的量化技术。

Method: 设计无校准数据的量化损失代理函数，使用结构化矩阵变换处理单个矩阵，对计算图中直接交互的配对权重使用双矩阵变换和自适应舍入方法。

Result: 在Gemma 2 9B模型上，4位量化平均基准分数从61.9提升到62.4，3位量化从52.0提升到60.6，计算开销增加不到3%。

Conclusion: 该方法在无需校准数据的情况下实现了与需要校准数据的GPTQ方法相当的性能，为实际部署提供了可行的量化解决方案。

Abstract: Post-training quantization is an effective method for reducing the serving cost of large language models, where the standard approach is to use a round-to-nearest quantization level scheme. However, this often introduces large errors due to outliers in the weights. Proposed mitigation mechanisms include applying adaptive rounding, random rotation transformations or committing to a post-training target using calibration data. Unfortunately, this reliance on calibration data can be severely limiting in some real-world scenarios as such data may be unavailable or subject to privacy regulations. In this paper, we propose algorithms to optimize transformations and adaptive rounding without access to any calibration data. The optimization is achieved by designing a suitable proxy function for the quantization loss without calibration data. To maintain inference efficiency, we perform structured matrix transformations for single matrices. For paired weights that interact directly in the computation graph, we use dual matrix transformations and adaptive rounding methods. We conduct experiments on Gemma 2 models, and observe consistent improvement over the baselines. For Gemma 2 9B quantization, our method improves the average benchmark score from 61.9 to 62.4 for 4-bit quantization and from 52.0 to 60.6 for 3-bit quantization, while adding less than 3% of computation overhead. Furthermore, our method achieves performance comparable to the commonly used GPTQ method, which requires calibration data.

</details>


### [41] [Training-Free Active Learning Framework in Materials Science with Large Language Models](https://arxiv.org/abs/2511.19730)
*Hongchen Wang,Rafael Espinosa Castañeda,Jay R. Werber,Yao Fehlis,Edward Kim,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: LLM-AL框架使用大语言模型进行主动学习，在材料科学四个数据集上相比传统ML模型减少70%实验次数，性能更优且探索性更强。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习的ML模型存在冷启动问题和领域特定特征工程限制，而LLMs的预训练知识和通用表示能力提供了新的实验选择范式。

Method: 提出LLM-AL框架，在迭代少样本设置下运行，探索两种提示策略：简洁数值输入和扩展描述性文本，以适应不同特征的数据集。

Result: 在所有数据集上，LLM-AL将到达最优候选所需的实验次数减少70%以上，始终优于传统ML模型，进行更广泛探索性搜索。

Conclusion: LLM-AL可作为传统主动学习管道的通用替代方案，实现更高效、可解释的实验选择，并具有LLM驱动自主发现的潜力。

Abstract: Active learning (AL) accelerates scientific discovery by prioritizing the most informative experiments, but traditional machine learning (ML) models used in AL suffer from cold-start limitations and domain-specific feature engineering, restricting their generalizability. Large language models (LLMs) offer a new paradigm by leveraging their pretrained knowledge and universal token-based representations to propose experiments directly from text-based descriptions. Here, we introduce an LLM-based active learning framework (LLM-AL) that operates in an iterative few-shot setting and benchmark it against conventional ML models across four diverse materials science datasets. We explored two prompting strategies: one using concise numerical inputs suited for datasets with more compositional and structured features, and another using expanded descriptive text suited for datasets with more experimental and procedural features to provide additional context. Across all datasets, LLM-AL could reduce the number of experiments needed to reach top-performing candidates by over 70% and consistently outperformed traditional ML models. We found that LLM-AL performs broader and more exploratory searches while still reaching the optima with fewer iterations. We further examined the stability boundaries of LLM-AL given the inherent non-determinism of LLMs and found its performance to be broadly consistent across runs, within the variability range typically observed for traditional ML approaches. These results demonstrate that LLM-AL can serve as a generalizable alternative to conventional AL pipelines for more efficient and interpretable experiment selection and potential LLM-driven autonomous discovery.

</details>


### [42] [DISCO: A Browser-Based Privacy-Preserving Framework for Distributed Collaborative Learning](https://arxiv.org/abs/2511.19750)
*Julien T. T. Vignoud,Valérian Rousset,Hugo El Guedj,Ignacio Aleman,Walid Bennaceur,Batuhan Faik Derinbay,Eduard Ďurech,Damien Gengler,Lucas Giordano,Felix Grimberg,Franziska Lippoldt,Christina Kopidaki,Jiafan Liu,Lauris Lopata,Nathan Maire,Paul Mansat,Martin Milenkoski,Emmanuel Omont,Güneş Özgün,Mina Petrović,Francesco Posa,Morgan Ridel,Giorgio Savini,Marcel Torne,Lucas Trognon,Alyssa Unell,Olena Zavertiaieva,Sai Praneeth Karimireddy,Tahseen Rabbani,Mary-Anne Hartley,Martin Jaggi*

Main category: cs.LG

TL;DR: DISCO是一个开源分布式协作学习平台，允许非技术用户在不共享原始数据的情况下协作构建机器学习模型，支持联邦学习和去中心化范式。


<details>
  <summary>Details</summary>
Motivation: 解决数据因隐私、知识产权和法律约束而无法共享的问题，避免数据碎片化导致的统计能力下降和可访问性偏见。

Method: 基于浏览器的本地训练，提供模块化设计，支持联邦学习和去中心化范式，提供不同级别的隐私保证和多种权重聚合策略。

Result: 开发了开源平台DISCO，代码库和展示界面已公开，支持跨平台使用（包括智能手机）。

Conclusion: DISCO为非技术用户提供了无需编程知识即可协作训练机器学习模型的解决方案，同时保护数据隐私并支持模型个性化。

Abstract: Data is often impractical to share for a range of well considered reasons, such as concerns over privacy, intellectual property, and legal constraints. This not only fragments the statistical power of predictive models, but creates an accessibility bias, where accuracy becomes inequitably distributed to those who have the resources to overcome these concerns. We present DISCO: an open-source DIStributed COllaborative learning platform accessible to non-technical users, offering a means to collaboratively build machine learning models without sharing any original data or requiring any programming knowledge. DISCO's web application trains models locally directly in the browser, making our tool cross-platform out-of-the-box, including smartphones. The modular design of \disco offers choices between federated and decentralized paradigms, various levels of privacy guarantees and several approaches to weight aggregation strategies that allow for model personalization and bias resilience in the collaborative training. Code repository is available at https://github.com/epfml/disco and a showcase web interface at https://discolab.ai

</details>


### [43] [When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements](https://arxiv.org/abs/2511.19794)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 论文提出了一种保守的评估协议，用于判断机器学习中1-2个百分点的性能提升是否真实有效，而不是随机噪声。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习论文经常报告1-2个百分点的基准测试改进，但这些增益对随机种子、数据排序和实现细节高度敏感，且缺乏不确定性估计或显著性检验。

Method: 提出了基于配对多种子运行、偏差校正加速(BCa)自助法置信区间和符号翻转置换检验的简单评估协议。

Result: 在CIFAR-10、CIFAR-10N和AG News上的实验显示，单次运行和非配对t检验经常错误地表明0.6-2.0个百分点的改进具有显著性，而提出的配对协议在仅有三个种子的情况下从未错误声明显著性。

Conclusion: 对于计算预算有限情况下的小幅增益，这种保守的评估方法是一个更安全的默认选择，可以防止过度声称。

Abstract: Recent machine learning papers often report 1-2 percentage point improvements from a single run on a benchmark. These gains are highly sensitive to random seeds, data ordering, and implementation details, yet are rarely accompanied by uncertainty estimates or significance tests. It is therefore unclear when a reported +1-2% reflects a real algorithmic advance versus noise.
  We revisit this problem under realistic compute budgets, where only a few runs are affordable. We propose a simple, PC-friendly evaluation protocol based on paired multi-seed runs, bias-corrected and accelerated (BCa) bootstrap confidence intervals, and a sign-flip permutation test on per-seed deltas. The protocol is intentionally conservative and is meant as a guardrail against over-claiming.
  We instantiate it on CIFAR-10, CIFAR-10N, and AG News using synthetic no-improvement, small-gain, and medium-gain scenarios. Single runs and unpaired t-tests often suggest significant gains for 0.6-2.0 point improvements, especially on text. With only three seeds, our paired protocol never declares significance in these settings. We argue that such conservative evaluation is a safer default for small gains under tight budgets.

</details>


### [44] [BlockCert: Certified Blockwise Extraction of Transformer Mechanisms](https://arxiv.org/abs/2511.17645)
*Sandro Andric*

Main category: cs.LG

TL;DR: BlockCert是一个用于认证式分块提取Transformer机制并支持认证局部编辑的框架，通过提取结构化替代实现并提供机器可检查的证书来保证近似误差边界。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性和模型编辑领域通常缺乏正式保证，无法明确提取或编辑后的模型在相关输入上与原模型的偏离程度。

Method: 给定预训练Transformer和提示分布，BlockCert提取残差块的结构化替代实现，提供绑定近似误差、记录覆盖指标和哈希底层工件的机器可检查证书，并在Lean 4中形式化基于Lipschitz的组合定理。

Result: 在GPT-2 small、TinyLlama-1.1B-Chat和Llama-3.2-3B上应用，获得高每块覆盖率和小的残差误差，在TinyLlama设置中完全拼接模型在压力提示上与基线困惑度匹配在约6e-5内。

Conclusion: 分块提取与显式证书对于真实Transformer语言模型是可行的，为机制可解释性和模型行为的形式推理提供了实用桥梁。

Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.

</details>


### [45] [Terminal Velocity Matching](https://arxiv.org/abs/2511.19797)
*Linqi Zhou,Mathias Parger,Ayaan Haque,Jiaming Song*

Main category: cs.LG

TL;DR: TVM是一种流匹配的泛化方法，通过建模任意两个扩散时间步之间的转换，在终端时间而非初始时间进行正则化，实现了高保真的一步和少步生成建模。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型需要多步推理，效率较低。TVM旨在实现高质量的一步和少步生成，同时通过理论保证提供分布匹配的上界。

Method: 1. 提出终端速度匹配(TVM)框架，建模任意扩散时间步间的转换；2. 引入最小架构修改实现稳定单阶段训练；3. 开发支持Jacobian-向量积反向传播的融合注意力核。

Result: 在ImageNet-256x256上：1步推理FID为3.29，4步推理FID为1.99；在ImageNet-512x512上：1步推理FID为4.32，4步推理FID为2.94，达到了一/少步模型的SOTA性能。

Conclusion: TVM在理论和实践上均表现出色，为一步和少步生成建模提供了有效的解决方案，在多个基准数据集上实现了最先进的性能。

Abstract: We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.

</details>


### [46] [Scalable Data Attribution via Forward-Only Test-Time Inference](https://arxiv.org/abs/2511.19803)
*Sibo Ma,Julian Nyarko*

Main category: cs.LG

TL;DR: 提出了一种高效的数据归因方法，通过训练期间的短时梯度传播模拟训练样本的参数影响，在推理时仅需前向评估即可获得归因结果，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统影响函数方法需要昂贵的反向传播或Hessian矩阵求逆，在现代网络中不实用。需要一种既能保持影响函数理论优势，又能在推理时高效运行的数据归因方法。

Method: 在训练期间通过短时梯度传播模拟每个训练样本的参数影响，推理时仅使用前向评估读取归因结果，将计算从推理阶段转移到模拟阶段。

Result: 在标准MLP基准测试中，该方法在LOO和LDS等归因指标上匹配或超越了TRAK等最先进基线方法，同时推理成本降低了数个数量级。

Conclusion: 该方法结合了影响函数的理论保真度和一阶方法的可扩展性，为大型预训练模型提供了实用、实时的数据归因理论框架。

Abstract: Data attribution seeks to trace model behavior back to the training examples that shaped it, enabling debugging, auditing, and data valuation at scale. Classical influence-function methods offer a principled foundation but remain impractical for modern networks because they require expensive backpropagation or Hessian inversion at inference. We propose a data attribution method that preserves the same first-order counterfactual target while eliminating per-query backward passes. Our approach simulates each training example's parameter influence through short-horizon gradient propagation during training and later reads out attributions for any query using only forward evaluations. This design shifts computation from inference to simulation, reflecting real deployment regimes where a model may serve billions of user queries but originate from a fixed, finite set of data sources (for example, a large language model trained on diverse corpora while compensating a specific publisher such as the New York Times). Empirically, on standard MLP benchmarks, our estimator matches or surpasses state-of-the-art baselines such as TRAK on standard attribution metrics (LOO and LDS) while offering orders-of-magnitude lower inference cost. By combining influence-function fidelity with first-order scalability, our method provides a theoretical framework for practical, real-time data attribution in large pretrained models.

</details>


### [47] [Learning to Clean: Reinforcement Learning for Noisy Label Correction](https://arxiv.org/abs/2511.19808)
*Marzi Heidari,Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TL;DR: 本文提出RLNLC框架，将噪声标签校正问题建模为强化学习任务，通过策略网络迭代修正训练标签，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习中噪声标签会严重降低预测模型性能，需要有效处理噪声标签问题。

Method: 将噪声标签校正定义为强化学习问题，构建状态空间（数据和标签）、动作空间（标签修正）、奖励机制，使用actor-critic方法训练深度特征表示策略网络。

Result: 在多个基准数据集上的广泛实验表明，RLNLC持续优于现有的噪声标签学习技术。

Conclusion: RLNLC框架能有效校正噪声标签，提升模型训练效果，为噪声标签问题提供了新的强化学习解决方案。

Abstract: The challenge of learning with noisy labels is significant in machine learning, as it can severely degrade the performance of prediction models if not addressed properly. This paper introduces a novel framework that conceptualizes noisy label correction as a reinforcement learning (RL) problem. The proposed approach, Reinforcement Learning for Noisy Label Correction (RLNLC), defines a comprehensive state space representing data and their associated labels, an action space that indicates possible label corrections, and a reward mechanism that evaluates the efficacy of label corrections. RLNLC learns a deep feature representation based policy network to perform label correction through reinforcement learning, utilizing an actor-critic method. The learned policy is subsequently deployed to iteratively correct noisy training labels and facilitate the training of the prediction model. The effectiveness of RLNLC is demonstrated through extensive experiments on multiple benchmark datasets, where it consistently outperforms existing state-of-the-art techniques for learning with noisy labels.

</details>


### [48] [Provably Outlier-resistant Semi-parametric Regression for Transferable Calibration of Low-cost Air-quality Sensors](https://arxiv.org/abs/2511.19810)
*Divyansh Chaurasia,Manoj Daram,Roshan Kumar,Nihal Thukarama Rao,Vipul Sangode,Pranjal Srivastava,Avnish Tripathi,Shoubhik Chakraborty,Akanksha,Ambasht Kumar,Davender Sethi,Sachchida Nand Tripathi,Purushottam Kar*

Main category: cs.LG

TL;DR: 提出了RESPIRE技术来校准低成本空气质量CO传感器，该技术在跨站点、跨季节和跨传感器设置中提供改进的预测性能，并具有抗异常值和可解释性的特点。


<details>
  <summary>Details</summary>
Motivation: 低成本空气质量传感器在校准过程中面临昂贵、耗时的问题，特别是在大规模地理分散部署时，需要一种更有效的校准方法。

Method: 开发RESPIRE校准技术，提供抗异常值的训练算法和可解释模型，能够检测模型过拟合情况。

Result: 基于在四个站点、两个季节和六个传感器包上收集的数据进行实证评估，RESPIRE在跨站点、跨季节和跨传感器设置中表现优于基线方法。

Conclusion: RESPIRE为大规模低成本空气质量监测网络提供了一种有效的传感器校准解决方案，代码已开源。

Abstract: We present a case study for the calibration of Low-cost air-quality (LCAQ) CO sensors from one of the largest multi-site-multi-season-multi-sensor-multi-pollutant mobile air-quality monitoring network deployments in India. LCAQ sensors have been shown to play a critical role in the establishment of dense, expansive air-quality monitoring networks and combating elevated pollution levels. The calibration of LCAQ sensors against regulatory-grade monitors is an expensive, laborious and time-consuming process, especially when a large number of sensors are to be deployed in a geographically diverse layout. In this work, we present the RESPIRE technique to calibrate LCAQ sensors to detect ambient CO (Carbon Monoxide) levels. RESPIRE offers specific advantages over baseline calibration methods popular in literature, such as improved prediction in cross-site, cross-season, and cross-sensor settings. RESPIRE offers a training algorithm that is provably resistant to outliers and an explainable model with the ability to flag instances of model overfitting. Empirical results are presented based on data collected during an extensive deployment spanning four sites, two seasons and six sensor packages. RESPIRE code is available at https://github.com/purushottamkar/respire.

</details>


### [49] [Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models](https://arxiv.org/abs/2511.19822)
*Wentao Hu,Mingkuan Zhao,Shuangyong Song,Xiaoyan Zhu,Xin Lai,Jiayin Wang*

Main category: cs.LG

TL;DR: MoP是一种新的稀疏专家混合模型剪枝方法，通过"聚类-选择"过程构建功能全面的专家集合，解决了传统剪枝方法在跨领域应用时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有后训练剪枝方法基于单一语料库制定剪枝标准，导致剪枝模型在其他领域应用时出现灾难性性能下降，需要为每个新领域重新剪枝，成本高昂。

Method: 提出马赛克剪枝(MoP)，使用跨任务领域的专家相似性度量进行功能聚类，然后基于激活变异性得分从每个簇中选择最具代表性的专家，确保剪枝模型保留功能互补的专家集合。

Result: 在各种MoE模型上的实验表明，MoP显著优于先前工作，在通用任务上获得7.24%的性能提升，在数学推理和代码生成等专业任务上获得8.92%的提升。

Conclusion: MoP能够构建功能全面的专家集合，使剪枝模型能够处理多样化的下游任务，解决了稀疏专家混合模型跨领域部署的泛化问题。

Abstract: Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select" process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model's capabilities, enabling it to handle diverse downstream tasks.Extensive experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\% gain on general tasks and 8.92\% on specialized tasks like math reasoning and code generation.

</details>


### [50] [GED-Consistent Disentanglement of Aligned and Unaligned Substructures for Graph Similarity Learning](https://arxiv.org/abs/2511.19837)
*Zhentao Zhan,Xiaoliang Xu,Jingjing Wang,Junmei Wang*

Main category: cs.LG

TL;DR: 提出了GCGSim框架，通过图级匹配和子结构级编辑成本来解决现有GNN方法在图相似性计算中的局限性，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的图编辑距离计算方法采用节点中心匹配范式，与GED的核心原则存在不匹配，导致无法捕捉全局结构对应关系和错误归因编辑成本。

Method: 提出GCGSim框架，基于图级匹配和子结构级编辑成本，包含三个核心技术贡献，学习解耦且语义有意义的子结构表示。

Result: 在四个基准数据集上的广泛实验表明，GCGSim实现了最先进的性能。

Conclusion: 该框架有效解决了现有方法的局限性，能够学习到解耦且语义有意义的子结构表示，为图相似性计算提供了更准确的解决方案。

Abstract: Graph Similarity Computation (GSC) is a fundamental graph related task where Graph Edit Distance (GED) serves as a prevalent metric. GED is determined by an optimal alignment between a pair of graphs that partitions each into aligned (zero-cost) and unaligned (cost-incurring) substructures. Due to NP-hard nature of exact GED computation, GED approximations based on Graph Neural Network(GNN) have emerged. Existing GNN-based GED approaches typically learn node embeddings for each graph and then aggregate pairwise node similarities to estimate the final similarity. Despite their effectiveness, we identify a mismatch between this prevalent node-centric matching paradigm and the core principles of GED. This discrepancy leads to two critical limitations: (1) a failure to capture the global structural correspondence for optimal alignment, and (2) a misattribution of edit costs driven by spurious node level signals. To address these limitations, we propose GCGSim, a GED-consistent graph similarity learning framework centering on graph-level matching and substructure-level edit costs. Specifically, we make three core technical contributions. Extensive experiments on four benchmark datasets show that GCGSim achieves state-of-the-art performance. Our comprehensive analyses further validate that the framework effectively learns disentangled and semantically meaningful substructure representations.

</details>


### [51] [Cisco Time Series Model Technical Report](https://arxiv.org/abs/2511.19841)
*Liang Gou,Archit Khare,Praneet Pabolu,Prachi Patel,Joseph Ross,Hercy Shen,Yuhan,Song,Jingze Sun,Kristal Curtis,Vedant Dharnidharka,Abhinav Mathur,Hao Yang*

Main category: cs.LG

TL;DR: Cisco Time Series Model是一个单变量零样本预测器，通过多分辨率输入架构创新改进TimesFM模型，在300B+数据点上训练，在可观测性数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理多分辨率输入的时间序列基础模型，提升在可观测性领域的预测性能，同时保持通用预测基准上的竞争力。

Method: 对流行的仅解码器时间序列模型(TimesFM)进行多分辨率输入架构创新，构建多分辨率仅解码器模型，在300B+独特数据点上训练，其中一半以上来自可观测性领域。

Result: 模型在可观测性数据集上实现卓越性能，在标准通用预测基准(GIFT-Eval)上保持相似性能，多分辨率结构使模型在长上下文输入上预测更准确。

Conclusion: 多分辨率架构创新成功提升了时间序列基础模型在特定领域的性能，同时保持了通用性，证明了该方法的有效性。

Abstract: We introduce the Cisco Time Series Model, a univariate zero-shot forecaster. This time series foundation model is the result of a general architectural innovation to a time series model enabling it to accept multiresolution input, applied to a popular decoder-only time series model (TimesFM). The resulting multiresolution decoder-only model is trained on over 300B unique data points, with more than half coming from the observability domain. Quantitative and qualitative evaluations demonstrate that the resulting model achieves superior performance on observability datasets while retaining very similar performance on a standard general-purpose forecasting benchmark (GIFT-Eval), and suggest that the multiresolution structure enables the model to make more accurate predictions on long context input.

</details>


### [52] [SX-GeoTree: Self-eXplaining Geospatial Regression Tree Incorporating the Spatial Similarity of Feature Attributions](https://arxiv.org/abs/2511.19845)
*Chaogui Kang,Lijian Luo,Qingfeng Guan,Yu Liu*

Main category: cs.LG

TL;DR: SX-GeoTree是一种自解释地理空间回归树，通过结合空间残差控制和解释鲁棒性，在保持预测精度的同时提高空间残差均匀性和解释稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统决策树在表格预测中存在两个问题：(i)难以捕捉空间依赖性，(ii)无法产生局部稳定的解释。

Method: 在递归分裂过程中集成三个耦合目标：杂质减少(MSE)、空间残差控制(全局Moran's I)、通过模块度最大化在共识相似性网络上实现解释鲁棒性。

Result: 在县级GDP和房价预测任务中，SX-GeoTree保持竞争性预测精度，同时改善空间残差均匀性，并将归因共识提高一倍。

Conclusion: 该框架展示了如何将空间相似性嵌入可解释模型，推进可信地理空间机器学习，并为领域感知可解释性提供可转移模板。

Abstract: Decision trees remain central for tabular prediction but struggle with (i) capturing spatial dependence and (ii) producing locally stable (robust) explanations. We present SX-GeoTree, a self-explaining geospatial regression tree that integrates three coupled objectives during recursive splitting: impurity reduction (MSE), spatial residual control (global Moran's I), and explanation robustness via modularity maximization on a consensus similarity network formed from (a) geographically weighted regression (GWR) coefficient distances (stimulus-response similarity) and (b) SHAP attribution distances (explanatory similarity). We recast local Lipschitz continuity of feature attributions as a network community preservation problem, enabling scalable enforcement of spatially coherent explanations without per-sample neighborhood searches. Experiments on two exemplar tasks (county-level GDP in Fujian, n=83; point-wise housing prices in Seattle, n=21,613) show SX-GeoTree maintains competitive predictive accuracy (within 0.01 $R^{2}$ of decision trees) while improving residual spatial evenness and doubling attribution consensus (modularity: Fujian 0.19 vs 0.09; Seattle 0.10 vs 0.05). Ablation confirms Moran's I and modularity terms are complementary; removing either degrades both spatial residual structure and explanation stability. The framework demonstrates how spatial similarity - extended beyond geometric proximity through GWR-derived local relationships - can be embedded in interpretable models, advancing trustworthy geospatial machine learning and offering a transferable template for domain-aware explainability.

</details>


### [53] [Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization](https://arxiv.org/abs/2511.19851)
*Kun Guo,Xuefei Li,Xijun Wang,Howard H. Yang,Wei Feng,Tony Q. S. Quek*

Main category: cs.LG

TL;DR: 本文提出了一种加速混合分割与联邦学习(HSFL)的方法，通过联合优化学习模式选择、批大小以及通信和计算资源来减少整体学习延迟。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)支持低延迟并行训练但可能收敛到较不准确的模型，而分割学习(SL)通过顺序训练实现更高精度但延迟增加。HSFL结合了两者的优势，但需要解决学习模式选择、批大小优化和资源分配问题来加速训练。

Method: 首先分析收敛性，揭示学习模式和批大小之间的相互作用；然后制定延迟最小化问题，提出两阶段解决方案：使用块坐标下降法求解松弛问题获得局部最优解，再通过舍入算法恢复整数批大小以获得接近最优性能。

Result: 实验结果表明，与现有方法相比，该方法在达到目标精度方面显著加速了收敛过程。

Conclusion: 通过联合优化学习模式、批大小和资源分配，可以有效加速HSFL的训练过程，实现更快的收敛速度。

Abstract: Federated learning (FL) and split learning (SL) are two effective distributed learning paradigms in wireless networks, enabling collaborative model training across mobile devices without sharing raw data. While FL supports low-latency parallel training, it may converge to less accurate model. In contrast, SL achieves higher accuracy through sequential training but suffers from increased delay. To leverage the advantages of both, hybrid split and federated learning (HSFL) allows some devices to operate in FL mode and others in SL mode. This paper aims to accelerate HSFL by addressing three key questions: 1) How does learning mode selection affect overall learning performance? 2) How does it interact with batch size? 3) How can these hyperparameters be jointly optimized alongside communication and computational resources to reduce overall learning delay? We first analyze convergence, revealing the interplay between learning mode and batch size. Next, we formulate a delay minimization problem and propose a two-stage solution: a block coordinate descent method for a relaxed problem to obtain a locally optimal solution, followed by a rounding algorithm to recover integer batch sizes with near-optimal performance. Experimental results demonstrate that our approach significantly accelerates convergence to the target accuracy compared to existing methods.

</details>


### [54] [Frailty-Aware Transformer for Recurrent Survival Modeling of Driver Retention in Ride-Hailing Platforms](https://arxiv.org/abs/2511.19893)
*Shuoyan Xu,Yu Zhang,Eric J. Miller*

Main category: cs.LG

TL;DR: 提出了一个基于Transformer的生存分析框架FACT，用于建模网约车司机的空闲行为，在时间依赖性C指数和Brier Score方面优于传统和深度学习生存模型。


<details>
  <summary>Details</summary>
Motivation: 网约车平台具有高频、行为驱动的特点，虽然生存分析已在其他领域的重复事件中应用，但在建模网约车司机行为方面仍未被充分探索。

Method: 使用大规模平台数据将空闲行为建模为重复生存过程，提出基于Transformer的框架，通过因果掩码捕获长期时间依赖性，并整合司机特定嵌入来建模潜在异质性。

Result: 在多伦多网约车数据上的结果显示，提出的FACT模型在时间依赖性C指数和Brier Score方面表现最佳，优于经典和深度学习生存模型。

Conclusion: 该方法能够实现更准确的风险估计，支持平台留存策略，并提供政策相关的见解。

Abstract: Ride-hailing platforms are characterized by high-frequency, behavior-driven environments. Although survival analysis has been applied to recurrent events in other domains, its use in modeling ride-hailing driver behavior remains largely unexplored. This study formulates idle behavior as a recurrent survival process using large-scale platform data and proposes a Transformer-based framework that captures long-term temporal dependencies with causal masking and incorporates driver-specific embeddings to model latent heterogeneity. Results on Toronto ride-hailing data demonstrate that the proposed Frailty-Aware Cox Transformer (FACT) achieves the highest time-dependent C-indices and lowest Brier Scores, outperforming classical and deep learning survival models. This approach enables more accurate risk estimation, supports platform retention strategies, and provides policy-relevant insights.

</details>


### [55] [EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning](https://arxiv.org/abs/2511.19935)
*Songlin Zhao,Michael Pitts,Zhuwei Qin*

Main category: cs.LG

TL;DR: EfficientXpert是一个轻量级领域剪枝框架，结合传播感知剪枝准则和高效适配器更新算法，能在LoRA微调过程中一步将通用预训练模型转换为稀疏的领域专家模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域的需求增长，但其大尺寸在资源受限环境中部署困难，现有压缩方法要么跨领域泛化能力差，要么开销高。

Method: 提出EfficientXpert框架，包含传播感知剪枝准则（Foresight Mask）和高效适配器更新算法（Partial Brain Surgeon），集成到LoRA微调过程中。

Result: 在医疗和法律任务中，在40%稀疏度下保持高达98%的稠密模型性能，优于最先进方法。分析显示领域依赖的结构变化会降低通用剪枝掩码的效果。

Conclusion: 需要针对每个领域量身定制的自适应、领域感知剪枝策略。

Abstract: The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.

</details>


### [56] [Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning](https://arxiv.org/abs/2511.19941)
*Shenjun Zhong,Zhifeng Chen,Zhaolin Chen*

Main category: cs.LG

TL;DR: 使用强化学习优化磁共振指纹识别中的翻转角调度，提高指纹可区分性并可能减少重复时间


<details>
  <summary>Details</summary>
Motivation: 磁共振指纹识别依赖于可调采集参数产生的瞬态信号动态，而翻转角等关键参数的优化是一个复杂的高维序列决策问题，需要自动化方法来设计最优序列

Method: 引入强化学习框架来优化MRF中的翻转角调度，通过RL算法自动选择参数以最大化参数空间中指纹的可区分性

Result: 学习到的调度表现出非周期性模式，增强了指纹的可分离性，并且可能减少重复时间，从而加速MRF采集

Conclusion: 强化学习为MRF序列优化提供了有效方法，能够生成非周期性翻转角调度，提高指纹区分度并可能缩短采集时间

Abstract: Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.

</details>


### [57] [Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning](https://arxiv.org/abs/2511.19942)
*Jingchu Gai,Guanning Zeng,Huaqing Zhang,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 本文提出了一种名为"差分平滑"的理论方法，通过形式化证明解释了RL微调导致多样性崩溃的原因，并证明了该方法在提升正确性和多样性方面优于传统RL和基于熵的启发式方法。


<details>
  <summary>Details</summary>
Motivation: RL微调大语言模型会导致多样性崩溃，现有启发式方法存在随意性，经常在正确性和多样性之间进行权衡，效果不稳定且有时相互矛盾。

Method: 提出差分平滑方法，基于分析发现只需在正确轨迹上应用奖励修改来解决多样性问题，该方法理论证明能同时提升正确性和多样性。

Result: 在1B到7B参数的模型上进行的广泛实验显示，差分平滑在CountDown和真实世界数学推理等任务中一致优于传统方法，在AIME24数据集上Pass@1和Pass@k提升高达6.7%。

Conclusion: 差分平滑是解决RL微调多样性崩溃问题的理论基础方法，优于现有启发式方法，为理解现有方法何时有效及为何失败提供了精确理论分析。

Abstract: It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\% improvements on AIME24 dataset.

</details>


### [58] [Hierarchical Spatio-Temporal Attention Network with Adaptive Risk-Aware Decision for Forward Collision Warning in Complex Scenarios](https://arxiv.org/abs/2511.19952)
*Haoran Hu,Junren Shi,Shuo Jiang,Kun Cheng,Xia Yang,Changhao Piao*

Main category: cs.LG

TL;DR: 提出了一个集成的前向碰撞预警框架，结合分层时空注意力网络和动态风险阈值调整算法，解决了计算复杂度和建模不足的双重挑战，在保持高精度的同时实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 当前前向碰撞预警系统存在计算成本高、交互模型简化导致不可靠、传统静态阈值警告误报率高的问题，需要平衡多智能体交互建模精度与实时决策适应性。

Method: 使用分层时空注意力网络（HSTAN）进行解耦架构设计：图注意力网络处理空间关系，级联GRU与自注意力处理时序关系；结合保形分位数回归生成预测区间，动态风险阈值调整算法通过物理信息风险势函数和自适应阈值机制转换预测为及时警告。

Result: 在NGSIM数据集上，HSTAN仅需12.3ms推理时间（比Transformer方法快73%），平均位移误差降至0.73m（比Social_LSTM提升42.2%）；完整系统F1分数达0.912，误报率8.2%，预警提前时间2.8秒，预测区间覆盖率达91.3%（90%置信度）。

Conclusion: 该框架在复杂环境中展现出卓越性能和实际部署可行性，有效平衡了精度与效率，为车辆安全和自动驾驶提供了可靠的前向碰撞预警解决方案。

Abstract: Forward Collision Warning systems are crucial for vehicle safety and autonomous driving, yet current methods often fail to balance precise multi-agent interaction modeling with real-time decision adaptability, evidenced by the high computational cost for edge deployment and the unreliability stemming from simplified interaction models.To overcome these dual challenges-computational complexity and modeling insufficiency-along with the high false alarm rates of traditional static-threshold warnings, this paper introduces an integrated FCW framework that pairs a Hierarchical Spatio-Temporal Attention Network with a Dynamic Risk Threshold Adjustment algorithm. HSTAN employs a decoupled architecture (Graph Attention Network for spatial, cascaded GRU with self-attention for temporal) to achieve superior performance and efficiency, requiring only 12.3 ms inference time (73% faster than Transformer methods) and reducing the Average Displacement Error (ADE) to 0.73m (42.2% better than Social_LSTM) on the NGSIM dataset. Furthermore, Conformalized Quantile Regression enhances reliability by generating prediction intervals (91.3% coverage at 90% confidence), which the DTRA module then converts into timely warnings via a physics-informed risk potential function and an adaptive threshold mechanism inspired by statistical process control.Tested across multi-scenario datasets, the complete system demonstrates high efficacy, achieving an F1 score of 0.912, a low false alarm rate of 8.2%, and an ample warning lead time of 2.8 seconds, validating the framework's superior performance and practical deployment feasibility in complex environments.

</details>


### [59] [Prompt Fairness: Sub-group Disparities in LLMs](https://arxiv.org/abs/2511.19956)
*Meiyu Zhong,Noel Teku,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文研究了LLM中的提示公平性问题，发现不同用户/风格的提示词即使询问相同问题，也会引发LLM不同的响应质量。作者提出信息论指标来量化这种偏差，并通过多数投票和提示中性化等干预措施来改善公平性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在不同用户的提示词下会产生不同的响应质量，这种提示公平性问题可能导致结构性不平等，需要量化和缓解。

Method: 使用信息论指标（子组敏感性和跨组一致性）来量化偏差，提出多数投票和提示中性化等实际干预措施来减少差异。

Result: 实验显示，在缓解前跨组分歧值达到0.28，通常在0.14-0.22范围内；应用中性化和多代策略后，分歧值持续下降，最大差距降至0.22，许多距离降至0.17或以下。

Conclusion: 提出的干预措施有效提高了LLM在不同用户群体间的响应稳定性和公平性，缓解了结构性不平等问题。

Abstract: Large Language Models (LLMs), though shown to be effective in many applications, can vary significantly in their response quality. In this paper, we investigate this problem of prompt fairness: specifically, the phrasing of a prompt by different users/styles, despite the same question being asked in principle, may elicit different responses from an LLM. To quantify this disparity, we propose to use information-theoretic metrics that can capture two dimensions of bias: subgroup sensitivity, the variability of responses within a subgroup and cross group consistency, the variability of responses across subgroups. Our analysis reveals that certain subgroups exhibit both higher internal variability and greater divergence from others. Our empirical analysis reveals that certain demographic sub groups experience both higher internal variability and greater divergence from others, indicating structural inequities in model behavior. To mitigate these disparities, we propose practical interventions, including majority voting across multiple generations and prompt neutralization, which together improve response stability and enhance fairness across user populations. In the experiments, we observe clear prompt sensitivity disparities across demographic subgroups: before mitigation, cross-group divergence values reach 0.28 and typically fall in the from 0.14 to 0.22 range. After applying our neutralization and multi generation strategy, these divergences consistently decrease, with the largest gap reduced to 0.22 and many distances falling to 0.17 or below, indicating more stable and consistent outputs across subgroups.

</details>


### [60] [ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models](https://arxiv.org/abs/2511.19959)
*Yujia Wang,Yuanpu Cao,Jinghui Chen*

Main category: cs.LG

TL;DR: ParaBlock是一种用于联邦学习大语言模型的高效通信方法，通过并行通信和计算线程提升效率，同时保持与标准联邦块坐标下降方法相同的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习大语言模型时代，即使单个模型块也包含大量参数，导致通信延迟显著，特别是对于资源受限的客户端。

Method: 提出ParaBlock方法，建立通信和计算两个并行线程来提升通信效率。

Result: 在大语言模型通用指令跟随和数学推理微调任务上的实证评估表明，ParaBlock不仅保持强大性能，还显著提升通信效率。

Conclusion: ParaBlock成功解决了联邦学习大语言模型训练中的通信效率问题，同时保持收敛性能。

Abstract: Federated learning (FL) has been extensively studied as a privacy-preserving training paradigm. Recently, federated block coordinate descent scheme has become a popular option in training large-scale models, as it allows clients to train only a subset of the model locally instead of the entire model. However, in the era of large language models (LLMs), even a single block can contain a significant number of parameters, posing substantial communication latency, particularly for resource-constrained clients. To address this challenge in federated training/fine-tuning LLMs, we propose ParaBlock, a novel approach that establishes two parallel threads for communication and computation to enhance communication efficiency. We theoretically prove that the proposed ParaBlock achieves the same convergence rate as the standard federated block coordinate descent methods. Empirical evaluations on fine-tuning LLMs on general instruction following and mathematical reasoning confirm that ParaBlock not only maintains strong performance but also significantly improves communication efficiency.

</details>


### [61] [Stragglers Can Contribute More: Uncertainty-Aware Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.19966)
*Yujia Wang,Fenglong Ma,Jinghui Chen*

Main category: cs.LG

TL;DR: FedEcho是一个新颖的异步联邦学习框架，通过不确定性感知蒸馏来缓解过时更新和数据异质性带来的挑战，在保持隐私的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习虽然提高了效率和可扩展性，但面临过时更新和快速客户端主导学习过程的偏见问题，现有方法通常只能解决其中一个问题，造成冲突。

Method: 采用不确定性感知蒸馏技术，服务器评估滞后客户端预测的可靠性，根据估计的不确定性动态调整这些预测的影响权重。

Result: 实验表明FedEcho在异步延迟大和数据异质性强的场景下，持续优于现有的异步联邦学习基线方法。

Conclusion: FedEcho通过不确定性感知蒸馏有效缓解了异步联邦学习中的过时更新和数据异质性挑战，实现了鲁棒性能且无需访问私有客户端数据。

Abstract: Asynchronous federated learning (FL) has recently gained attention for its enhanced efficiency and scalability, enabling local clients to send model updates to the server at their own pace without waiting for slower participants. However, such a design encounters significant challenges, such as the risk of outdated updates from straggler clients degrading the overall model performance and the potential bias introduced by faster clients dominating the learning process, especially under heterogeneous data distributions. Existing methods typically address only one of these issues, creating a conflict where mitigating the impact of outdated updates can exacerbate the bias created by faster clients, and vice versa. To address these challenges, we propose FedEcho, a novel framework that incorporates uncertainty-aware distillation to enhance the asynchronous FL performances under large asynchronous delays and data heterogeneity. Specifically, uncertainty-aware distillation enables the server to assess the reliability of predictions made by straggler clients, dynamically adjusting the influence of these predictions based on their estimated uncertainty. By prioritizing more certain predictions while still leveraging the diverse information from all clients, FedEcho effectively mitigates the negative impacts of outdated updates and data heterogeneity. Through extensive experiments, we demonstrate that FedEcho consistently outperforms existing asynchronous federated learning baselines, achieving robust performance without requiring access to private client data.

</details>


### [62] [Rethinking Semi-Supervised Node Classification with Self-Supervised Graph Clustering](https://arxiv.org/abs/2511.19976)
*Songbo Wang,Renchi Yang,Yurui Lai,Xiaoyang Lin,Tsz Nam Chan*

Main category: cs.LG

TL;DR: NCGC框架将自监督图聚类与半监督分类结合，通过软正交GNN和聚类模块的协同训练，在七个真实图上显著提升了节点分类性能。


<details>
  <summary>Details</summary>
Motivation: 现实图中节点常形成紧密社区，这些社区蕴含丰富信号可弥补半监督节点分类中的标签稀缺问题，但现有方法未充分利用这一特性。

Method: 提出NCGC框架：1）理论统一GNN和谱图聚类的优化目标，开发软正交GNN；2）包含自监督图聚类模块，使用两个聚类目标和Sinkhorn-Knopp归一化；3）通过多任务目标结合分类和聚类损失。

Result: 在七个真实图上，NCGC框架使用不同经典GNN骨干网络时，始终显著优于流行的GNN模型和近期基线方法。

Conclusion: NCGC通过整合自监督聚类和半监督分类，充分利用图结构中的社区信息，有效提升了节点分类性能，证明了该框架的有效性和通用性。

Abstract: The emergence of graph neural networks (GNNs) has offered a powerful tool for semi-supervised node classification tasks. Subsequent studies have achieved further improvements through refining the message passing schemes in GNN models or exploiting various data augmentation techniques to mitigate limited supervision. In real graphs, nodes often tend to form tightly-knit communities/clusters, which embody abundant signals for compensating label scarcity in semi-supervised node classification but are not explored in prior methods.
  Inspired by this, this paper presents NCGC that integrates self-supervised graph clustering and semi-supervised classification into a unified framework. Firstly, we theoretically unify the optimization objectives of GNNs and spectral graph clustering, and based on that, develop soft orthogonal GNNs (SOGNs) that leverage a refined message passing paradigm to generate node representations for both classification and clustering. On top of that, NCGC includes a self-supervised graph clustering module that enables the training of SOGNs for learning representations of unlabeled nodes in a self-supervised manner. Particularly, this component comprises two non-trivial clustering objectives and a Sinkhorn-Knopp normalization that transforms predicted cluster assignments into balanced soft pseudo-labels. Through combining the foregoing clustering module with the classification model using a multi-task objective containing the supervised classification loss on labeled data and self-supervised clustering loss on unlabeled data, NCGC promotes synergy between them and achieves enhanced model capacity. Our extensive experiments showcase that the proposed NCGC framework consistently and considerably outperforms popular GNN models and recent baselines for semi-supervised node classification on seven real graphs, when working with various classic GNN backbones.

</details>


### [63] [Operator Learning at Machine Precision](https://arxiv.org/abs/2511.19980)
*Aras Bacho,Aleksei G. Sorokin,Xianjin Yang,Théo Bourdais,Edoardo Calvello,Matthieu Darcy,Alexander Hsu,Bamdad Hosseini,Houman Owhadi*

Main category: cs.LG

TL;DR: CHONKNORIS是一种基于牛顿-康托罗维奇方法的神经算子学习范式，通过回归Tikhonov正则化牛顿更新的Cholesky因子而非直接逼近解算子，能够达到机器精度。该方法在多种非线性前向和逆PDE问题上表现优异，并提出了聚合多个预训练专家的FONKNORIS变体。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子学习方法在增加复杂度时往往无法显著提高精度，与更简单的核方法和降阶模型相当。需要开发能够达到机器精度的算子学习方法。

Method: 基于数值分析中的牛顿型方法，回归Tikhonov正则化牛顿-康托罗维奇更新的Cholesky因子，通过展开迭代构建神经架构。该方法只需要达到压缩映射的较低精度要求，而非端到端逼近解算子。

Result: 在非线性椭圆方程、Burgers方程、非线性Darcy流问题、Calderón问题、逆波散射问题和地震成像问题等基准测试中表现优异。FONKNORIS变体能够准确求解未见过的非线性PDE，如Klein-Gordon和Sine-Gordon方程。

Conclusion: CHONKNORIS通过结合数值分析和神经网络，提供了一种能够达到机器精度的算子学习新范式，在多种PDE问题上展现出优越性能，并为构建基础模型提供了可行路径。

Abstract: Neural operator learning methods have garnered significant attention in scientific computing for their ability to approximate infinite-dimensional operators. However, increasing their complexity often fails to substantially improve their accuracy, leaving them on par with much simpler approaches such as kernel methods and more traditional reduced-order models. In this article, we set out to address this shortcoming and introduce CHONKNORIS (Cholesky Newton--Kantorovich Neural Operator Residual Iterative System), an operator learning paradigm that can achieve machine precision. CHONKNORIS draws on numerical analysis: many nonlinear forward and inverse PDE problems are solvable by Newton-type methods. Rather than regressing the solution operator itself, our method regresses the Cholesky factors of the elliptic operator associated with Tikhonov-regularized Newton--Kantorovich updates. The resulting unrolled iteration yields a neural architecture whose machine-precision behavior follows from achieving a contractive map, requiring far lower accuracy than end-to-end approximation of the solution operator. We benchmark CHONKNORIS on a range of nonlinear forward and inverse problems, including a nonlinear elliptic equation, Burgers' equation, a nonlinear Darcy flow problem, the Calderón problem, an inverse wave scattering problem, and a problem from seismic imaging. We also present theoretical guarantees for the convergence of CHONKNORIS in terms of the accuracy of the emulated Cholesky factors. Additionally, we introduce a foundation model variant, FONKNORIS (Foundation Newton--Kantorovich Neural Operator Residual Iterative System), which aggregates multiple pre-trained CHONKNORIS experts for diverse PDEs to emulate the solution map of a novel nonlinear PDE. Our FONKNORIS model is able to accurately solve unseen nonlinear PDEs such as the Klein--Gordon and Sine--Gordon equations.

</details>


### [64] [Rethinking Message Passing Neural Networks with Diffusion Distance-guided Stress Majorization](https://arxiv.org/abs/2511.19984)
*Haoran Zheng,Renchi Yang,Yubo Zhou,Jianliang Xu*

Main category: cs.LG

TL;DR: 提出DDSM模型，一种基于应力优化和正交正则化的消息传递神经网络，通过引入扩散距离来解决过平滑和过相关问题，在多种图上表现优异


<details>
  <summary>Details</summary>
Motivation: 传统MPNN模型由于最小化Dirichlet能量和邻域聚合操作，存在严重的过平滑和过相关问题

Method: 基于应力优化和正交正则化的优化框架，引入节点扩散距离指导消息传递操作，并开发高效的距离近似算法

Result: 在15个强基线模型上，DDSM在同质性和异质性图上均取得显著且一致的性能提升

Conclusion: DDSM通过新的优化框架有效解决了MPNN的过平滑和过相关问题，在多种图结构数据上表现出色

Abstract: Message passing neural networks (MPNNs) have emerged as go-to models for learning on graph-structured data in the past decade. Despite their effectiveness, most of such models still incur severe issues such as over-smoothing and -correlation, due to their underlying objective of minimizing the Dirichlet energy and the derived neighborhood aggregation operations. In this paper, we propose the DDSM, a new MPNN model built on an optimization framework that includes the stress majorization and orthogonal regularization for overcoming the above issues. Further, we introduce the diffusion distances for nodes into the framework to guide the new message passing operations and develop efficient algorithms for distance approximations, both backed by rigorous theoretical analyses. Our comprehensive experiments showcase that DDSM consistently and considerably outperforms 15 strong baselines on both homophilic and heterophilic graphs.

</details>


### [65] [On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices](https://arxiv.org/abs/2511.19986)
*Lianming Huang,Haibo Hu,Qiao Li,Nan Guan,Chun Jason Xue*

Main category: cs.LG

TL;DR: 提出一种按需多任务稀疏框架，通过最大化参数重用和块粒度分解来最小化任务切换开销


<details>
  <summary>Details</summary>
Motivation: 现有稀疏方法在单独优化每个任务时忽略了频繁任务切换带来的显著I/O开销问题

Method: 将权重分解为可重用的块粒度单元，跨任务对齐稀疏结构以最大化重叠，动态加载下一个任务所需的少量差异块

Result: 在真实自动驾驶平台上，相比现有稀疏方法平均加速任务切换超过6.6倍

Conclusion: 该框架通过参数重用和动态块加载有效缓解了传统整体方法的冷启动延迟，实现了优越的切换效率

Abstract: Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.

</details>


### [66] [RankOOD - Class Ranking-based Out-of-Distribution Detection](https://arxiv.org/abs/2511.19996)
*Dishanika Denipitiyage,Naveen Karunanayake,Suranga Seneviratne,Sanjay Chawla*

Main category: cs.LG

TL;DR: RankOOD是一种基于排序的OOD检测方法，使用Plackett-Luce损失训练模型，在TinyImageNet基准上实现SOTA性能，将FPR95降低4.3%。


<details>
  <summary>Details</summary>
Motivation: 基于交叉熵损失训练的深度学习模型在ID类预测中会产生排序模式，OOD样本可能被分配高概率到ID类别，但不太可能遵循排序分类。

Method: 首先使用初始分类器为每个类别提取排序列表，然后使用Plackett-Luce损失进行第二轮训练，其中类别排序是预测变量。

Result: 在近OOD TinyImageNet评估基准上达到最先进性能，FPR95降低4.3%。

Conclusion: RankOOD框架通过利用ID类预测中的排序模式，有效提升了OOD检测性能。

Abstract: We propose RankOOD, a rank-based Out-of-Distribution (OOD) detection approach based on training a model with the Placket-Luce loss, which is now extensively used for preference alignment tasks in foundational models. Our approach is based on the insight that with a deep learning model trained using the Cross Entropy Loss, in-distribution (ID) class prediction induces a ranking pattern for each ID class prediction. The RankOOD framework formalizes the insight by first extracting a rank list for each class using an initial classifier and then uses another round of training with the Plackett-Luce loss, where the class rank, a fixed permutation for each class, is the predicted variable. An OOD example may get assigned with high probability to an ID example, but the probability of it respecting the ranking classification is likely to be small. RankOOD, achieves SOTA performance on the near-ODD TinyImageNet evaluation benchmark, reducing FPR95 by 4.3%.

</details>


### [67] [REWA: Witness-Overlap Theory -- Foundations for Composable Binary Similarity Systems](https://arxiv.org/abs/2511.19998)
*Nikit Phadke*

Main category: cs.LG

TL;DR: REWA提出了一种基于见证集重叠结构的相似性通用理论，可将各种相似性定义（图邻域、因果关系、拓扑特征等）简化为紧凑编码，并保持排名保留保证。


<details>
  <summary>Details</summary>
Motivation: 统一和简化各种相似性度量方法，提供模块化构建相似性系统的理论基础，避免复杂的哈希函数工程。

Method: 使用有限见证集、半随机比特分配和重叠单调性，通过重叠间隙条件保证top-k排名保留，仅需对数级比特数。

Result: 证明了在见证集重叠条件下，相似性排名可被紧凑编码保留，统一了Bloom过滤器、minhash、LSH等多种方法。

Conclusion: REWA为基于见证重叠的相似性系统提供了原则性基础，支持模块化设计和组合构建，扩展了相似性系统的设计空间。

Abstract: REWA introduces a general theory of similarity based on witness-overlap structures. We show that whenever similarity between concepts can be expressed as monotone witness overlap -- whether arising from graph neighborhoods, causal relations, temporal structure, topological features, symbolic patterns, or embedding-based neighborhoods -- it admits a reduction to compact encodings with provable ranking preservation guarantees. REWA systems consist of: (1) finite witness sets $W(v)$, (2) semi-random bit assignments generated from each witness, and (3) monotonicity of expected similarity in the overlap $Δ(u, v) = |W(u) \cap W(v)|$. We prove that under an overlap-gap condition on the final witness sets -- independent of how they were constructed -- top-$k$ rankings are preserved using $m = O(\log(|V|/δ))$ bits. The witness-set formulation is compositional: any sequence of structural, temporal, causal, topological, information-theoretic, or learned transformations can be combined into pipelines that terminate in discrete witness sets. The theory applies to the final witness overlap, enabling modular construction of similarity systems from reusable primitives. This yields a vast design space: millions of composable similarity definitions inherit logarithmic encoding complexity. REWA subsumes and unifies Bloom filters, minhash, LSH bitmaps, random projections, sketches, and hierarchical filters as special cases. It provides a principled foundation for similarity systems whose behavior is governed by witness overlap rather than hash-function engineering. This manuscript presents the axioms, the main reducibility theorem, complete proofs with explicit constants, and a detailed discussion of compositional design, limitations, and future extensions including multi-bit encodings, weighted witnesses, and non-set representations.

</details>


### [68] [Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting](https://arxiv.org/abs/2511.20004)
*Peining Zhang,Hongchen Qin,Haochen Zhang,Ziqi Guo,Guiling Wang,Jinbo Bi*

Main category: cs.LG

TL;DR: 时间序列基础模型在零样本设置下，当输入上下文窗口足够长（覆盖超过1-2个完整季节周期）时，可以超越完全训练的LSTM模型进行叶面积指数预测


<details>
  <summary>Details</summary>
Motivation: 研究时间序列基础模型在农业监测中零样本预测叶面积指数的能力，探索通用基础模型是否能在不进行任务特定调优的情况下超越专门的监督模型

Method: 使用HiQ数据集（美国，2000-2022），系统比较统计基线、全监督LSTM和Sundial基础模型在多种评估协议下的表现

Result: Sundial在零样本设置下，当输入上下文窗口足够长时，能够超越完全训练的LSTM模型

Conclusion: 预训练的时间序列基础模型在农业和环境应用中具有作为即插即用预测器的强大潜力

Abstract: This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.

</details>


### [69] [iRadioDiff: Physics-Informed Diffusion Model for Indoor Radio Map Construction and Localization](https://arxiv.org/abs/2511.20015)
*Xiucheng Wang,Tingwei Yuan,Yang Cao,Nan Cheng,Ruijin Sun,Weihua Zhuang*

Main category: cs.LG

TL;DR: iRadioDiff是一个基于扩散模型的室内无线电地图构建框架，通过物理信息提示和多路径关键先验来生成高保真度的电磁场分布图，在室内定位任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统电磁求解器构建高保真室内无线电地图延迟过高，而基于学习的方法依赖稀疏测量或假设材料均匀，无法准确建模室内环境的异质性和多路径特性。

Method: 提出采样自由的扩散框架iRadioDiff，通过接入点位置、材料反射/透射系数的物理信息提示，以及衍射点、强透射边界、视距轮廓等多路径关键先验来引导生成过程。

Result: 实验表明iRadioDiff在室内无线电地图构建和基于接收信号强度的室内定位任务中达到最先进性能，并在不同布局和材料配置下具有良好的泛化能力。

Conclusion: iRadioDiff能够准确建模非平稳场不连续性，高效构建物理一致的无线电地图，为室内定位提供了有效的解决方案。

Abstract: Radio maps (RMs) serve as environment-aware electromagnetic (EM) representations that connect scenario geometry and material properties to the spatial distribution of signal strength, enabling localization without costly in-situ measurements. However, constructing high-fidelity indoor RMs remains challenging due to the prohibitive latency of EM solvers and the limitations of learning-based methods, which often rely on sparse measurements or assumptions of homogeneous material, which are misaligned with the heterogeneous and multipath-rich nature of indoor environments. To overcome these challenges, we propose iRadioDiff, a sampling-free diffusion-based framework for indoor RM construction. iRadioDiff is conditioned on access point (AP) positions, and physics-informed prompt encoded by material reflection and transmission coefficients. It further incorporates multipath-critical priors, including diffraction points, strong transmission boundaries, and line-of-sight (LoS) contours, to guide the generative process via conditional channels and boundary-weighted objectives. This design enables accurate modeling of nonstationary field discontinuities and efficient construction of physically consistent RMs. Experiments demonstrate that iRadioDiff achieves state-of-the-art performance in indoor RM construction and received signal strength based indoor localization, which offers effective generalization across layouts and material configurations. Code is available at https://github.com/UNIC-Lab/iRadioDiff.

</details>


### [70] [Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering](https://arxiv.org/abs/2511.20030)
*Haoran Zheng,Renchi Yang,Hongtao Wang,Jianliang Xu*

Main category: cs.LG

TL;DR: 提出DGF方案用于多模态属性图聚类，通过双图滤波和三元交叉对比学习解决现有方法在低模态相关性和高特征噪声下的性能问题


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法依赖视图间高相关性，但忽略了多模态属性图中由大型预训练模型输出的低模态相关性和强特征噪声特性，导致聚类性能不佳

Method: 提出双图滤波方案，包含特征级去噪组件和三元交叉对比训练策略，通过跨模态、邻域和社区的实例级对比学习来学习鲁棒节点表示

Result: 在8个基准MMAG数据集上的实验表明，DGF在聚类质量上显著优于多种最先进基线方法

Conclusion: DGF方案能有效克服传统图滤波器的局限性，在多模态属性图聚类任务中取得优越性能

Abstract: Multimodal Attributed Graphs (MMAGs) are an expressive data model for representing the complex interconnections among entities that associate attributes from multiple data modalities (text, images, etc.). Clustering over such data finds numerous practical applications in real scenarios, including social community detection, medical data analytics, etc. However, as revealed by our empirical studies, existing multi-view clustering solutions largely rely on the high correlation between attributes across various views and overlook the unique characteristics (e.g., low modality-wise correlation and intense feature-wise noise) of multimodal attributes output by large pre-trained language and vision models in MMAGs, leading to suboptimal clustering performance.
  Inspired by foregoing empirical observations and our theoretical analyses with graph signal processing, we propose the Dual Graph Filtering (DGF) scheme, which innovatively incorporates a feature-wise denoising component into node representation learning, thereby effectively overcoming the limitations of traditional graph filters adopted in the extant multi-view graph clustering approaches. On top of that, DGF includes a tri-cross contrastive training strategy that employs instance-level contrastive learning across modalities, neighborhoods, and communities for learning robust and discriminative node representations. Our comprehensive experiments on eight benchmark MMAG datasets exhibit that DGF is able to outperform a wide range of state-of-the-art baselines consistently and significantly in terms of clustering quality measured against ground-truth labels.

</details>


### [71] [The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs](https://arxiv.org/abs/2511.20104)
*Craig Dickson*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed "emergent misalignment" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales.
  We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%.
  We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's 'degrees of freedom' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.

</details>


### [72] [RED-F: Reconstruction-Elimination based Dual-stream Contrastive Forecasting for Multivariate Time Series Anomaly Prediction](https://arxiv.org/abs/2511.20044)
*PengYu Chen,Xiaohou Shi,Yuan Chang,Yan Sun,Sajal K. Das*

Main category: cs.LG

TL;DR: 提出了RED-F框架，通过重构消除和双流对比预测来增强微弱异常前兆的检测能力，将绝对信号检测转化为相对轨迹比较的简单任务


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法在仅使用正常数据训练时，倾向于重构正常模式，导致在面对微弱异常前兆时预测结果被正常模式主导，淹没了预测所需的信号

Method: 采用重构消除模型(REM)通过混合时频机制消除前兆生成纯净基线，然后双流对比预测模型(DFM)并行处理纯净基线和保留前兆的原始序列，通过对比两个预测流的差异来放大前兆信号

Result: 在六个真实世界数据集上的广泛实验表明，RED-F在异常预测任务中具有卓越能力

Conclusion: RED-F框架通过重构消除和对比预测机制，有效解决了微弱异常前兆检测的挑战，在多元时间序列异常预测中表现出色

Abstract: The proactive prediction of anomalies (AP) in mul- tivariate time series (MTS) is a critical challenge to ensure system dependability. The difficulty lies in identifying subtle anomaly precursors concealed within normal signals. However, existing unsupervised methods, trained exclusively on normal data, demonstrate a fundamental propensity to reconstruct normal patterns. Consequently, when confronted with weak precursors, their predictions are dominated by the normal pattern, submerging the very signal required for prediction. To contend with the limitation, we propose RED-F, a Reconstruction- Elimination based Dual-stream Contrastive Forecasting frame- work, comprising the Reconstruction-Elimination Model (REM) and the Dual-stream Contrastive Forecasting Model (DFM). The REM utilizes a hybrid time-frequency mechanism to mitigate the precursor, generating a purified, normal-pattern baseline. The DFM then receives this purified baseline and the original sequence which retains the precursor as parallel inputs. At the core of our framework, RED-F employs a contrastive forecast that transforms the difficult task of absolute signal detection into a simpler, more robust task of relative trajectory comparison by computing the divergence between these two predictive streams. This contrastive mechanism serves to amplify the faint precursor signal. Furthermore, the DFM is trained with a novel Multi-Series Prediction (MSP) objective, which leverages distant future con- text to enhance its predictive sensitivity. Extensive experiments on six real-world datasets demonstrate the superior capability of RED-F in anomaly prediction tasks.

</details>


### [73] [IDAP++: Advancing Divergence-Based Pruning via Filter-Level and Layer-Level Optimization](https://arxiv.org/abs/2511.20141)
*Aleksei Samarin,Artem Nazarenko,Egor Kotenko,Valentin Malykh,Alexander Savelev,Aleksei Toropov*

Main category: cs.LG

TL;DR: 提出基于信息流分析的神经网络压缩统一框架，通过张量流散度量化信息转换，实现滤波器级和架构级冗余消除的两阶段优化方法。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在滤波器和架构层面的冗余问题，为不同架构提供统一的压缩度量标准，适应资源受限环境的部署需求。

Method: 两阶段优化：第一阶段使用迭代散度感知剪枝移除冗余滤波器；第二阶段分析层间信息传播贡献，选择性消除对性能影响最小的整个层。

Result: 在多种现代架构和数据集上实现显著模型压缩，同时保持竞争力精度，参数减少效果与最先进方案相当且在卷积模型到变换器等多种架构上表现更优。

Conclusion: 流散度作为滤波器级和层级优化的有效指导原则，为资源受限环境中的部署提供实用价值。

Abstract: This paper presents a novel approach to neural network compression that addresses redundancy at both the filter and architectural levels through a unified framework grounded in information flow analysis. Building on the concept of tensor flow divergence, which quantifies how information is transformed across network layers, we develop a two-stage optimization process. The first stage employs iterative divergence-aware pruning to identify and remove redundant filters while preserving critical information pathways. The second stage extends this principle to higher-level architecture optimization by analyzing layer-wise contributions to information propagation and selectively eliminating entire layers that demonstrate minimal impact on network performance. The proposed method naturally adapts to diverse architectures, including convolutional networks, transformers, and hybrid designs, providing a consistent metric for comparing the structural importance across different layer types. Experimental validation across multiple modern architectures and datasets reveals that this combined approach achieves substantial model compression while maintaining competitive accuracy. The presented approach achieves parameter reduction results that are globally comparable to those of state-of-the-art solutions and outperforms them across a wide range of modern neural network architectures, from convolutional models to transformers. The results demonstrate how flow divergence serves as an effective guiding principle for both filter-level and layer-level optimization, offering practical benefits for deployment in resource-constrained environments.

</details>


### [74] [SOMBRL: Scalable and Optimistic Model-Based RL](https://arxiv.org/abs/2511.20066)
*Bhavya Sukhija,Lenart Treven,Carmelo Sferrazza,Florian Dörfler,Pieter Abbeel,Andreas Krause*

Main category: cs.LG

TL;DR: 提出SOMBRL方法，基于乐观面对不确定性原则，通过不确定性感知的动态模型和贪婪优化奖励与认知不确定性的加权和，实现模型强化学习中的高效探索。


<details>
  <summary>Details</summary>
Motivation: 解决基于模型的强化学习中系统动态未知且必须从在线交互中学习时的有效探索挑战，提供可扩展且理论上有保证的探索方法。

Method: SOMBRL方法学习不确定性感知的动态模型，贪婪地最大化外部奖励与智能体认知不确定性的加权和，兼容任何策略优化器或规划器。

Result: 在状态和视觉控制环境中表现出色，在所有任务和基线方法中均显示强劲性能；在动态RC汽车硬件上超越最先进方法，证明了基于原则的探索对MBRL的益处。

Conclusion: SOMBRL为基于模型的强化学习提供了一种灵活、可扩展且理论上有保证的探索解决方案，在非线性动态的各种设置下具有次线性遗憾保证。

Abstract: We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.

</details>


### [75] [On the Limits of Momentum in Decentralized and Federated Optimization](https://arxiv.org/abs/2511.20168)
*Riccardo Zaccone,Sai Praneeth Karimireddy,Carlo Masone*

Main category: cs.LG

TL;DR: 该论文分析了联邦学习中动量方法在循环客户端参与下的表现，证明动量无法克服统计异质性，且步长衰减快于Θ(1/t)会导致收敛到依赖初始化和异质性边界的常数。


<details>
  <summary>Details</summary>
Motivation: 探索动量在联邦学习分布式SGD中的应用，特别是解决统计异质性问题，但在去中心化场景中动量是否能保证收敛仍不明确。

Method: 理论分析动量在循环客户端参与下的收敛行为，通过数学证明和数值实验验证理论结果。

Result: 动量方法仍然受到统计异质性的影响，步长衰减快于Θ(1/t)会导致收敛到依赖初始化和异质性边界的常数。

Conclusion: 动量无法完全解决联邦学习中的统计异质性问题，与SGD类似，需要更有效的方法来处理客户端参与的不确定性。

Abstract: Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $Θ\left(1/t\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.

</details>


### [76] [QiMeng-CRUX: Narrowing the Gap between Natural Language and Verilog via Core Refined Understanding eXpression](https://arxiv.org/abs/2511.20099)
*Lei Huang,Rui Zhang,Jiaming Guo,Yang Zhang,Di Huang,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 提出了CRUX结构化中间表示空间和两阶段训练框架，在Verilog代码生成任务中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖自由形式的自然语言描述，这些描述往往模糊、冗余且缺乏结构，给下游Verilog代码生成带来挑战

Method: 引入CRUX结构化中间空间，设计包含联合表达式建模和双空间优化的两阶段训练框架CRUX-V

Result: 在多个Verilog生成基准测试中达到最先进性能，特别是在复杂设计任务中表现优异

Conclusion: CRUX空间可有效缩小自由形式自然语言描述与精确Verilog生成之间的差距，且具有可迁移性

Abstract: Large language models (LLMs) have shown promising capabilities in hardware description language (HDL) generation. However, existing approaches often rely on free-form natural language descriptions that are often ambiguous, redundant, and unstructured, which poses significant challenges for downstream Verilog code generation. We treat hardware code generation as a complex transformation from an open-ended natural language space to a domain-specific, highly constrained target space. To bridge this gap, we introduce Core Refined Understanding eXpression (CRUX), a structured intermediate space that captures the essential semantics of user intent while organizing the expression for precise Verilog code generation. We further design a two-stage training framework, comprising Joint Expression Modeling and Dual-Space Optimization, to enhance the quality of both CRUX and Verilog code. Experiments across multiple Verilog generation benchmarks demonstrate that our model, CRUX-V, achieves state-of-the-art performance among general models, particularly under challenging design tasks. Furthermore, the CRUX space proves transferable and beneficial when used as input prompts for other code models, highlighting its effectiveness in narrowing the gap between free-form natural language descriptions and precise Verilog generation.

</details>


### [77] [Leveraging weights signals - Predicting and improving generalizability in reinforcement learning](https://arxiv.org/abs/2511.20234)
*Olivier Moulin,Vincent Francois-lavet,Paul Elbers,Mark Hoogendoorn*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络内部权重预测RL智能体泛化能力的方法，并改进了PPO损失函数来提升智能体的泛化性能


<details>
  <summary>Details</summary>
Motivation: 解决RL智能体在训练环境上过拟合、泛化能力不足的问题

Method: 基于智能体神经网络内部权重预测泛化分数，并改进PPO损失函数

Result: 实验证明改进后的PPO算法能训练出具有更强泛化能力的智能体

Conclusion: 通过预测泛化分数并改进损失函数，可以有效提升RL智能体的泛化能力

Abstract: Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.

</details>


### [78] [Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling](https://arxiv.org/abs/2511.20257)
*Zhiguo Zhang,Xiaoliang Ma,Daniel Schlesinger*

Main category: cs.LG

TL;DR: 提出了一种物理引导、可解释的时空学习框架，用于空气污染预测，在性能和可解释性之间取得平衡，并在斯德哥尔摩地区数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确的空气污染预测对公共健康至关重要，但大多数模型在性能和可解释性之间存在权衡。需要开发既能保持高预测性能又具有可解释性的模型。

Method: 将空气污染物浓度的时空行为分解为两个透明加法模块：物理引导的传输核（基于风和地理条件的定向权重）和可解释注意力机制（学习局部响应并将未来浓度归因于特定历史滞后和外生驱动因素）。

Result: 在斯德哥尔摩地区综合数据集上评估，该模型在多个预测时间范围内始终优于最先进的基线方法。

Conclusion: 该模型将高预测性能与时空可解释性相结合，为实际应用中的空气质量管理提供了更可靠的基础。

Abstract: Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.

</details>


### [79] [Multivariate Forecasting of Bitcoin Volatility with Gradient Boosting: Deterministic, Probabilistic, and Feature Importance Perspectives](https://arxiv.org/abs/2511.20105)
*Grzegorz Dudek,Mateusz Kasprzyk,Paweł Pełka*

Main category: cs.LG

TL;DR: 本研究应用LightGBM模型对比特币已实现波动率进行确定性和概率性预测，使用69个市场、行为和宏观经济指标作为预测因子，并与计量经济学和机器学习基准模型进行比较。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场具有高度非线性和高方差特性，传统模型难以有效捕捉其波动率动态。需要开发能够处理复杂市场特征并提供可解释性洞察的预测方法。

Method: 使用LightGBM模型进行确定性和概率性预测。概率预测采用两种分位数方法：基于pinball损失函数的直接分位数回归，以及将点预测转换为预测分布的残差模拟方法。使用增益和排列特征重要性技术识别主要波动率驱动因素。

Result: LightGBM模型在捕捉加密货币市场非线性特征方面表现优异，主要波动率驱动因素包括交易量、滞后波动率指标、投资者关注度和市值。模型提供了对波动率动态的可解释性洞察。

Conclusion: LightGBM模型能够有效处理加密货币市场的高方差特性，在波动率预测方面优于传统基准模型，同时通过特征重要性分析提供了有价值的市场洞察。

Abstract: This study investigates the application of the Light Gradient Boosting Machine (LGBM) model for both deterministic and probabilistic forecasting of Bitcoin realized volatility. Utilizing a comprehensive set of 69 predictors -- encompassing market, behavioral, and macroeconomic indicators -- we evaluate the performance of LGBM-based models and compare them with both econometric and machine learning baselines. For probabilistic forecasting, we explore two quantile-based approaches: direct quantile regression using the pinball loss function, and a residual simulation method that transforms point forecasts into predictive distributions. To identify the main drivers of volatility, we employ gain-based and permutation feature importance techniques, consistently highlighting the significance of trading volume, lagged volatility measures, investor attention, and market capitalization. The results demonstrate that LGBM models effectively capture the nonlinear and high-variance characteristics of cryptocurrency markets while providing interpretable insights into the underlying volatility dynamics.

</details>


### [80] [Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits](https://arxiv.org/abs/2511.20273)
*Areeb Ahmad,Abhinav Joshi,Ashutosh Modi*

Main category: cs.LG

TL;DR: 该论文提出了一种更细粒度的视角，将Transformer中的注意力头和MLP层分解为正交奇异方向，揭示了单个组件内叠加的独立计算。


<details>
  <summary>Details</summary>
Motivation: 现有的机制可解释性方法通常将注意力头和MLP层视为不可分割的单元，忽略了它们内部可能学习到的功能子结构。

Method: 将Transformer组件分解为正交奇异方向，在标准任务（IOI、GP、GT）上验证该方法，分析计算图中节点的低秩方向激活。

Result: 发现先前识别的典型功能头（如名称移动器）编码了多个与不同奇异方向对齐的重叠子功能，有意义的计算存在于紧凑子空间中。

Conclusion: Transformer计算比先前假设的更加分布式、结构化和组合性，这为细粒度机制可解释性和深入理解模型内部开辟了新途径。

Abstract: Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.

</details>


### [81] [CLIMATEAGENT: Multi-Agent Orchestration for Complex Climate Data Science Workflows](https://arxiv.org/abs/2511.20109)
*Hyeonjae Kim,Chenyue Li,Wen Deng,Mengxi Jin,Wen Huang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: ClimateAgent是一个自主的多智能体框架，用于编排端到端的气候数据分析工作流，通过分解用户问题、动态获取数据、生成代码和报告，在85个真实世界任务中实现100%完成率和8.32的报告质量得分。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体和静态脚本管道缺乏气候特定背景和灵活性，在实际应用中表现不佳，需要专门的气候数据分析自动化解决方案。

Method: 采用多智能体框架：Orchestrate-Agent和Plan-Agent协调子任务分解；Data-Agents动态检查API合成下载脚本；Coding-Agent生成Python代码、可视化并带有自校正循环。

Result: 在Climate-Agent-Bench-85基准测试中，ClimateAgent实现100%任务完成率和8.32报告质量得分，显著优于GitHub-Copilot(6.27)和GPT-5基线(3.26)。

Conclusion: 多智能体编排结合动态API感知和自校正执行，显著提升了气候科学分析任务的可靠端到端自动化能力。

Abstract: Climate science demands automated workflows to transform comprehensive questions into data-driven statements across massive, heterogeneous datasets. However, generic LLM agents and static scripting pipelines lack climate-specific context and flexibility, thus, perform poorly in practice. We present ClimateAgent, an autonomous multi-agent framework that orchestrates end-to-end climate data analytic workflows. ClimateAgent decomposes user questions into executable sub-tasks coordinated by an Orchestrate-Agent and a Plan-Agent; acquires data via specialized Data-Agents that dynamically introspect APIs to synthesize robust download scripts; and completes analysis and reporting with a Coding-Agent that generates Python code, visualizations, and a final report with a built-in self-correction loop. To enable systematic evaluation, we introduce Climate-Agent-Bench-85, a benchmark of 85 real-world tasks spanning atmospheric rivers, drought, extreme precipitation, heat waves, sea surface temperature, and tropical cyclones. On Climate-Agent-Bench-85, ClimateAgent achieves 100% task completion and a report quality score of 8.32, outperforming GitHub-Copilot (6.27) and a GPT-5 baseline (3.26). These results demonstrate that our multi-agent orchestration with dynamic API awareness and self-correcting execution substantially advances reliable, end-to-end automation for climate science analytic tasks.

</details>


### [82] [HVAdam: A Full-Dimension Adaptive Optimizer](https://arxiv.org/abs/2511.20277)
*Yiheng Zhang,Shaowu Wu,Yuanzhuo Xu,Jiajun Wu,Shang Xu,Steve Drew,Xiaoguang Niu*

Main category: cs.LG

TL;DR: 提出了Anon优化器，通过可调节的自适应机制在SGD和Adam之间插值甚至超越两者，解决了自适应优化器泛化性能差的问题。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器如Adam在大规模模型训练中表现出色，但在传统架构上泛化性能不如非自适应方法如SGD，主要原因是预调节器的自适应限制了优化器适应不同优化景观的能力。

Method: 提出Anon优化器，具有连续可调的自适应性；引入增量延迟更新机制，比AMSGrad的最大值跟踪策略更灵活，增强了对梯度噪声的鲁棒性。

Result: 理论上在凸和非凸设置下都建立了收敛保证；实证上在图像分类、扩散模型和语言建模任务中持续优于最先进的优化器。

Conclusion: 自适应性可以作为有价值的可调设计原则，Anon提供了首个统一可靠的框架，能够弥合经典和现代优化器之间的差距并超越它们的优势特性。

Abstract: Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.

</details>


### [83] [Geometry of Decision Making in Language Models](https://arxiv.org/abs/2511.20315)
*Abhinav Joshi,Divyanshu Bhatt,Ashutosh Modi*

Main category: cs.LG

TL;DR: 该论文通过内在维度分析研究了LLM在多项选择题回答任务中的决策过程，发现模型在早期层使用低维流形，中间层扩展维度，后期层压缩维度并收敛到与决策相关的表示。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多样化任务中表现出强大的泛化能力，但其内部决策过程仍然不透明，需要从几何角度理解其隐藏表示。

Method: 对28个开源transformer模型进行大规模研究，使用多个估计器估计各层的内在维度，并量化每层在MCQA任务上的性能。

Result: 发现模型间存在一致的ID模式：早期层在低维流形上操作，中间层扩展空间，后期层压缩空间并收敛到决策相关表示。

Conclusion: LLM隐式学习将语言输入投影到与任务特定决策对齐的结构化低维流形上，为理解语言模型中泛化和推理的出现提供了新的几何视角。

Abstract: Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.

</details>


### [84] [Soft Adaptive Policy Optimization](https://arxiv.org/abs/2511.20347)
*Chang Gao,Chujie Zheng,Xiong-Hui Chen,Kai Dang,Shixuan Liu,Bowen Yu,An Yang,Shuai Bai,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: SAPO是一种用于大语言模型强化学习的软自适应策略优化方法，通过温度控制的门机制替代硬裁剪，在保持序列级一致性的同时实现更稳定高效的训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于分组的策略优化方法（如GSPO和GRPO）使用硬裁剪，难以同时保持训练稳定性和学习效果，特别是在混合专家模型中，token级重要性比率的高方差问题更加严重。

Method: 提出软自适应策略优化（SAPO），用平滑的温度控制门机制替代硬裁剪，自适应地衰减离策略更新，同时保留有用的学习信号。该方法既保持序列级一致性，又具有token级自适应性。

Result: 在数学推理基准测试中，SAPO在相同训练预算下表现出更好的训练稳定性和更高的Pass@1性能。在Qwen3-VL模型系列上的应用表明，SAPO在不同任务和模型规模下都能带来一致的性能提升。

Conclusion: SAPO为大语言模型的强化学习训练提供了更可靠、可扩展和有效的优化策略。

Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.

</details>


### [85] [AdaCap: An Adaptive Contrastive Approach for Small-Data Neural Networks](https://arxiv.org/abs/2511.20170)
*Bruno Belucci,Karim Lounici,Katia Meziani*

Main category: cs.LG

TL;DR: AdaCap是一种结合置换对比损失和Tikhonov闭式输出映射的训练方案，在小样本表格数据上显著提升神经网络性能，特别是残差模型。


<details>
  <summary>Details</summary>
Motivation: 神经网络在小样本表格数据集上表现不佳，而树模型仍占主导地位。需要开发专门针对小样本场景的神经网络训练方法。

Method: 提出自适应对比方法(AdaCap)，结合基于置换的对比损失和基于Tikhonov的闭式输出映射。使用元预测器根据数据集特征预测AdaCap的适用性。

Result: 在85个真实世界回归数据集上，AdaCap在小样本场景中产生一致且统计显著的改进，特别是对残差模型效果更好。元预测器能准确预测AdaCap何时有益。

Conclusion: AdaCap作为一种针对性正则化机制，在神经网络最脆弱的地方强化其性能，为小样本表格数据上的神经网络应用提供了有效解决方案。

Abstract: Neural networks struggle on small tabular datasets, where tree-based models remain dominant. We introduce Adaptive Contrastive Approach (AdaCap), a training scheme that combines a permutation-based contrastive loss with a Tikhonov-based closed-form output mapping. Across 85 real-world regression datasets and multiple architectures, AdaCap yields consistent and statistically significant improvements in the small-sample regime, particularly for residual models. A meta-predictor trained on dataset characteristics (size, skewness, noise) accurately anticipates when AdaCap is beneficial. These results show that AdaCap acts as a targeted regularization mechanism, strengthening neural networks precisely where they are most fragile. All results and code are publicly available at https://github.com/BrunoBelucci/adacap.

</details>


### [86] [Short-Range Oversquashing](https://arxiv.org/abs/2511.20406)
*Yaaqov Mishayev,Yonatan Sverdlov,Tal Amir,Nadav Dym*

Main category: cs.LG

TL;DR: 本文揭示了过压缩现象不仅存在于长距离任务中，也出现在短距离任务中，并区分了瓶颈现象和梯度消失两种机制。研究表明虚拟节点无法解决短距离瓶颈问题，而Transformer模型在此类任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: MPNN在图学习中广泛应用，但其处理长距离信息的能力受到过压缩现象的限制。现有研究对过压缩的理解存在分歧，部分研究者主张使用Graph Transformers，而其他研究者认为可以通过虚拟节点等技术在MPNN框架内解决此问题。

Method: 通过分析短距离任务中的过压缩现象，区分瓶颈现象和梯度消失两种机制。比较虚拟节点方法和Transformer模型在解决这些问题的效果。

Result: 发现短距离任务中也存在过压缩现象，且现有解释无法捕捉短距离瓶颈效应。虚拟节点无法解决短距离瓶颈问题，而Transformer模型在此类任务中表现成功。

Conclusion: Transformer模型相比专门的MPNN是解决过压缩问题的更有效方案，特别是在处理短距离瓶颈问题上具有优势。

Abstract: Message Passing Neural Networks (MPNNs) are widely used for learning on graphs, but their ability to process long-range information is limited by the phenomenon of oversquashing. This limitation has led some researchers to advocate Graph Transformers as a better alternative, whereas others suggest that it can be mitigated within the MPNN framework, using virtual nodes or other rewiring techniques.
  In this work, we demonstrate that oversquashing is not limited to long-range tasks, but can also arise in short-range problems. This observation allows us to disentangle two distinct mechanisms underlying oversquashing: (1) the bottleneck phenomenon, which can arise even in low-range settings, and (2) the vanishing gradient phenomenon, which is closely associated with long-range tasks.
  We further show that the short-range bottleneck effect is not captured by existing explanations for oversquashing, and that adding virtual nodes does not resolve it. In contrast, transformers do succeed in such tasks, positioning them as the more compelling solution to oversquashing, compared to specialized MPNNs.

</details>


### [87] [Learning Subgroups with Maximum Treatment Effects without Causal Heuristics](https://arxiv.org/abs/2511.20189)
*Lincen Yang,Zhong Li,Matthijs van Leeuwen,Saber Salehkaleybar*

Main category: cs.LG

TL;DR: 本文提出了一种基于结构因果模型（SCM）的子群发现方法，避免了传统方法中依赖点估计或临时因果启发式的问题，将最优子群发现转化为标准监督学习问题。


<details>
  <summary>Details</summary>
Motivation: 当前子群发现方法存在两个主要问题：一是依赖准确的点估计条件处理效应，二是使用缺乏严格理论依据的临时因果启发式。本文旨在直接在SCM框架下解决这些问题。

Method: 在基于划分的模型假设下，将最优子群发现问题转化为数据生成模型恢复问题，从而可以使用任何基于划分的方法（如CART）来学习具有最大处理效应的子群。

Result: 在大量合成和半合成数据集上的实验表明，该方法比广泛基线方法更准确地识别具有最大处理效应的子群。

Conclusion: 基于SCM框架的方法避免了因果启发式，能够更有效地发现最优子群，为精准医学、公共政策等领域的定向决策提供了更好的解决方案。

Abstract: Discovering subgroups with the maximum average treatment effect is crucial for targeted decision making in domains such as precision medicine, public policy, and education. While most prior work is formulated in the potential outcome framework, the corresponding structural causal model (SCM) for this task has been largely overlooked. In practice, two approaches dominate. The first estimates pointwise conditional treatment effects and then fits a tree on those estimates, effectively turning subgroup estimation into the harder problem of accurate pointwise estimation. The second constructs decision trees or rule sets with ad-hoc 'causal' heuristics, typically without rigorous justification for why a given heuristic may be used or whether such heuristics are necessary at all. We address these issues by studying the problem directly under the SCM framework. Under the assumption of a partition-based model, we show that optimal subgroup discovery reduces to recovering the data-generating models and hence a standard supervised learning problem (regression or classification). This allows us to adopt any partition-based methods to learn the subgroup from data. We instantiate the approach with CART, arguably one of the most widely used tree-based methods, to learn the subgroup with maximum treatment effect. Finally, on a large collection of synthetic and semi-synthetic datasets, we compare our method against a wide range of baselines and find that our approach, which avoids such causal heuristics, more accurately identifies subgroups with maximum treatment effect. Our source code is available at https://github.com/ylincen/causal-subgroup.

</details>


### [88] [In-Context Compositional Learning via Sparse Coding Transformer](https://arxiv.org/abs/2511.20194)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.LG

TL;DR: 本文提出了一种基于稀疏编码原理的注意力机制改进方法，通过将注意力块重新解释为输入到输出的映射，使用编码和解码字典原子来增强Transformer在组合学习任务中的能力。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在处理组合学习任务时存在局限性，因为它们并非天生设计用于处理组合任务且提供有限的结构归纳偏置。

Method: 将注意力块重新解释为通过投影到两组学习到的字典原子（编码字典和解码字典）来映射输入到输出，对系数施加稀疏性以增强结构化表示，并估计目标问题的系数作为上下文示例系数的线性组合。

Result: 在S-RAVEN和RAVEN数据集上验证了方法的有效性，对于某些组合泛化任务，即使标准Transformer失败时，该方法仍能保持性能。

Conclusion: 该方法能够学习和应用组合规则，有效增强了Transformer在组合任务中的能力。

Abstract: Transformer architectures have achieved remarkable success across language, vision, and multimodal tasks, and there is growing demand for them to address in-context compositional learning tasks. In these tasks, models solve the target problems by inferring compositional rules from context examples, which are composed of basic components structured by underlying rules. However, some of these tasks remain challenging for Transformers, which are not inherently designed to handle compositional tasks and offer limited structural inductive bias. In this work, inspired by the principle of sparse coding, we propose a reformulation of the attention to enhance its capability for compositional tasks. In sparse coding, data are represented as sparse combinations of dictionary atoms with coefficients that capture their compositional rules. Specifically, we reinterpret the attention block as a mapping of inputs into outputs through projections onto two sets of learned dictionary atoms: an encoding dictionary and a decoding dictionary. The encoding dictionary decomposes the input into a set of coefficients, which represent the compositional structure of the input. To enhance structured representations, we impose sparsity on these coefficients. The sparse coefficients are then used to linearly combine the decoding dictionary atoms to generate the output. Furthermore, to assist compositional generalization tasks, we propose estimating the coefficients of the target problem as a linear combination of the coefficients obtained from the context examples. We demonstrate the effectiveness of our approach on the S-RAVEN and RAVEN datasets. For certain compositional generalization tasks, our method maintains performance even when standard Transformers fail, owing to its ability to learn and apply compositional rules.

</details>


### [89] [MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology](https://arxiv.org/abs/2511.20490)
*Kiril Vasilev,Alexandre Misrahi,Eeshaan Jain,Phil F Cheng,Petros Liakopoulos,Olivier Michielin,Michael Moor,Charlotte Bunne*

Main category: cs.LG

TL;DR: MTBBench是一个模拟分子肿瘤委员会(MTB)决策过程的基准测试，用于评估多模态大语言模型在生物医学推理中的表现，特别关注多模态、纵向临床数据的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型评估基准无法捕捉真实世界临床工作流程的复杂性，特别是缺乏模拟分子肿瘤委员会(MTB)这种多专家决策环境的评估。

Method: 开发MTBBench基准测试，模拟MTB风格的决策过程，包含临床挑战性、多模态和纵向的肿瘤学问题，并通过临床医生验证的真实标注。

Result: 评估显示即使是大规模模型也缺乏可靠性，经常产生幻觉，难以处理时间序列数据推理，无法调和冲突证据或不同模态信息。

Conclusion: MTBBench提供了一个具有挑战性和真实性的测试平台，用于推进多模态大语言模型在精准肿瘤学MTB环境中的推理能力、可靠性和工具使用。

Abstract: Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.

</details>


### [90] [Decoupling and Damping: Structurally-Regularized Gradient Matching for Multimodal Graph Condensation](https://arxiv.org/abs/2511.20222)
*Lian Shen,Zhendan Chen,Yinhui jiang,Meijia Song,Ziming Su,Juan Liu,Xiangrong Liu*

Main category: cs.LG

TL;DR: 提出SR-GM框架解决多模态图神经网络训练中的梯度冲突和结构噪声放大问题，通过梯度解耦和结构阻尼正则化实现高效图压缩。


<details>
  <summary>Details</summary>
Motivation: 多模态图在关键Web应用中日益重要，但大规模训练带来巨大计算负担。现有图压缩方法在多模态设置中因梯度冲突和消息传递架构放大噪声而失败。

Method: SR-GM框架包含两个组件：1）梯度解耦机制，通过正交投影解决模态间梯度冲突；2）结构阻尼正则化器，利用图的Dirichlet能量将拓扑从噪声放大器转变为稳定力。

Result: 实验表明SR-GM显著提高准确率并加速收敛，消融研究证实同时解决梯度冲突和结构放大对获得优越性能至关重要。压缩后的多模态图展现出强大的跨架构泛化能力。

Conclusion: 该研究为资源受限环境中的多模态图学习提供了可扩展方法，有望加速神经架构搜索等应用。

Abstract: In critical web applications such as e-commerce and recommendation systems, multimodal graphs integrating rich visual and textual attributes are increasingly central, yet their large scale introduces substantial computational burdens for training Graph Neural Networks (GNNs). While Graph Condensation (GC) offers a promising solution by synthesizing smaller datasets, existing methods falter in the multimodal setting. We identify a dual challenge causing this failure: (1) conflicting gradients arising from semantic misalignments between modalities, and (2) the GNN's message-passing architecture pathologically amplifying this gradient noise across the graph structure. To address this, we propose Structurally-Regularized Gradient Matching (SR-GM), a novel condensation framework tailored for multimodal graphs. SR-GM introduces two synergistic components: first, a gradient decoupling mechanism that resolves inter-modality conflicts at their source via orthogonal projection; and second, a structural damping regularizer that acts directly on the gradient field. By leveraging the graph's Dirichlet energy, this regularizer transforms the topology from a noise amplifier into a stabilizing force during optimization. Extensive experiments demonstrate that SR-GM significantly improves accuracy and accelerates convergence compared to baseline methods. Ablation studies confirm that addressing both gradient conflict and structural amplification in tandem is essential for achieving superior performance. Moreover, the condensed multimodal graphs exhibit strong cross-architecture generalization and promise to accelerate applications like Neural Architecture Search. This research provides a scalable methodology for multimodal graph-based learning in resource-constrained environments.

</details>


### [91] [BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents](https://arxiv.org/abs/2511.20597)
*Kaiyuan Zhang,Mark Tenenholtz,Kyle Polley,Jerry Ma,Denis Yarats,Ninghui Li*

Main category: cs.LG

TL;DR: 本文研究了AI网络代理中的提示注入攻击，创建了一个包含真实HTML负载的攻击基准，评估了现有防御措施的有效性，并提出了多层防御策略。


<details>
  <summary>Details</summary>
Motivation: AI代理集成到网络浏览器中带来了超越传统网络应用威胁模型的安全挑战，特别是提示注入攻击在实际环境中的影响尚未得到充分理解。

Method: 合成嵌入在真实HTML负载中的提示注入攻击基准，强调影响实际操作的攻击而非仅文本输出，并进行全面的实证评估现有防御措施。

Result: 创建了复杂度和干扰频率与现实环境相似的攻击基准，评估了前沿AI模型的防御效果，提出了包含架构和模型防御的多层防御策略。

Conclusion: 通过深度防御方法为设计实用、安全的网络代理提供了蓝图，有效应对不断演变的提示注入攻击。

Abstract: The integration of artificial intelligence (AI) agents into web browsers introduces security challenges that go beyond traditional web application threat models. Prior work has identified prompt injection as a new attack vector for web agents, yet the resulting impact within real-world environments remains insufficiently understood.
  In this work, we examine the landscape of prompt injection attacks and synthesize a benchmark of attacks embedded in realistic HTML payloads. Our benchmark goes beyond prior work by emphasizing injections that can influence real-world actions rather than mere text outputs, and by presenting attack payloads with complexity and distractor frequency similar to what real-world agents encounter. We leverage this benchmark to conduct a comprehensive empirical evaluation of existing defenses, assessing their effectiveness across a suite of frontier AI models. We propose a multi-layered defense strategy comprising both architectural and model-based defenses to protect against evolving prompt injection attacks. Our work offers a blueprint for designing practical, secure web agents through a defense-in-depth approach.

</details>


### [92] [DiCaP: Distribution-Calibrated Pseudo-labeling for Semi-Supervised Multi-Label Learning](https://arxiv.org/abs/2511.20225)
*Bo Han,Zhuoming Li,Xiaoyu Wang,Yaxin Hou,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 提出DiCaP框架，通过估计后验精度来校准伪标签权重，结合双阈值机制区分置信和模糊样本，在半监督多标签学习中实现性能提升


<details>
  <summary>Details</summary>
Motivation: 现有半监督多标签学习方法对所有伪标签赋予相同权重，忽略了伪标签质量差异，可能放大噪声预测的影响

Method: 基于理论验证伪标签最优权重应反映其正确性概率，提出DiCaP框架估计后验精度校准权重，使用双阈值机制分离置信和模糊样本区域

Result: 在多个基准数据集上验证，方法取得一致改进，超越最先进方法达4.27%

Conclusion: DiCaP框架通过校准伪标签权重和双阈值机制，有效提升了半监督多标签学习的性能

Abstract: Semi-supervised multi-label learning (SSMLL) aims to address the challenge of limited labeled data in multi-label learning (MLL) by leveraging unlabeled data to improve the model's performance. While pseudo-labeling has become a dominant strategy in SSMLL, most existing methods assign equal weights to all pseudo-labels regardless of their quality, which can amplify the impact of noisy or uncertain predictions and degrade the overall performance. In this paper, we theoretically verify that the optimal weight for a pseudo-label should reflect its correctness likelihood. Empirically, we observe that on the same dataset, the correctness likelihood distribution of unlabeled data remains stable, even as the number of labeled training samples varies. Building on this insight, we propose Distribution-Calibrated Pseudo-labeling (DiCaP), a correctness-aware framework that estimates posterior precision to calibrate pseudo-label weights. We further introduce a dual-thresholding mechanism to separate confident and ambiguous regions: confident samples are pseudo-labeled and weighted accordingly, while ambiguous ones are explored by unsupervised contrastive learning. Experiments conducted on multiple benchmark datasets verify that our method achieves consistent improvements, surpassing state-of-the-art methods by up to 4.27%.

</details>


### [93] [The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting](https://arxiv.org/abs/2511.20601)
*Heman Shakeri*

Main category: cs.LG

TL;DR: 血糖预测的深度序列模型未能有效利用胰岛素、饮食和活动等临床驱动因素，存在"驱动因素盲区"问题。该问题由三个因素导致：架构偏好自相关、数据质量问题和生理异质性。


<details>
  <summary>Details</summary>
Motivation: 当前深度序列模型在血糖预测中未能充分利用临床驱动因素（胰岛素、饮食、活动），尽管这些因素的生理机制已被充分理解，这限制了模型的预测性能。

Method: 提出Δ_drivers指标来衡量多变量模型相对于单变量基线的性能增益，分析导致驱动因素盲区的三个因素：架构偏好自相关、数据保真度问题和生理异质性。

Result: 文献中Δ_drivers通常接近零，表明多变量模型未能有效利用驱动因素。提出了包括生理特征编码器、因果正则化和个性化等缓解策略。

Conclusion: 建议未来工作常规报告Δ_drivers指标，以防止驱动因素盲区模型被误认为是最先进的，并推荐采用生理特征编码、因果正则化和个性化等策略来改善模型性能。

Abstract: Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Δ_{\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Δ_{\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Δ_{\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.

</details>


### [94] [DiFR: Inference Verification Despite Nondeterminism](https://arxiv.org/abs/2511.20621)
*Adam Karvonen,Daniel Reuter,Roy Rinberg,Luke Marks,Adrià Garriga-Alonso,Keri Warr*

Main category: cs.LG

TL;DR: Token-DiFR是一种通过比较生成令牌与可信参考实现预测来验证LLM推理输出的方法，使用采样种子同步约束有效输出，使输出令牌本身成为零成本的可审计证据。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理需求增长，需要验证推理过程是否正确执行，没有错误或篡改。但由于良性数值噪声，重复运行相同推理过程会产生不同结果，难以区分合法变化与实际问题。

Method: 引入Token-DiFR方法，通过将生成令牌与在相同随机种子条件下可信参考实现的预测进行比较来验证推理输出。采样种子同步严格约束有效输出，使输出令牌本身成为可审计证据。还引入Activation-DiFR，使用随机正交投影将激活压缩为紧凑指纹进行验证。

Result: Token-DiFR可靠识别采样错误、模拟错误和模型量化，在300个输出令牌内检测4位量化的AUC > 0.999。Activation-DiFR仅用2个输出令牌就能检测4位量化，AUC > 0.999，通信开销比现有方法减少25-75%。

Conclusion: Token-DiFR和Activation-DiFR为LLM推理验证提供了高效解决方案，Token-DiFR实现零成本验证，Activation-DiFR在样本效率要求高的场景下显著降低通信开销。开源集成vLLM加速可验证推理的实际部署。

Abstract: As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.

</details>


### [95] [ROOT: Robust Orthogonalized Optimizer for Neural Network Training](https://arxiv.org/abs/2511.20626)
*Wei He,Kai Han,Hang Zhou,Hanting Chen,Zhicheng Liu,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: ROOT是一种鲁棒正交化优化器，通过维度鲁棒正交化方案和优化鲁棒框架解决大语言模型训练中的稳定性问题，在噪声和非凸场景下表现优于现有优化器。


<details>
  <summary>Details</summary>
Motivation: 大语言模型优化面临算法精度敏感和训练不稳定的挑战，现有正交化优化器存在维度脆弱性和对异常值噪声的脆弱性。

Method: 开发维度鲁棒正交化方案（自适应牛顿迭代配合细粒度系数）和优化鲁棒框架（近端优化抑制异常值噪声），构建ROOT优化器。

Result: 实验表明ROOT在鲁棒性、收敛速度和最终性能方面显著优于Muon和Adam优化器，特别在噪声和非凸场景下表现突出。

Conclusion: ROOT为开发能够处理现代大规模模型训练复杂性的鲁棒精确优化器建立了新范式。

Abstract: The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.

</details>


### [96] [MXtalTools: A Toolkit for Machine Learning on Molecular Crystals](https://arxiv.org/abs/2511.20327)
*Michael Kilgour,Mark E. Tuckerman,Jutta Rogal*

Main category: cs.LG

TL;DR: MXtalTools是一个用于分子晶体数据驱动建模的灵活Python包，支持机器学习研究分子固态。


<details>
  <summary>Details</summary>
Motivation: 为分子晶体的机器学习研究提供灵活的工具包，促进分子固态的数据驱动建模。

Method: 包含数据集合成整理、模型训练推理、晶体参数化表示、结构采样优化、端到端可微分晶体采样构建分析等模块化功能。

Result: 开发了支持CUDA加速的高通量晶体建模工具包，代码开源并提供详细文档。

Conclusion: MXtalTools提供了模块化的分子晶体建模工具，可集成到现有工作流或构建新的建模流程。

Abstract: We present MXtalTools, a flexible Python package for the data-driven modelling of molecular crystals, facilitating machine learning studies of the molecular solid state. MXtalTools comprises several classes of utilities: (1) synthesis, collation, and curation of molecule and crystal datasets, (2) integrated workflows for model training and inference, (3) crystal parameterization and representation, (4) crystal structure sampling and optimization, (5) end-to-end differentiable crystal sampling, construction and analysis. Our modular functions can be integrated into existing workflows or combined and used to build novel modelling pipelines. MXtalTools leverages CUDA acceleration to enable high-throughput crystal modelling. The Python code is available open-source on our GitHub page, with detailed documentation on ReadTheDocs.

</details>


### [97] [Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning](https://arxiv.org/abs/2511.20349)
*M. E. A. Kherchouche,F. Galpin,T. Dumas,F. Schnitzler,D. Menard,L. Zhang*

Main category: cs.LG

TL;DR: 提出了两种机器学习方法来加速VVC帧内分区的RDO过程：基于回归的方法预测CU的归一化RD成本，以及基于强化学习的方法将分区决策建模为MDP问题。两种方法都使用相邻块的RD成本作为输入特征，并通过预设阈值选择合适的分割方式。


<details>
  <summary>Details</summary>
Motivation: 为了解决VVC帧内分区在RDO过程中涉及的穷举搜索带来的高计算复杂度问题，需要开发高效的加速方法。

Method: 1. 基于回归的方法：预测给定CU的归一化RD成本；2. 基于强化学习的方法：将分区决策建模为马尔可夫决策过程，使用DQN算法从CU决策轨迹中学习。两种方法都使用相邻块的RD成本作为输入特征，并通过预设阈值选择分割方式。

Result: 论文比较了两种机器学习技术，它们都是尺寸无关的，并整合了相邻块的RD成本作为输入特征。

Conclusion: 提出的两种方法能够有效加速VVC帧内分区的RDO过程，其中基于强化学习的方法特别适合处理具有马尔可夫性质的分区决策问题。

Abstract: In this paper, a complexity study is conducted for Versatile Video Codec (VVC) intra partitioning to accelerate the exhaustive search involved in Rate-Distortion Optimization (RDO) process. To address this problem, two main machine learning techniques are proposed and compared. Unlike existing methods, the proposed approaches are size independent and incorporate the Rate-Distortion (RD) costs of neighboring blocks as input features. The first method is a regression based technique that predicts normalized RD costs of a given Coding Unit (CU). As partitioning possesses the Markov property, the associated decision-making problem can be modeled as a Markov Decision Process (MDP) and solved by Reinforcement Learning (RL). The second approach is a RL agent learned from trajectories of CU decision across two depths with Deep Q-Network (DQN) algorithm. Then a pre-determined thresholds are applied for both methods to select a suitable split for the current CU.

</details>


### [98] [PRISM: Periodic Representation with multIscale and Similarity graph Modelling for enhanced crystal structure property prediction](https://arxiv.org/abs/2511.20362)
*Àlex Solé,Albert Mosella-Montoro,Joan Cardona,Daniel Aravena,Silvia Gómez-Coca,Eliseo Ruiz,Javier Ruiz-Hidalgo*

Main category: cs.LG

TL;DR: PRISM是一个图神经网络框架，通过集成多尺度表示和周期性特征编码来改进晶体结构预测，显著提升了晶体性质预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前方法往往忽略晶体结构固有的周期性边界条件和多尺度相互作用，这限制了图表示学习在晶体结构分析中的应用效果。

Method: PRISM采用一组专家模块，每个模块专门编码周期性系统的不同结构和化学方面，明确整合多尺度表示和周期性特征编码。

Result: 在多个基于晶体结构的基准测试中，PRISM显著提高了预测精度，超越了现有最先进方法。

Conclusion: PRISM框架通过有效整合周期性边界条件和多尺度相互作用，为晶体性质预测提供了更准确和强大的解决方案。

Abstract: Crystal structures are characterised by repeating atomic patterns within unit cells across three-dimensional space, posing unique challenges for graph-based representation learning. Current methods often overlook essential periodic boundary conditions and multiscale interactions inherent to crystalline structures. In this paper, we introduce PRISM, a graph neural network framework that explicitly integrates multiscale representations and periodic feature encoding by employing a set of expert modules, each specialised in encoding distinct structural and chemical aspects of periodic systems. Extensive experiments across crystal structure-based benchmarks demonstrate that PRISM improves state-of-the-art predictive accuracy, significantly enhancing crystal property prediction.

</details>


### [99] [MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers](https://arxiv.org/abs/2511.20382)
*Audrey Pei-Hsuan Chen*

Main category: cs.LG

TL;DR: MoRE是一个多组学表示学习框架，通过冻结预训练transformer主干，使用参数高效微调策略将异质检测数据对齐到共享潜在空间，在保持生物学结构的同时显著减少可训练参数。


<details>
  <summary>Details</summary>
Motivation: 多组学数据存在极端维度、模态异质性和批次效应等挑战，而预训练transformer在生物序列建模中显示出广泛泛化能力，但在多组学整合中应用不足。

Method: MoRE在冻结预训练主干上附加轻量级模态特定适配器和任务自适应融合层，联合优化掩码建模目标与监督对比学习和批次不变对齐损失。

Result: MoRE在批次鲁棒性和生物学保守性方面达到竞争性表现，同时相比完全微调模型显著减少可训练参数。

Conclusion: MoRE是迈向通用组学基础模型的实用步骤，展示了冻结预训练transformer在多组学整合中的潜力。

Abstract: Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with scArches, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.

</details>


### [100] [Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI](https://arxiv.org/abs/2511.20395)
*M. C. Schoppema,B. H. M. van der Velden,A. Hürriyetoğlu,M. D. Klijnstra,E. J. Faassen,A. Gerssen,H. J. van der Fels-Klerx*

Main category: cs.LG

TL;DR: 开发了可解释的深度学习模型，用于预测荷兰Zeeland河口双壳类软体动物中河豚毒素(TTX)污染，识别出日照时间、全球辐射、水温和氯化物浓度是主要影响因素。


<details>
  <summary>Details</summary>
Motivation: 自2012年以来，欧洲温带水域的双壳类软体动物中发现河豚毒素污染，导致食品安全风险和经济损失，需要早期预测TTX污染。

Method: 开发基于深度学习的可解释模型，输入气象和水文特征，输出TTX污染的存在或缺失。

Result: 日出时间、日落时间、全球辐射、水温和氯化物浓度对TTX污染贡献最大，有效日照时间是重要驱动因素。

Conclusion: 可解释深度学习模型识别出日照时间、全球辐射、水温和水氯化物浓度与双壳类软体动物中TTX污染相关，为食品行业和监管机构提供了有价值的风险缓解工具。

Abstract: Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.
  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.
  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.
  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.

</details>


### [101] [Model-Based Learning of Whittle indices](https://arxiv.org/abs/2511.20397)
*Joël Charles-Rebuffé,Nicolas Gast,Bruno Gaujal*

Main category: cs.LG

TL;DR: BLINQ是一种新的基于模型的算法，用于学习可索引、可通信且单链马尔可夫决策过程的Whittle指数。该方法通过构建MDP的经验估计并计算其Whittle指数，在样本效率和计算成本方面显著优于现有的Q学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Q学习方法在样本效率和计算成本方面存在不足，特别是在学习Whittle指数时表现不佳。作者希望开发一种更高效的方法来学习这些指数。

Method: BLINQ通过构建马尔可夫决策过程的经验估计，然后使用现有最先进算法的扩展版本计算其Whittle指数。

Result: 数值实验表明，BLINQ在获得准确近似所需的样本数量方面显著优于现有的Q学习方法，并且在任何合理高样本数量下的总计算成本甚至低于Q学习。即使Q学习方法使用预训练神经网络加速，这些优势仍然存在。

Conclusion: BLINQ提供了一种高效学习Whittle指数的方法，在样本效率和计算成本方面都优于现有的Q学习方法，为相关领域的研究提供了新的解决方案。

Abstract: We present BLINQ, a new model-based algorithm that learns the Whittle indices of an indexable, communicating and unichain Markov Decision Process (MDP). Our approach relies on building an empirical estimate of the MDP and then computing its Whittle indices using an extended version of a state-of-the-art existing algorithm. We provide a proof of convergence to the Whittle indices we want to learn as well as a bound on the time needed to learn them with arbitrary precision. Moreover, we investigate its computational complexity. Our numerical experiments suggest that BLINQ significantly outperforms existing Q-learning approaches in terms of the number of samples needed to get an accurate approximation. In addition, it has a total computational cost even lower than Q-learning for any reasonably high number of samples. These observations persist even when the Q-learning algorithms are speeded up using pre-trained neural networks to predict Q-values.

</details>


### [102] [Tight Margin-Based Generalization Bounds for Voting Classifiers over Finite Hypothesis Sets](https://arxiv.org/abs/2511.20407)
*Kasper Green Larsen,Natascha Schalburg*

Main category: cs.LG

TL;DR: 提出了第一个基于间隔的投票分类器泛化界，在假设集大小、间隔、训练点比例、样本数量和失败概率之间的权衡上渐近紧致


<details>
  <summary>Details</summary>
Motivation: 为投票分类器建立更精确的泛化理论保证，填补现有理论在权衡关系上的不足

Method: 通过理论分析推导出基于间隔的泛化界，考虑多个关键参数的交互影响

Result: 获得了渐近紧致的泛化边界，在假设集大小、间隔、训练点比例、样本数量和失败概率之间的权衡关系上达到最优

Conclusion: 该工作为投票分类器提供了更精确的理论基础，有助于理解其泛化性能的关键影响因素

Abstract: We prove the first margin-based generalization bound for voting classifiers, that is asymptotically tight in the tradeoff between the size of the hypothesis set, the margin, the fraction of training points with the given margin, the number of training samples and the failure probability.

</details>


### [103] [Diffusion for Fusion: Designing Stellarators with Generative AI](https://arxiv.org/abs/2511.20445)
*Misha Padidar,Teresa Huang,Andrew Giuliani,Marina Spivak*

Main category: cs.LG

TL;DR: 本文提出使用条件扩散模型快速生成具有理想特性的准对称仿星器设计，作为机器学习在仿星器设计中的案例研究。


<details>
  <summary>Details</summary>
Motivation: 仿星器设计通常需要大量计算时间，而机器学习方法可以利用大型优化仿星器数据集来加速设计过程。

Method: 在QUASR数据库上训练条件扩散模型，生成具有特定长宽比和平均旋转变换的准对称仿星器设计。

Result: 生成的仿星器表现出良好性能：准对称性偏差小于5%，且能实现训练中未见的设计特性。

Conclusion: 扩散模型在仿星器设计中具有潜力，有望将准对称性偏差进一步降低到1%以下，为仿星器设计提供了有前景的生成建模方法。

Abstract: Stellarators are a prospective class of fusion-based power plants that confine a hot plasma with three-dimensional magnetic fields. Typically framed as a PDE-constrained optimization problem, stellarator design is a time-consuming process that can take hours to solve on a computing cluster. Developing fast methods for designing stellarators is crucial for advancing fusion research. Given the recent development of large datasets of optimized stellarators, machine learning approaches have emerged as a potential candidate. Motivated by this, we present an open inverse problem to the machine learning community: to rapidly generate high-quality stellarator designs which have a set of desirable characteristics. As a case study in the problem space, we train a conditional diffusion model on data from the QUASR database to generate quasisymmetric stellarator designs with desirable characteristics (aspect ratio and mean rotational transform). The diffusion model is applied to design stellarators with characteristics not seen during training. We provide evaluation protocols and show that many of the generated stellarators exhibit solid performance: less than 5% deviation from quasisymmetry and the target characteristics. The modest deviation from quasisymmetry highlights an opportunity to reach the sub 1% target. Beyond the case study, we share multiple promising avenues for generative modeling to advance stellarator design.

</details>


### [104] [Towards Trustworthy Wi-Fi Sensing: Systematic Evaluation of Deep Learning Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2511.20456)
*Shreevanth Krishnaa Gopalakrishnan,Stephen Hailes*

Main category: cs.LG

TL;DR: 本文系统评估了CSI深度学习模型在不同威胁模型下的鲁棒性，发现较小模型虽然高效但在对抗攻击下鲁棒性较差，物理可实现的信号空间扰动相比无约束特征空间攻击效果更差，对抗训练能有效缓解这些漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在基于CSI的人类感知系统中日益重要，模型决策容易受到细微扰动影响，这对无处不在的感知系统的安全性和可靠性提出了担忧。在无线感知部署到真实环境之前，量化理解模型的鲁棒性至关重要。

Method: 建立了评估框架，在三个公共数据集上比较紧凑时间自编码器模型与较大深度架构，量化模型规模、训练机制和物理约束对鲁棒性的影响，涵盖白盒、黑盒/迁移和通用扰动等不同威胁模型。

Result: 实验表明较小模型虽然高效且在干净数据上表现相当，但鲁棒性明显较差；物理可实现的信号空间扰动相比无约束特征空间攻击显著降低了攻击成功率；对抗训练能够缓解这些漏洞，在两种模型类别中都能提高平均鲁棒准确率，且对干净性能的退化适中。

Conclusion: 随着无线感知向可靠、跨域操作发展，这些发现为鲁棒性估计提供了量化基准，并为设计安全可信的人类中心感知系统提供了指导原则。

Abstract: Machine learning has become integral to Channel State Information (CSI)-based human sensing systems and is expected to power applications such as device-free activity recognition and identity detection in future cellular and Wi-Fi generations. However, these systems rely on models whose decisions can be subtly perturbed, raising concerns for security and reliability in ubiquitous sensing. Quantifying and understanding the robustness of such models, defined as their ability to maintain accurate predictions under adversarial perturbations, is therefore critical before wireless sensing can be safely deployed in real-world environments.
  This work presents a systematic evaluation of the robustness of CSI deep learning models under diverse threat models (white-box, black-box/transfer, and universal perturbations) and varying degrees of attack realism. We establish a framework to compare compact temporal autoencoder models with larger deep architectures across three public datasets, quantifying how model scale, training regime, and physical constraints influence robustness. Our experiments show that smaller models, while efficient and equally performant on clean data, are markedly less robust. We further confirm that physically realizable signal-space perturbations, designed to be feasible in real wireless channels, significantly reduce attack success compared to unconstrained feature-space attacks. Adversarial training mitigates these vulnerabilities, improving mean robust accuracy with only moderate degradation in clean performance across both model classes. As wireless sensing advances towards reliable, cross-domain operation, these findings provide quantitative baselines for robustness estimation and inform design principles for secure and trustworthy human-centered sensing systems.

</details>


### [105] [NVIDIA Nemotron Parse 1.1](https://arxiv.org/abs/2511.20478)
*Kateryna Chumachenko,Amala Sanjay Deshmukh,Jarno Seppanen,Ilia Karmanov,Chia-Chih Chen,Lukas Voegtle,Philipp Fischer,Marek Wawrzos,Saeid Motiian,Roman Ageev,Kedi Wu,Alexandre Milesi,Maryam Moosaei,Krzysztof Pawelec,Padmavathy Subramanian,Mehrzad Samadi,Xin Yu,Celina Dear,Sarah Stoddard,Jenna Diamond,Jesse Oliver,Leanna Chraghchian,Patrick Skelly,Tom Balough,Yao Xu,Jane Polak Scowcroft,Daniel Korzekwa,Darragh Hanley,Sandip Bhaskar,Timo Roman,Karan Sapra,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron-Parse-1.1是一个轻量级文档解析和OCR模型，相比前代在OCR、Markdown格式、表格解析和图像文本提取方面有显著提升，支持更长的输出序列，采用885M参数的编码器-解码器架构。


<details>
  <summary>Details</summary>
Motivation: 提升文档解析和OCR能力，特别是在处理视觉密集文档时提供更长的输出序列，同时保持轻量级架构。

Method: 采用编码器-解码器架构，包含885M参数（其中语言解码器为256M），支持文本段边界框提取和语义分类。

Result: 在公共基准测试中达到竞争性精度，成为强大的轻量级OCR解决方案；同时发布优化版本Nemotron-Parse-1.1-TC，在减少视觉token长度的情况下实现20%速度提升且质量损失最小。

Conclusion: Nemotron-Parse-1.1是一个高效的轻量级文档解析模型，在多个任务上表现优异，已公开发布模型权重和训练数据。

Abstract: We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.

</details>


### [106] [DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning](https://arxiv.org/abs/2511.20509)
*Mihaela Hudişteanu,Edwige Cyffers,Nikita P. Kalinin*

Main category: cs.LG

TL;DR: 提出了DP-MicroAdam，一种内存高效且支持稀疏性的自适应差分隐私优化器，在多个基准测试中优于现有自适应DP优化器，并与DP-SGD竞争或超越其性能。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器在非隐私训练中是标准选择，能实现更快收敛和更好性能，但差分隐私训练仍主要使用DP-SGD，需要大量计算和超参数调优。

Method: 开发DP-MicroAdam优化器，支持内存效率和稀疏性，在随机非凸优化中证明以最优速率收敛。

Result: 在CIFAR-10、大规模ImageNet训练和预训练transformer的私有微调等基准测试中，DP-MicroAdam优于现有自适应DP优化器，与DP-SGD相比具有竞争性或更优的准确率。

Conclusion: 自适应优化在差分隐私下可以同时提高性能和稳定性。

Abstract: Adaptive optimizers are the de facto standard in non-private training as they often enable faster convergence and improved performance. In contrast, differentially private (DP) training is still predominantly performed with DP-SGD, typically requiring extensive compute and hyperparameter tuning. We propose DP-MicroAdam, a memory-efficient and sparsity-aware adaptive DP optimizer. We prove that DP-MicroAdam converges in stochastic non-convex optimization at the optimal $\mathcal{O}(1/\sqrt{T})$ rate, up to privacy-dependent constants. Empirically, DP-MicroAdam outperforms existing adaptive DP optimizers and achieves competitive or superior accuracy compared to DP-SGD across a range of benchmarks, including CIFAR-10, large-scale ImageNet training, and private fine-tuning of pretrained transformers. These results demonstrate that adaptive optimization can improve both performance and stability under differential privacy.

</details>


### [107] [Adam Simplified: Bias Correction Simplified](https://arxiv.org/abs/2511.20516)
*Sam Laing,Antonio Orvieto*

Main category: cs.LG

TL;DR: 论文通过系统实验证明Adam优化器中的偏差校正组件在最优超参数配置下对最终测试性能没有改进，有时甚至有害，挑战了该组件必须包含的传统观点。


<details>
  <summary>Details</summary>
Motivation: 研究Adam优化器中偏差校正组件的作用，该组件的重要性通常被视为理所当然但实际贡献尚不清楚。

Method: 在视觉和语言建模任务上进行系统消融实验，分析偏差校正在不同超参数配置下的影响。

Result: 在最优超参数配置下，偏差校正不会改善最终测试性能；除非实施适当的学习率调度，否则偏差校正有时会损害性能。

Conclusion: 偏差校正可视为一种隐式学习率调度，其效果强烈依赖于平滑超参数β₁、β₂的选择，研究结果挑战了该组件必须普遍包含的观点。

Abstract: The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.

</details>


### [108] [Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media](https://arxiv.org/abs/2511.20543)
*Alhasan Abdellatif,Hannah P. Menke,Ahmed H. Elsheikh,Florian Doster,Kamaljit Singh*

Main category: cs.LG

TL;DR: UFNO-FiLM通过引入FiLM层解耦标量输入和空间特征，并使用空间加权损失函数，在保持UFNO高频和低频分量优势的同时，显著提升了多相流预测精度。


<details>
  <summary>Details</summary>
Motivation: UFNO虽然通过并行UNet路径保留了高低频分量，但将标量输入作为空间分布场处理会导致频域中的冗余常数信号，且标准损失函数未考虑误差敏感性的空间变化。

Method: 1. 使用FiLM层解耦标量输入和空间特征，避免常数信号进入傅里叶变换；2. 采用空间加权损失函数，优先学习关键区域。

Result: 在地下多相流实验中，相比UFNO实现了21%的气体饱和度平均绝对误差降低。

Conclusion: UFNO-FiLM通过有效的标量输入处理和空间加权损失，显著提升了预测精度，证明了该方法在物理重要区域学习中的有效性。

Abstract: The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.

</details>


### [109] [E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems](https://arxiv.org/abs/2511.20564)
*Rui Xue,Shichao Zhu,Liang Qin,Guangmou Pan,Yang Song,Tianfu Wu*

Main category: cs.LG

TL;DR: E2E-GRec是一个端到端的图神经网络推荐训练框架，解决了传统两阶段方法的高计算开销和缺乏联合优化的问题，通过子图采样、图特征自编码器和动态损失平衡实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 传统GNN推荐系统采用两阶段流水线（预训练GNN生成嵌入，然后用于推荐系统），存在高计算开销和缺乏联合优化的问题，导致GNN对推荐任务的指导性不足。

Method: 提出E2E-GRec框架，包含三个关键组件：高效子图采样确保训练可扩展性；图特征自编码器作为自监督任务指导GNN学习结构有意义的嵌入；两级特征融合机制结合基于Gradnorm的动态损失平衡，稳定图感知多任务端到端训练。

Result: 在大规模生产数据上的离线评估和在线A/B测试显示，E2E-GRec持续超越传统方法，在多个推荐指标上获得显著提升（如停留时长相对提升0.133%，用户跳过视频数量减少0.3171%）。

Conclusion: E2E-GRec通过端到端训练统一GNN与推荐系统，解决了传统两阶段方法的关键限制，在计算效率和推荐性能上都取得了显著改进。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-trained offline to generate node embeddings, which are then used as static features for downstream recommender systems. This decoupled paradigm leads to two key limitations: (1) high computational overhead, since large-scale GNN inference must be repeatedly executed to refresh embeddings; and (2) lack of joint optimization, as the gradient from the recommender system cannot directly influence the GNN learning process, causing the GNN to be suboptimally informative for the recommendation task. In this paper, we propose E2E-GRec, a novel end-to-end training framework that unifies GNN training with the recommender system. Our framework is characterized by three key components: (i) efficient subgraph sampling from a large-scale cross-domain heterogeneous graph to ensure training scalability and efficiency; (ii) a Graph Feature Auto-Encoder (GFAE) serving as an auxiliary self-supervised task to guide the GNN to learn structurally meaningful embeddings; and (iii) a two-level feature fusion mechanism combined with Gradnorm-based dynamic loss balancing, which stabilizes graph-aware multi-task end-to-end training. Extensive offline evaluations, online A/B tests (e.g., a +0.133% relative improvement in stay duration, a 0.3171% reduction in the average number of videos a user skips) on large-scale production data, together with theoretical analysis, demonstrate that E2E-GRec consistently surpasses traditional approaches, yielding significant gains across multiple recommendation metrics.

</details>


### [110] [MSTN: Fast and Efficient Multivariate Time Series Model](https://arxiv.org/abs/2511.20577)
*Sumit S Shevtekar,Chandresh K Maurya,Gourab Sil*

Main category: cs.LG

TL;DR: 提出了多尺度时序网络（MSTN），通过分层多尺度架构自适应建模从毫秒级到长期依赖的完整时序变化谱，在多个时序任务上实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现实世界时序数据具有高度非平稳性和跨多时间尺度的复杂动态特性，现有模型依赖固定尺度结构先验导致对时序动态的过度正则化，限制了其对完整时序变化谱的自适应建模能力

Method: MSTN框架包含：多尺度卷积编码器构建分层特征金字塔、序列建模组件处理长期时序依赖、门控融合机制结合SE和多头时序注意力实现动态上下文感知特征集成

Result: 在时序长期预测、插值、分类和泛化性研究中，MSTN在32个基准数据集中的24个上建立了新的最先进性能，优于EMTSF、LLM4TS、HiMTM等当代方法

Conclusion: MSTN通过统一框架自适应建模多尺度时序模式，在多样化时序任务上展现出持续优异的性能，为未来架构发展提供了灵活基础

Abstract: Real-world time-series data is highly non stationary and complex in dynamics that operate across multiple timescales, ranging from fast, short-term changes to slow, long-term trends. Most existing models rely on fixed-scale structural priors, such as patch-based tokenization, fixed frequency transformations, or frozen backbone architectures. This often leads to over-regularization of temporal dynamics, which limits their ability to adaptively model the full spectrum of temporal variations and impairs their performance on unpredictable, Sudden, high-magnitude events. To address this, we introduce the Multi-scale Temporal Network (MSTN), a novel deep learning architecture founded on a hierarchical multi-scale and sequence modeling principle. The MSTN framework integrates: (i) a multi-scale convolutional encoder that constructs a hierarchical feature pyramid for local patterns (ii) a sequence modeling component for long-range temporal dependencies. We empirically validate this with BiLSTM and Transformer variants, establishing a flexible foundation for future architectural advancements. and (iii) a gated fusion mechanism augmented with squeeze-and-excitation (SE) and multi-head temporal attention (MHTA) for dynamic, context-aware feature integration. This design enables MSTN to adaptively model temporal patterns from milliseconds to long-range dependencies within a unified framework. Extensive evaluations across time-series long-horizon forecasting, imputation, classification and generalizability study demonstrate that MSTN achieves competitive state-of-the-art (SOTA) performance, showing improvements over contemporary approaches including EMTSF, LLM4TS, HiMTM, TIME-LLM, MTST, SOFTS, iTransformer, TimesNet, and PatchTST. In total, MSTN establishes new SOTA performance on 24 of 32 benchmark datasets, demonstrating its consistent performance across diverse temporal tasks.

</details>


### [111] [A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent](https://arxiv.org/abs/2511.20584)
*Shuo Xie,Tianhao Wang,Beining Wu,Zhiyuan Li*

Main category: cs.LG

TL;DR: 该论文揭示了自适应优化器与归一化最速下降(NSD)之间的紧密联系，建立了非凸设置下的自适应光滑性理论，并证明了自适应光滑性能够实现Nesterov动量加速，同时提出了自适应梯度方差概念用于随机优化。


<details>
  <summary>Details</summary>
Motivation: 研究自适应优化器与归一化最速下降之间的理论联系，特别是在不同几何结构下的收敛性分析差异，旨在统一两种算法家族的理论框架。

Method: 将自适应光滑性理论扩展到非凸设置，建立自适应光滑性对自适应优化器收敛性的精确刻画，并引入自适应梯度方差概念用于随机优化分析。

Result: 证明了自适应光滑性能够实现Nesterov动量加速，在凸设置下获得标准光滑性无法实现的加速保证；在随机优化中，自适应梯度方差能够实现维度无关的收敛保证。

Conclusion: 自适应光滑性和自适应梯度方差为自适应优化器提供了统一的理论框架，揭示了在非欧几里得几何下自适应优化器的独特优势，为算法设计和分析提供了新的理论基础。

Abstract: Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.

</details>


### [112] [Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models](https://arxiv.org/abs/2511.20587)
*Karim Kadry,Abdallah Abdelwahed,Shoaib Goraya,Ajay Manicka,Naravich Chutisilp,Farhad Nezami,Elazer Edelman*

Main category: cs.LG

TL;DR: Anatomica是一个推理时框架，用于生成具有局部几何拓扑控制的多类解剖体素图，通过可微惩罚函数和持续同调技术实现对解剖结构几何和拓扑特征的精确控制。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够灵活控制解剖结构几何特征（大小、形状、位置）和拓扑特征（连通分量、环、空洞）的生成框架，用于合成数据集以支持虚拟试验和机器学习工作流程。

Method: 使用不同维度、位置和形状的立方体控制域分割相关子结构，通过体素矩控制几何特征，通过持续同调控制拓扑特征，并在潜在扩散模型中实现神经场解码器部分提取子结构。

Result: Anatomica能够灵活应用于不同解剖系统，通过组合约束控制任意维度和坐标系下的复杂结构。

Conclusion: 该框架实现了对解剖体素图的精确几何拓扑控制，为合成数据集的有理设计提供了有效工具。

Abstract: We present Anatomica: an inference-time framework for generating multi-class anatomical voxel maps with localized geo-topological control. During generation, we use cuboidal control domains of varying dimensionality, location, and shape to slice out relevant substructures. These local substructures are used to compute differentiable penalty functions that steer the sample towards target constraints. We control geometric features such as size, shape, and position through voxel-wise moments, while topological features such as connected components, loops, and voids are enforced through persistent homology. Lastly, we implement Anatomica for latent diffusion models, where neural field decoders partially extract substructures, enabling the efficient control of anatomical properties. Anatomica applies flexibly across diverse anatomical systems, composing constraints to control complex structures over arbitrary dimensions and coordinate systems, thereby enabling the rational design of synthetic datasets for virtual trials or machine learning workflows.

</details>


### [113] [Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning](https://arxiv.org/abs/2511.20591)
*Charlotte Beylier,Hannah Selder,Arthur Fleig,Simon M. Hofmann,Nico Scherf*

Main category: cs.LG

TL;DR: 提出了注意力导向指标（ATOMs）来研究强化学习智能体在训练过程中的注意力发展，通过三个Pong游戏变体验证了该方法能有效区分不同注意力模式及其对应的行为差异。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的学习过程除了数学算法外仍缺乏深入理解，需要新的方法来研究智能体注意力在训练中的发展。

Method: 引入ATOMs指标，在三个设计不同的Pong游戏变体上进行控制实验，结合行为评估分析智能体注意力模式。

Result: ATOMs成功区分了不同游戏变体训练出的智能体的注意力模式，这些差异转化为行为差异；训练过程中观察到注意力按阶段发展且各变体一致。

Conclusion: ATOMs有助于更好理解强化学习智能体的学习过程以及注意力与学习之间的关系。

Abstract: The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.

</details>


### [114] [Latent Diffusion Inversion Requires Understanding the Latent Space](https://arxiv.org/abs/2511.20592)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 研究发现潜在扩散模型在潜在空间存在非均匀记忆现象，提出基于解码器回拉度量的维度排序方法，显著提升了成员推理攻击性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型反转技术主要关注数据域扩散模型，而忽略了潜在空间生成模型中的编码器/解码器对和潜在代码的作用。

Method: 提出基于解码器回拉度量的潜在维度排序方法，识别对记忆贡献最大的维度，在计算攻击统计量时移除记忆较少的维度。

Result: 在多个数据集上，该方法使成员推理攻击的AUROC平均提升2.7%，TPR@1%FPR显著增加6.42%。

Conclusion: 揭示了自编码器几何结构对LDM记忆的潜在影响，为分析扩散生成模型的隐私风险提供了新视角。

Abstract: The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\% and substantial increases in TPR@1\%FPR (6.42\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pokémon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.

</details>


### [115] [Adaptive Hopfield Network: Rethinking Similarities in Associative Memory](https://arxiv.org/abs/2511.20609)
*Shurong Wang,Yuqi Pan,Zhuoyang Shen,Meng Zhang,Hongwei Wang,Guoqi Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的关联记忆模型，通过引入自适应相似度机制来确保检索的正确性，解决了现有模型基于邻近度评估检索质量而无法保证最强关联的问题。


<details>
  <summary>Details</summary>
Motivation: 现有关联记忆模型基于邻近度评估检索质量，无法保证检索到的模式与查询具有最强关联，缺乏正确性保证。作者认为查询是存储记忆模式的生成变体，需要重新定义检索的正确性标准。

Method: 提出了自适应相似度机制，通过学习从上下文样本中近似未知的似然度，将这种机制集成到自适应Hopfield网络(A-Hop)中。

Result: 理论证明自适应相似度在三种典型变体类型（噪声、掩码和偏置）下实现最优正确检索。实验表明在记忆检索、表格分类、图像分类和多实例学习等任务中达到最先进性能。

Conclusion: 自适应相似度机制能够有效解决关联记忆中的正确检索问题，为生物智能中的内容可寻址记忆系统提供了新的理论框架和实用方法。

Abstract: Associative memory models are content-addressable memory systems fundamental to biological intelligence and are notable for their high interpretability. However, existing models evaluate the quality of retrieval based on proximity, which cannot guarantee that the retrieved pattern has the strongest association with the query, failing correctness. We reframe this problem by proposing that a query is a generative variant of a stored memory pattern, and define a variant distribution to model this subtle context-dependent generative process. Consequently, correct retrieval should return the memory pattern with the maximum a posteriori probability of being the query's origin. This perspective reveals that an ideal similarity measure should approximate the likelihood of each stored pattern generating the query in accordance with variant distribution, which is impossible for fixed and pre-defined similarities used by existing associative memories. To this end, we develop adaptive similarity, a novel mechanism that learns to approximate this insightful but unknown likelihood from samples drawn from context, aiming for correct retrieval. We theoretically prove that our proposed adaptive similarity achieves optimal correct retrieval under three canonical and widely applicable types of variants: noisy, masked, and biased. We integrate this mechanism into a novel adaptive Hopfield network (A-Hop), and empirical results show that it achieves state-of-the-art performance across diverse tasks, including memory retrieval, tabular classification, image classification, and multiple instance learning.

</details>


### [116] [Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model](https://arxiv.org/abs/2511.20636)
*Ziyue Wang,Yayati Jadhav,Peter Pak,Amir Barati Farimani*

Main category: cs.LG

TL;DR: Image2Gcode是一个端到端的数据驱动框架，直接从图像和零件图纸生成打印机就绪的G代码，绕过了传统的CAD建模阶段。


<details>
  <summary>Details</summary>
Motivation: 传统的机械设计和制造流程依赖CAD建模，这是一个主要瓶颈：构建特定对象的3D几何形状速度慢，不适合快速原型制作。即使小的设计变更也需要在CAD软件中手动更新，使得迭代耗时且难以扩展。

Method: 框架首先从图像中提取切片结构线索，然后使用去噪扩散概率模型(DDPM)处理G代码序列。通过迭代去噪，模型将高斯噪声转换为可执行的打印移动轨迹和相应的挤出参数，建立从视觉输入到原生工具路径的直接映射。

Result: 通过直接从2D图像生成结构化G代码，Image2Gcode消除了对CAD或STL中间文件的需求，降低了增材制造的门槛，加速了从设计到制造的周期。

Conclusion: 该方法支持从简单草图或视觉参考进行按需原型制作，并与上游2D到3D重建模块集成，实现从概念到物理工件的自动化流程，提高了设计迭代、修复工作流程和分布式制造的可访问性。

Abstract: Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [117] [Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder](https://arxiv.org/abs/2511.19577)
*Abhay Goyal,Navin Kumar,Kimberly DiMeola,Rafael Trujillo,Soorya Ram Shimgekar,Christian Poellabauer,Pi Zonooz,Ermonda Gjoni-Markaj,Declan Barry,Lynn Madden*

Main category: cs.AI

TL;DR: 本研究探索使用可穿戴设备和AI方法预测慢性疼痛和鸦片使用障碍患者的疼痛峰值，发现机器学习模型预测准确率较高，而大语言模型在此方面表现有限。


<details>
  <summary>Details</summary>
Motivation: 慢性疼痛和鸦片使用障碍是相互关联的常见慢性疾病，目前缺乏针对接受MOUD治疗患者的循证整合治疗方案。可穿戴设备有潜力监测复杂患者信息，但大语言模型在疼痛峰值分析中的应用尚未探索。

Method: 采用一系列AI方法，包括机器学习和大型语言模型，结合可穿戴设备数据来检查疼痛峰值的临床相关性。

Result: 机器学习模型在预测疼痛峰值方面达到相对较高的准确率（>0.7），而大语言模型在提供疼痛峰值见解方面表现有限。

Conclusion: 通过可穿戴设备的实时监测结合先进AI模型，可促进疼痛峰值的早期检测，支持个性化干预，帮助降低鸦片复吸风险，改善MOUD依从性，并增强CP和OUD护理的整合。鉴于大语言模型整体表现有限，这些发现凸显了开发能在OUD/CP背景下提供可操作见解的大语言模型的必要性。

Abstract: Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.

</details>


### [118] [Fara-7B: An Efficient Agentic Model for Computer Use](https://arxiv.org/abs/2511.19663)
*Ahmed Awadallah,Yash Lara,Raghav Magazine,Hussein Mozannar,Akshay Nambi,Yash Pandya,Aravind Rajeswaran,Corby Rosset,Alexey Taymanov,Vibhav Vineet,Spencer Whitehead,Andrew Zhao*

Main category: cs.AI

TL;DR: FaraGen是一个用于多步骤网页任务的合成数据生成系统，能够生成多样化的任务和解决方案，并以约1美元的成本生成经过验证的轨迹。基于这些数据训练的Fara-7B模型在多个基准测试中表现出色，甚至能与更大的前沿模型竞争。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理（CUAs）的发展受到缺乏大规模高质量数据集的限制，现有数据集无法充分捕捉人类与计算机的交互方式。

Method: 开发FaraGen系统，从常用网站生成多样化任务，产生多个解决方案尝试，并使用多个验证器过滤成功轨迹。基于生成的数据训练Fara-7B模型，该模型仅使用屏幕截图感知计算机，通过预测坐标执行动作。

Result: Fara-7B在WebVoyager、Online-Mind2Web和WebTailBench等基准测试中优于同类规模的CUA模型，且能与更大的前沿模型竞争。FaraGen系统能以约1美元的成本生成经过验证的轨迹。

Conclusion: 可扩展的数据生成系统在推进小型高效代理模型方面具有关键优势，Fara-7B的开源发布和WebTailBench基准的推出将促进该领域发展。

Abstract: Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.

</details>


### [119] [HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization](https://arxiv.org/abs/2511.19669)
*Souradip Poddar,Chia-Tung Ho,Ziming Wei,Weidong Cao,Haoxing Ren,David Z. Pan*

Main category: cs.AI

TL;DR: HeaRT是一个基础推理引擎，用于自动化设计循环，在电路设计优化中实现>97%的推理准确率和>98%的Pass@1性能，同时比现有方法快3倍以上。


<details>
  <summary>Details</summary>
Motivation: 传统AI驱动的AMS设计自动化算法受限于对高质量数据集的依赖、跨架构可移植性差以及缺乏自适应机制。

Method: 提出HeaRT基础推理引擎，采用智能自适应的人类风格设计优化方法。

Result: 在40个电路基准测试中，推理准确率>97%，Pass@1性能>98%，电路复杂度增加时仍保持稳定，运行时间仅为SOTA基线的<0.5倍。

Conclusion: HeaRT在各种优化方法中，在尺寸和拓扑设计适应任务中实现>3倍的收敛速度提升，同时保留先前的设计意图。

Abstract: Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.

</details>


### [120] [FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking](https://arxiv.org/abs/2511.19671)
*Rishab Sharma,Iman Saberi,Elham Alipour,Jie JW Wu,Fatemeh Fard*

Main category: cs.AI

TL;DR: FISCAL框架通过生成金融事实核查的合成数据，训练出轻量级验证器MiniCheck-FISCAL，在金融事实核查任务上超越GPT-3.5 Turbo等模型，接近更大模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前金融领域的大语言模型存在事实可靠性不足和计算效率低下的问题，需要开发更准确且高效的金融事实核查系统。

Method: 提出FISCAL框架生成金融合成数据，使用这些数据训练轻量级验证器MiniCheck-FISCAL，通过领域特定的合成数据和高效微调实现性能提升。

Result: MiniCheck-FISCAL在金融事实核查任务上超越GPT-3.5 Turbo和同类开源模型，接近Mixtral-8x22B等大20倍模型的准确性，在外部数据集上媲美GPT-4o和Claude-3.5。

Conclusion: 领域特定的合成数据结合高效微调，可使紧凑模型在金融AI应用中实现最先进的准确性、鲁棒性和可扩展性。

Abstract: Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).

</details>


### [121] [Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions](https://arxiv.org/abs/2511.19749)
*Farzan Karimi-Malekabadi,Pooya Razavi,Sonya Powers*

Main category: cs.AI

TL;DR: LLMs can accelerate educational assessment item alignment review with high accuracy, reducing manual workload while maintaining alignment quality.


<details>
  <summary>Details</summary>
Motivation: 传统人工对齐评审准确但耗时费力，特别是在大规模题库中。需要探索LLMs是否能加速这一过程而不牺牲准确性。

Method: 使用12,000多个K-5年级项目-技能对，测试了三种LLM模型在三个任务上的表现：识别错位项目、从完整标准中选择正确技能、以及在分类前缩小候选列表。

Result: GPT-4o-mini在识别对齐状态方面达到83-94%准确率；数学表现强劲但阅读较低；预过滤候选技能使正确技能出现在前五建议的概率超过95%。

Conclusion: LLMs结合候选过滤策略可显著减少人工评审负担，建议开发混合管道，将基于LLM的筛选与模糊情况下的人工评审相结合，提供可扩展的项目验证解决方案。

Abstract: As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.

</details>


### [122] [Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs](https://arxiv.org/abs/2511.19773)
*Meng Lu,Ran Xu,Yi Fang,Wenxuan Zhang,Yue Yu,Gaurav Srivastava,Yuchen Zhuang,Mohamed Elhoseiny,Charles Fleming,Carl Yang,Zhengzhong Tu,Yang Xie,Guanghua Xiao,Hanrui Wang,Di Jin,Wenqi Shi,Xuan Wang*

Main category: cs.AI

TL;DR: VISTA-Gym是一个可扩展的训练环境，通过统一多模态推理任务和标准化视觉工具接口，为视觉语言模型提供工具集成视觉推理能力的训练。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在图像理解方面表现良好，但在多步骤视觉交互推理方面能力有限，需要提升工具选择、调用和协调的能力。

Method: 开发VISTA-Gym环境，统一7个任务的13个数据集，提供标准化视觉工具接口、可执行交互循环和可验证反馈信号。通过多轮轨迹采样和端到端强化学习训练VISTA-R1模型。

Result: 在11个公共推理密集型VQA基准测试中，VISTA-R1-8B模型比相似规模的现有最优基线模型性能提升9.51%-18.72%。

Conclusion: VISTA-Gym是一个有效的训练平台，能够解锁视觉语言模型的工具集成推理能力。

Abstract: While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.

</details>


### [123] [NOEM$^{3}$A: A Neuro-Symbolic Ontology-Enhanced Method for Multi-Intent Understanding in Mobile Agents](https://arxiv.org/abs/2511.19780)
*Ioannis Tzachristas,Aifen Sui*

Main category: cs.AI

TL;DR: 提出了一种神经符号框架，通过将结构化意图本体与紧凑语言模型集成，实现移动AI代理的多意图理解。该方法利用检索增强提示、logit偏置和可选分类头，将符号意图结构注入输入和输出表示中。


<details>
  <summary>Details</summary>
Motivation: 解决移动AI代理中多意图理解的挑战，通过符号对齐策略实现准确高效的设备端自然语言理解，同时降低能耗和内存占用。

Method: 集成结构化意图本体与紧凑语言模型，采用检索增强提示、logit偏置和可选分类头，将符号意图结构注入模型输入和输出表示。

Result: 在MultiWOZ 2.3的模糊/复杂对话子集上，3B参数的Llama模型通过本体增强接近GPT-4的准确率（85% vs 90%），同时显著降低能耗和内存占用。定性比较显示本体增强模型产生更接地气、消歧的多意图解释。

Conclusion: 符号对齐是实现在设备端进行准确高效自然语言理解的有效策略，验证了神经符号框架在移动AI代理多意图理解中的价值。

Abstract: We introduce a neuro-symbolic framework for multi-intent understanding in mobile AI agents by integrating a structured intent ontology with compact language models. Our method leverages retrieval-augmented prompting, logit biasing and optional classification heads to inject symbolic intent structure into both input and output representations. We formalize a new evaluation metric-Semantic Intent Similarity (SIS)-based on hierarchical ontology depth, capturing semantic proximity even when predicted intents differ lexically. Experiments on a subset of ambiguous/demanding dialogues of MultiWOZ 2.3 (with oracle labels from GPT-o3) demonstrate that a 3B Llama model with ontology augmentation approaches GPT-4 accuracy (85% vs 90%) at a tiny fraction of the energy and memory footprint. Qualitative comparisons show that ontology-augmented models produce more grounded, disambiguated multi-intent interpretations. Our results validate symbolic alignment as an effective strategy for enabling accurate and efficient on-device NLU.

</details>


### [124] [KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)](https://arxiv.org/abs/2511.19798)
*Weizhi Liu,Xi Chen,Zekun Jiang,Liang Zhao,Kunyuan Jiang,Ruisi Tang,Li Wang,Mingke You,Hanyu Zhou,Hongyu Chen,Qiankun Xiong,Yong Nie,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: KOM是一个用于膝骨关节炎(KOA)管理的多智能体系统，能够自动化评估、风险预测和治疗处方，在资源有限环境中提升医疗效率。


<details>
  <summary>Details</summary>
Motivation: 膝骨关节炎影响全球6亿多人，个性化多学科干预虽然有效但需要大量医疗资源，难以在资源有限环境中实施。

Method: 开发KOM多智能体系统，自动化KOA评估、风险预测和治疗处方，基于患者个体特征、疾病状态、风险因素和禁忌症生成定制管理计划。

Result: 基准实验显示KOM在影像分析和处方生成方面优于通用大语言模型；三臂随机模拟研究表明KOM与临床医生协作减少诊断和规划时间38.5%，并提升治疗质量。

Conclusion: KOM有助于实现自动化KOA管理，整合到临床工作流程可提升护理效率，其模块化架构为其他慢性病AI辅助管理系统开发提供参考。

Abstract: Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.

</details>


### [125] [A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization](https://arxiv.org/abs/2511.19829)
*Ke Chen,Yifeng Wang,Hassan Almosapeeh,Haohan Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于评估指导的提示优化方法，通过建立系统化的提示评估框架和免执行的评估器，实现了稳定、可解释且模型无关的提示优化。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法存在局限性：静态模板方法无法适应复杂动态场景，查询依赖方法依赖不稳定文本反馈或黑盒奖励模型，缺乏统一的提示质量定义导致评估信号碎片化。

Method: 首先建立面向性能的系统化提示评估框架，开发免执行评估器直接预测多维质量分数，然后通过评估器指导可解释的查询依赖优化器诊断失败模式并重写提示。

Result: 评估器在预测提示性能方面达到最高准确率，评估指导的优化在8个数据集和3个骨干模型上持续超越静态模板和查询依赖基线方法。

Conclusion: 提出了统一的基于指标的提示质量视角，证明了评估指导的优化管道能够跨多样化任务提供稳定、可解释且模型无关的改进。

Abstract: Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.

</details>


### [126] [Reinforcement Learning with $ω$-Regular Objectives and Constraints](https://arxiv.org/abs/2511.19849)
*Dominik Wagner,Leon Witzman,Luke Ong*

Main category: cs.AI

TL;DR: 提出一种结合ω-正则目标与显式约束的强化学习方法，分别处理安全要求和优化目标，避免单一标量评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励，难以表达时间性、条件性或安全关键目标，且容易导致奖励黑客行为。ω-正则目标能表达更丰富的行为属性，但单一标量评估掩盖了安全-性能权衡。

Method: 开发基于线性规划的模型强化学习算法，在极限情况下产生满足ω-正则约束阈值的同时最大化ω-正则目标满足概率的策略，并建立到约束极限平均问题的转换。

Result: 算法在理论上保证能产生满足约束的最优策略，并建立了与约束极限平均问题的等价转换关系。

Conclusion: 该方法成功解决了强化学习中安全约束与优化目标分离的问题，为复杂环境下的安全强化学习提供了理论框架。

Abstract: Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.

</details>


### [127] [MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support](https://arxiv.org/abs/2511.19864)
*Valerie Lockhart,Dan McCreary,Troy A. Peterson*

Main category: cs.AI

TL;DR: MicroSims是一个基于AI的轻量级教育模拟框架，能够快速生成可嵌入任何数字学习平台的交互式模拟，无需编程知识即可定制。


<details>
  <summary>Details</summary>
Motivation: 传统教育模拟创建需要大量资源和技术专长，限制了其广泛应用。MicroSims旨在解决成本、技术复杂性和平台依赖等障碍，促进教育公平。

Method: 采用标准化设计模式支持AI辅助生成，iframe架构实现通用嵌入和沙盒安全，透明可修改代码支持定制和教学透明度。

Result: 基于物理教育研究和STEM元分析，交互式模拟可将概念理解提升30-40%。MicroSims在保持这些益处的同时降低了准入门槛。

Conclusion: MicroSims为全球教育工作者提供了按需创建定制化、课程对齐模拟的能力，为AI驱动的自适应学习系统奠定了基础。

Abstract: Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.

</details>


### [128] [Agentic AI-Empowered Conversational Embodied Intelligence Networks in 6G](https://arxiv.org/abs/2511.19865)
*Mingkai Chen,Zijie Feng,Lei Wang,Yaser Khamayseh*

Main category: cs.AI

TL;DR: 提出了协作对话具身智能网络（CC-EIN），通过多模态特征融合、自适应语义通信、任务协调和可解释性机制，解决6G时代多具身智能设备在复杂任务执行中的协作挑战。


<details>
  <summary>Details</summary>
Motivation: 在6G时代，多具身智能设备（MEIDs）之间的语义协作对复杂任务执行至关重要，但现有系统在多模态信息融合、自适应通信和决策可解释性方面面临挑战。

Method: CC-EIN包含四个核心模块：PerceptiNet进行图像和雷达数据的跨模态融合生成统一语义表示；自适应语义通信策略根据任务紧急程度和信道质量动态调整编码方案和传输功率；语义驱动协作机制支持异构设备间的任务分解和无冲突协调；InDec模块通过Grad-CAM可视化增强决策透明度。

Result: 在地震后救援场景的仿真中，CC-EIN实现了95.4%的任务完成率和95%的传输效率，同时保持了强大的语义一致性和能源效率。

Conclusion: CC-EIN有效解决了多具身智能设备协作中的关键挑战，为6G时代的智能协作系统提供了可行的解决方案。

Abstract: In the 6G era, semantic collaboration among multiple embodied intelligent devices (MEIDs) becomes crucial for complex task execution. However, existing systems face challenges in multimodal information fusion, adaptive communication, and decision interpretability. To address these limitations, we propose a collaborative Conversational Embodied Intelligence Network (CC-EIN) integrating multimodal feature fusion, adaptive semantic communication, task coordination, and interpretability. PerceptiNet performs cross-modal fusion of image and radar data to generate unified semantic representations. An adaptive semantic communication strategy dynamically adjusts coding schemes and transmission power according to task urgency and channel quality. A semantic-driven collaboration mechanism further supports task decomposition and conflict-free coordination among heterogeneous devices. Finally, the InDec module enhances decision transparency through Grad-CAM visualization. Simulation results in post-earthquake rescue scenarios demonstrate that CC-EIN achieves 95.4% task completion rate and 95% transmission efficiency while maintaining strong semantic consistency and energy efficiency.

</details>


### [129] [Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy](https://arxiv.org/abs/2511.19872)
*Daniel I Jackson,Emma L Jensen,Syed-Amad Hussain,Emre Sezgin*

Main category: cs.AI

TL;DR: 本研究将通用自我效能感量表(GSES)应用于10个大语言模型，发现在不同任务条件下模型的自我评估存在显著差异，但自我评估并不能可靠反映实际能力，高自我效能感与拟人化推理风格相关。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型的评估主要关注任务准确性，而忽视了自我评估这一可靠智能的关键方面。本研究旨在探究LLM的模拟自我评估能力及其与实际表现的关系。

Method: 将10项通用自我效能感量表(GSES)改编后应用于10个LLM，在四种条件下进行测试：无任务、计算推理、社会推理和摘要任务，分析自我评估的稳定性、准确性以及与表现的关系。

Result: GSES响应在重复测试和随机项目顺序下高度稳定，但模型在不同条件下的自我效能感水平差异显著，总体得分低于人类标准。所有模型在计算和社会问题上都达到完美准确率，而摘要表现差异很大。自我评估不能可靠反映能力，后续置信度提示导致适度下调。

Conclusion: 心理测量提示为LLM通信行为提供了结构化洞察，但不能提供校准的性能估计。高自我效能感与更自信、拟人化的推理风格相关，而低分则反映了谨慎、去拟人化的解释。

Abstract: Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.

</details>


### [130] [RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2511.19895)
*Yuanyuan Lin,Xiangyu Ouyang,Teng Zhang,Kaixin Sui*

Main category: cs.AI

TL;DR: RPM-MCTS是一种基于蒙特卡洛树搜索的代码生成方法，通过知识检索作为过程奖励模型来评估中间算法步骤，无需复杂训练，能定位和纠正错误步骤，减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统树搜索方法在代码生成中难以有效评估中间算法步骤，无法及时定位和纠正错误步骤，导致生成错误代码和计算成本增加。

Method: 使用知识检索作为过程奖励模型，在扩展阶段采用相似性过滤移除冗余节点，利用沙箱执行反馈定位错误步骤并进行针对性纠正。

Result: 在四个公共代码生成基准测试中优于现有最优方法，token消耗减少约15%，使用RPM-MCTS构建的数据对基础模型进行全微调可显著提升代码能力。

Conclusion: RPM-MCTS通过知识检索和沙箱反馈有效解决了中间步骤评估和错误纠正问题，在提升代码生成质量的同时降低了计算成本。

Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.

</details>


### [131] [Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity](https://arxiv.org/abs/2511.19925)
*Qiyao Wei,Edward Morrell,Lea Goetz,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 该论文提出了一种基于知识图谱生成基准数据集的新方法，用于评估LLM输出语义相似度方法，解决了现有基准依赖人工标注、成本高、领域适用性有限等问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM文本输出语义相似度的方法存在缺陷：可能捕捉语法而非语义内容，基准数据集生成成本高，依赖主观人工判断，领域适用性有限，语义等价定义不明确。

Method: 利用知识图谱生成语义相似或不相似的自然语言陈述对，将不相似对分为四种子类型，在四个不同领域（通用知识、生物医学、金融、生物学）生成基准数据集，比较传统NLP评分和LLM-as-a-judge预测方法。

Result: 语义变化子类型和基准领域都会影响语义相似度方法的性能，没有哪种方法始终表现最优。LLM-as-a-judge在检测文本语义内容方面存在重要影响。

Conclusion: 提出的基于知识图谱的基准生成方法有效解决了现有基准的局限性，为评估LLM输出语义相似度提供了更可靠的工具，揭示了不同语义相似度方法的性能差异和局限性。

Abstract: Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.

</details>


### [132] [A System-Level Taxonomy of Failure Modes in Large Language Model Applications](https://arxiv.org/abs/2511.19933)
*Vaishali Vinay*

Main category: cs.AI

TL;DR: 本文提出了一个系统级的LLM隐藏故障模式分类法，分析了现有评估方法的不足，并提供了构建可靠LLM系统的设计原则。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被快速集成到决策支持工具和自动化工作流中，其在生产环境中的行为仍未被充分理解，且故障模式与传统机器学习模型有根本差异。

Method: 提出了包含15种隐藏故障模式的系统级分类法，分析现有评估与监控实践的差距，并研究部署LLM的生产挑战。

Result: 识别了多步推理漂移、潜在不一致性、上下文边界退化、错误工具调用、版本漂移和成本驱动性能崩溃等关键故障模式。

Conclusion: 将LLM可靠性框架化为系统工程问题而非纯模型中心问题，为未来评估方法、AI系统鲁棒性和可靠LLM部署研究提供分析基础。

Abstract: Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.

</details>


### [133] [M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19969)
*Weizi Shao,Taolin Zhang,Zijie Zhou,Chen Chen,Chengyu Wang,Xiaofeng He*

Main category: cs.AI

TL;DR: 提出M$^3$Prune框架，通过层次化通信图剪枝消除多模态多代理系统中的冗余边，在保持性能的同时显著降低token开销。


<details>
  <summary>Details</summary>
Motivation: 现有多代理系统存在显著的token开销和计算成本问题，阻碍大规模部署，需要找到任务性能和token开销之间的最佳平衡。

Method: 采用层次化通信图剪枝：先进行模态内图稀疏化识别关键边，然后构建动态通信拓扑进行模态间图稀疏化，最后逐步剪枝冗余边获得高效层次化拓扑。

Result: 在通用和领域特定的mRAG基准测试中，该方法始终优于单代理和鲁棒多代理系统，同时显著减少token消耗。

Conclusion: M$^3$Prune框架有效解决了多模态多代理系统的效率问题，实现了性能与开销的优化平衡。

Abstract: Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.

</details>


### [134] [Reducing Latency of LLM Search Agent via Speculation-based Algorithm-System Co-Design](https://arxiv.org/abs/2511.20048)
*Zixiao Huang,Wen Zeng,Tianyu Fu,Tengxuan Liu,Yizhou Sun,Ke Hong,Xinhao Yang,Chengchun Liu,Yan Li,Quanlu Zhang,Guohao Dai,Zhenhua Zhu,Yu Wang*

Main category: cs.AI

TL;DR: SPAgent通过算法-系统协同设计框架，引入两阶段自适应推测机制和两级调度器，显著减少LLM搜索代理的延迟，实现1.65倍端到端加速同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: LLM搜索代理性能强大但延迟严重，因为每个步骤都需要串行LLM推理和工具执行。传统推测范式虽然能打破串行执行，但收益有限，因为它保留了完整原始工作负载并增加了额外推理开销。

Method: SPAgent采用算法-系统协同设计：算法层面引入两阶段自适应推测机制，在安全时选择性省略验证；系统层面使用两级调度器基于引擎负载调节推测请求，确保推测始终有益。

Result: 在广泛实验设置中，SPAgent实现了高达1.65倍的端到端加速，同时保持相同甚至更高的准确性，使多步骤搜索代理的实际部署成为可能。

Conclusion: SPAgent通过扩展推测在搜索代理中的作用，成功解决了LLM搜索代理的延迟瓶颈，为多步骤搜索代理的实际部署提供了可行方案。

Abstract: LLM-based search agents achieve strong performance but suffer from severe latency, as each step requires serialized LLM reasoning followed by action of tool execution. We revisit this bottleneck through the lens of speculation. While traditional predict-verify speculation paradigm can break serial execution, its benefit remains limited, as it retains the full original workload and adds extra inference overhead. We observe that early agent steps often involve simple evidence-gathering, where correct actions can often be predicted without full reasoning. Building on these observations, we present SPAgent, an algorithm-system co-design framework that expands the role of speculation in search agents to reduce latency. Algorithmically, SPAgent introduces a two-phase adaptive speculation mechanism that selectively omits verification when safe. System-wise, a two-level scheduler regulates speculative requests based on engine load to ensure speculation remains beneficial. We implement SPAgent in real-world systems. Across extensive experimental settings, SPAgent achieves up to $1.65\times$ end-to-end speedup while maintaining same or even achieving higher accuracy, enabling practical deployment of multi-step search agents.

</details>


### [135] ["Are We Done Yet?": A Vision-Based Judge for Autonomous Task Completion of Computer Use Agents](https://arxiv.org/abs/2511.20067)
*Marta Sumyk,Oleksandr Kosovan*

Main category: cs.AI

TL;DR: 提出基于视觉语言模型的自主评估框架，通过屏幕截图直接评估计算机使用代理的任务完成情况，在macOS应用上实现73%的检测准确率，使任务成功率相对提升27%。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理经常无法可靠判断任务是否完成，需要有效的评估和反馈机制来提高其自主性和可靠性。

Method: 使用视觉语言模型从屏幕截图和任务描述直接评估任务完成情况，构建包含42个macOS应用和1260个人工标注任务的数据集。

Result: 任务成功检测准确率达到73%，应用评估器反馈后整体任务成功率相对提升27%。

Conclusion: 基于视觉的评估可作为有效的反馈机制，显著提升自主计算机使用代理的可靠性和自我纠正能力。

Abstract: Computer Use Agents (CUAs) are designed to autonomously operate digital interfaces, yet they often fail to reliably determine whether a given task has been completed. We present an autonomous evaluation and feedback framework that uses vision-language models to assess task completion directly from screenshots and task descriptions. Our dataset covers 42 built-in macOS applications and 1,260 human-labeled tasks across a wide range of scenarios. Our framework achieves up to 73 percent accuracy in task success detection and yields an average relative improvement of 27 percent in overall task success when evaluator feedback is applied. These results show that vision-based evaluation can serve as an effective feedback mechanism that improves the reliability and self-correction of autonomous computer-use agents.

</details>


### [136] [VICoT-Agent: A Vision-Interleaved Chain-of-Thought Framework for Interpretable Multimodal Reasoning and Scalable Remote Sensing Analysis](https://arxiv.org/abs/2511.20085)
*Chujie Wang,Zhiyuan Luo,Ruiqi Liu,Can Ran,Shenghua Fan,Xi Chen,Chu He*

Main category: cs.AI

TL;DR: 提出了VICoT多模态智能体框架，通过动态整合视觉工具到思维链中实现显式多轮推理，在遥感图像分析任务中显著优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 遥感图像分析任务正从传统目标识别向复杂智能推理演进，需要更强的推理能力和灵活的工具调用能力。

Method: 采用基于栈的推理结构和模块化MCP兼容工具套件，使LLM能够高效执行多轮交错的视觉语言推理任务；提出推理栈蒸馏方法将复杂Agent行为迁移到轻量级模型。

Result: 在多个遥感基准测试中，VICoT在推理透明度、执行效率和生成质量方面显著优于现有SOTA框架。

Conclusion: VICoT框架通过显式多轮推理和工具动态整合，为复杂遥感图像分析提供了有效的解决方案，并通过蒸馏方法实现了推理能力的轻量化迁移。

Abstract: The current remote sensing image analysis task is increasingly evolving from traditional object recognition to complex intelligence reasoning, which places higher requirements on the model's reasoning ability and the flexibility of tool invocation. To this end, we propose a new multimodal agent framework, Vision-Interleaved Chain-of-Thought Framework (VICoT), which implements explicit multi-round reasoning by dynamically incorporating visual tools into the chain of thought. Through a stack-based reasoning structure and a modular MCP-compatible tool suite, VICoT enables LLMs to efficiently perform multi-round, interleaved vision-language reasoning tasks with strong generalization and flexibility.We also propose the Reasoning Stack distillation method to migrate complex Agent behaviors to small, lightweight models, which ensures the reasoning capability while significantly reducing complexity. Experiments on multiple remote sensing benchmarks demonstrate that VICoT significantly outperforms existing SOTA frameworks in reasoning transparency, execution efficiency, and generation quality.

</details>


### [137] [From data to concepts via wiring diagrams](https://arxiv.org/abs/2511.20138)
*Jason Lo,Mohammadnima Jafari*

Main category: cs.AI

TL;DR: 本文提出了准骨架接线图的概念，证明了其与Hasse图的对应关系，并设计了从顺序数据中提取接线图的算法。该算法在分析自主代理玩电脑游戏的行为时成功识别了获胜策略。


<details>
  <summary>Details</summary>
Motivation: 接线图是表示抽象概念（如时间过程）的有向标记图，需要开发从顺序数据中自动提取接线图的方法。

Method: 引入准骨架接线图概念，证明其与Hasse图的对应关系，设计基于此的算法从顺序数据中提取接线图，并与DBSCAN和凝聚层次聚类算法进行比较。

Result: 算法在分析自主代理游戏行为时正确识别了获胜策略，在数据扰动情况下表现优于传统聚类方法。

Conclusion: 本文成功整合了范畴论、图论、聚类、强化学习和数据工程的技术，为从顺序数据中提取接线图提供了有效方法。

Abstract: A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.

</details>


### [138] [Towards Benign Memory Forgetting for Selective Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2511.20196)
*Zhen Zeng,Leijiang Gu,Zhangling Duan,Feng Li,Zenglin Shi,Cees G. M. Snoek,Meng Wang*

Main category: cs.AI

TL;DR: 提出了SMFA方法，通过记忆遗忘适配器和保留锚点引导的掩码机制，实现多模态大语言模型对隐私敏感信息的精确可控遗忘，同时保持模型的通用图像理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在移除隐私敏感信息时，往往会损害模型的通用图像理解性能，无法实现良性遗忘。

Method: SMFA首先微调模型将敏感响应替换为拒绝回答，生成记忆遗忘适配器，然后应用保留锚点引导的掩码机制来避免对无关知识和理解能力的干扰。

Result: 实验表明，与现有方法不同，SMFA能够实现精确可控的遗忘，同时保持模型的基础图像理解能力。

Conclusion: SMFA方法在多模态大语言模型选择性遗忘方面取得了突破，能够有效移除敏感知识而不损害通用视觉理解能力。

Abstract: Multimodal Large Language Models (MLLMs) achieve remarkable capabilities but can inadvertently memorize privacy-sensitive information. Although existing unlearning methods can remove such knowledge, they fail to achieve benign forgetting because they often degrade the model's general image understanding performance. To address this, we propose the Sculpted Memory Forgetting Adapter (SMFA), which confines forgetting to targeted memory regions while preserving overall capabilities. SMFA first fine-tunes the model to replace sensitive responses with refusals, yielding a memory forgetting adapter, and then applies a retaining anchor-guided masking mechanism to prevent interference with unrelated knowledge and understanding ability. To systematically evaluate selective MLLM unlearning, we introduce S-MLLMUn Bench, the first benchmark designed to jointly assess the removal of sensitive knowledge and retention of general visual understanding. Extensive experiments show that, unlike prior methods, SMFA achieves precise and controllable unlearning while maintaining the model's foundational image understanding.

</details>


### [139] [Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025](https://arxiv.org/abs/2511.20200)
*Yitian Huang,Yuxuan Lei,Jianxun Lian,Hao Liao*

Main category: cs.AI

TL;DR: 团队MSRA_SC在CPDC 2025竞赛中提出了一个统一框架，通过上下文工程和GRPO训练在两个赛道都取得了优异成绩，包括API赛道任务2第一名、任务1第二名等。


<details>
  <summary>Details</summary>
Motivation: 解决常识人物对话挑战中工具调用稳定性、执行可靠性和角色扮演指导的问题，同时避免小样本过拟合。

Method: 1. 上下文工程：动态工具剪枝和人物剪裁进行输入压缩，结合参数归一化和函数合并等后处理技术；2. GPU赛道采用GRPO训练，用强化学习替代监督微调。

Result: 最终评估中：API赛道任务2排名第1，任务1排名第2，任务3和GPU赛道均排名第3。

Conclusion: 提出的简单而有效的框架在两个赛道都表现出色，证明了该方法在常识人物对话任务中的有效性。

Abstract: This report presents the solution and results of our team MSRA\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution

</details>


### [140] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是首个将导航性能与商业可行性结合的微导航经济测试平台，通过成本收益分析揭示传统导航指标与商业部署之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有导航基准只关注任务成功率，忽略了经济可行性这一对商业自动驾驶配送机器人部署至关重要的因素。

Method: CostNav建立完整的成本收益模型，包括硬件、训练、能源、维护成本和配送收入，使用行业参数从缩小规模模拟扩展到实际配送场景。

Result: 基准方法在SLA合规性上达到43.0%，但商业上不可行：每次运行亏损30.009美元，且无盈亏平衡点，99.7%的运行成本来自碰撞引发的维护。

Conclusion: CostNav填补了导航研究与商业部署之间的鸿沟，为评估基于规则的导航、模仿学习和成本感知强化学习提供了基础，使跨导航范式的经济权衡决策数据化。

Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.

</details>


### [141] [Actionable and diverse counterfactual explanations incorporating domain knowledge and causal constraints](https://arxiv.org/abs/2511.20236)
*Szymon Bobek,Łukasz Bałec,Grzegorz J. Nalepa*

Main category: cs.AI

TL;DR: 提出了DANCE方法，通过整合特征依赖关系和因果约束来生成多样、可操作且知识约束的反事实解释，确保反事实的合理性和现实可行性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法往往忽略现实数据集中的复杂依赖关系，导致生成不现实或不可行的修改建议。受网络安全和电子邮件营销应用的启发，需要确保反事实的合理性和可操作性。

Method: DANCE方法从数据中学习线性和非线性约束，或整合专家提供的依赖图，确保反事实的合理性和可操作性。通过保持特征关系的一致性，生成符合现实约束的解释，并平衡合理性、多样性和稀疏性。

Result: 在140个公共数据集上的广泛评估表明，该方法能够生成有意义、领域相关的反事实解释，在广泛使用的指标上优于现有方法。

Conclusion: DANCE方法有效解决了现有算法在特征依赖关系处理方面的关键限制，能够生成既合理又可操作的反事实解释，在现实应用中具有重要价值。

Abstract: Counterfactual explanations enhance the actionable interpretability of machine learning models by identifying the minimal changes required to achieve a desired outcome of the model. However, existing methods often ignore the complex dependencies in real-world datasets, leading to unrealistic or impractical modifications. Motivated by cybersecurity applications in the email marketing domain, we propose a method for generating Diverse, Actionable, and kNowledge-Constrained Explanations (DANCE), which incorporates feature dependencies and causal constraints to ensure plausibility and real-world feasibility of counterfactuals. Our method learns linear and nonlinear constraints from data or integrates expert-provided dependency graphs, ensuring counterfactuals are plausible and actionable. By maintaining consistency with feature relationships, the method produces explanations that align with real-world constraints. Additionally, it balances plausibility, diversity, and sparsity, effectively addressing key limitations in existing algorithms. The work is developed based on a real-life case study with Freshmail, the largest email marketing company in Poland and supported by a joint R&D project Sendguard. Furthermore, we provide an extensive evaluation using 140 public datasets, which highlights its ability to generate meaningful, domain-relevant counterfactuals that outperform other existing approaches based on widely used metrics. The source code for reproduction of the results can be found in a GitHub repository we provide.

</details>


### [142] [SMoG: Schema Matching on Graph](https://arxiv.org/abs/2511.20285)
*Mingyu Jeon,Jaeyoung Suh,Suwan Cho*

Main category: cs.AI

TL;DR: SMoG是一个基于知识图谱的模式匹配框架，通过迭代执行简单的1跳SPARQL查询来解决医疗领域EHR系统与OMOP CDM标准模型的对齐问题，在保持性能的同时提高了可解释性和效率。


<details>
  <summary>Details</summary>
Motivation: 在医疗数据集成中，大语言模型存在幻觉和缺乏最新领域知识的问题，而现有知识图谱增强方法依赖复杂多跳查询或存储密集的向量检索方法，效率低下。

Method: 提出SMoG框架，采用迭代执行简单1跳SPARQL查询的策略，直接从SPARQL端点查询知识图谱，生成可验证的查询路径。

Result: 在真实医疗数据集上的实验表明，SMoG达到了与最先进基线方法相当的性能，同时显著降低了存储需求。

Conclusion: SMoG通过简单高效的1跳查询策略，在知识图谱增强的模式匹配中实现了有效性和效率的平衡，提高了可解释性和可靠性。

Abstract: Schema matching is a critical task in data integration, par- ticularly in the medical domain where disparate Electronic Health Record (EHR) systems must be aligned to standard models like OMOP CDM. While Large Language Models (LLMs) have shown promise in schema matching, they suf- fer from hallucination and lack of up-to-date domain knowl- edge. Knowledge Graphs (KGs) offer a solution by pro- viding structured, verifiable knowledge. However, existing KG-augmented LLM approaches often rely on inefficient complex multi-hop queries or storage-intensive vector-based retrieval methods. This paper introduces SMoG (Schema Matching on Graph), a novel framework that leverages iter- ative execution of simple 1-hop SPARQL queries, inspired by successful strategies in Knowledge Graph Question An- swering (KGQA). SMoG enhances explainability and relia- bility by generating human-verifiable query paths while sig- nificantly reducing storage requirements by directly querying SPARQL endpoints. Experimental results on real-world med- ical datasets demonstrate that SMoG achieves performance comparable to state-of-the-art baselines, validating its effec- tiveness and efficiency in KG-augmented schema matching.

</details>


### [143] [Improving Language Agents through BREW](https://arxiv.org/abs/2511.20297)
*Shashank Kirtania,Param Biyani,Priyanshu Gupta,Yasharth Bajpai,Roshni Iyer,Sumit Gulwani,Gustavo Soares*

Main category: cs.AI

TL;DR: BREW框架通过构建和精化经验学习知识库来优化智能体性能，相比传统权重优化方法更高效、可解释且可扩展。


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO和GRPO等权重优化的智能体训练方法计算开销大、收敛困难，且生成的策略难以解释、适应或增量改进。

Method: 提出BREW框架，通过知识库构建和精化来优化智能体，采用有效的记忆分区方法提高检索效率，利用任务评分器和行为准则学习洞察，并通过状态空间搜索确保鲁棒性。

Result: 在OSWorld、τ²Bench和SpreadsheetBench等真实世界基准测试中，BREW实现任务精度提升10-20%，API/工具调用减少10-15%，执行时间更快，同时保持与基础模型相当的计算效率。

Conclusion: 将知识库确立为模块化、可控的智能体优化基础，为行为塑造提供了透明、可解释和可扩展的显式机制。

Abstract: Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\%$ improvement in task precision, $10-15\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.

</details>


### [144] [Data Augmentation Techniques to Reverse-Engineer Neural Network Weights from Input-Output Queries](https://arxiv.org/abs/2511.20312)
*Alexander Beiser,Flavio Martinelli,Wulfram Gerstner,Johanni Brea*

Main category: cs.AI

TL;DR: 本文提出了一种新的数据增强技术，用于在教师-学生设置中更好地恢复网络参数，即使教师网络的参数数量远超训练数据点。


<details>
  <summary>Details</summary>
Motivation: 现有方法在教师网络参数数量超过训练数据时失败，因为学生会过度拟合查询数据而不是对齐参数。需要更好的方法来采样教师网络的输入-输出映射。

Method: 设计了针对网络隐藏层表示空间的新数据增强技术，而不是使用标准的旋转、翻转和添加噪声等增强方法。

Result: 新增强技术扩展了可恢复网络大小的最先进范围，能够恢复参数数量比训练数据点多100倍的网络。

Conclusion: 专门针对网络隐藏层表示空间设计的增强技术比标准增强方法更有效，能够显著提高网络参数恢复的能力。

Abstract: Network weights can be reverse-engineered given enough informative samples of a network's input-output function. In a teacher-student setup, this translates into collecting a dataset of the teacher mapping -- querying the teacher -- and fitting a student to imitate such mapping. A sensible choice of queries is the dataset the teacher is trained on. But current methods fail when the teacher parameters are more numerous than the training data, because the student overfits to the queries instead of aligning its parameters to the teacher. In this work, we explore augmentation techniques to best sample the input-output mapping of a teacher network, with the goal of eliciting a rich set of representations from the teacher hidden layers. We discover that standard augmentations such as rotation, flipping, and adding noise, bring little to no improvement to the identification problem. We design new data augmentation techniques tailored to better sample the representational space of the network's hidden layers. With our augmentations we extend the state-of-the-art range of recoverable network sizes. To test their scalability, we show that we can recover networks of up to 100 times more parameters than training data-points.

</details>


### [145] [Active Inference in Discrete State Spaces from First Principles](https://arxiv.org/abs/2511.20321)
*Patrick Kenny*

Main category: cs.AI

TL;DR: 本文澄清了主动推理与自由能原理的关系，展示了在离散状态空间中实现主动推理的优化方法，提出了基于约束散度最小化的替代框架。


<details>
  <summary>Details</summary>
Motivation: 澄清主动推理概念，将其与自由能原理分离，提供不依赖期望自由能的实现方法。

Method: 将主动推理优化问题表述为约束散度最小化问题，使用标准平均场方法求解，避免使用期望自由能概念。

Result: 提出的感知/行动散度准则在建模感知时与变分自由能一致，在建模行动时与期望自由能泛函相差一个熵正则化项。

Conclusion: 主动推理可以在不依赖自由能原理的情况下实现，通过约束散度最小化框架提供更清晰的数学基础。

Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.

</details>


### [146] [NNGPT: Rethinking AutoML with Large Language Models](https://arxiv.org/abs/2511.20333)
*Roman Kochnev,Waleed Khalid,Tolgay Atinc Uzun,Xi Zhang,Yashkumar Sanjaybhai Dhameliya,Furui Qin,Chandini Vysyaraju,Raghuvir Duvvuri,Avi Goyal,Dmitry Ignatov,Radu Timofte*

Main category: cs.AI

TL;DR: NNGPT是一个开源框架，将大型语言模型转变为用于神经网络开发的自改进AutoML引擎，通过生成新模型、评估和自我改进的闭环系统实现持续优化。


<details>
  <summary>Details</summary>
Motivation: 构建自改进的AI系统是AI领域的根本挑战，现有框架无法通过生成新模型来扩展神经网络数据集，限制了持续学习能力。

Method: 集成五个协同的LLM管道：零样本架构合成、超参数优化、代码感知精度预测、检索增强的PyTorch块合成和强化学习，基于LEMUR数据集构建闭环系统。

Result: NN-RAG在1,289个目标上达到73%可执行性，3-shot提示提升常见数据集精度，HPO在LEMUR上RMSE 0.60优于Optuna，已生成超过5K验证模型。

Conclusion: NNGPT被证明是一个自主的AutoML引擎，能够显著减少试验需求，代码和检查点将公开发布以促进可复现性和社区使用。

Abstract: Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.

</details>


### [147] [VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning](https://arxiv.org/abs/2511.20422)
*Bo Pang,Chenxi Xu,Jierui Ren,Guoping Wang,Sheng Li*

Main category: cs.AI

TL;DR: VibraVerse是一个大规模几何-声学对齐数据集，通过明确的因果链连接3D几何、物理属性、模态参数和声学信号，支持物理一致的多模态学习。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习框架缺乏物理一致性，忽视了物体几何、材料、振动模态和产生声音之间的内在因果关系。

Method: 引入CLASP对比学习框架进行跨模态对齐，保持物体物理结构与其声学响应之间的因果对应关系，构建统一的表示空间。

Result: 在几何到声音预测、声音引导形状重建和跨模态表示学习等基准任务上表现出优越的准确性、可解释性和泛化能力。

Conclusion: VibraVerse为物理一致和因果可解释的多模态学习建立了基准，为声音引导的具身感知和物理世界理解提供了基础。

Abstract: Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [148] [Extending Douglas-Rachford Splitting for Convex Optimization](https://arxiv.org/abs/2511.19637)
*Max Nilsson,Anton Åkerman,Pontus Giselsson*

Main category: math.OC

TL;DR: 本文证明了在凸优化场景下，存在比Douglas-Rachford分裂方法更广泛的节俭、无提升的解析分裂方法能够无条件收敛，并完整刻画了这类方法，导出了新的ADMM型和Chambolle-Pock型收敛方法。


<details>
  <summary>Details</summary>
Motivation: Douglas-Rachford分裂方法是解决两个极大单调算子和的经典算法，但之前认为它是唯一在一般两算子设置下无条件收敛的节俭、无提升解析分裂方法。本文旨在研究在凸优化特殊情况下这种唯一性是否成立。

Method: 通过分析凸优化场景（算子为闭真凸函数的次微分），完整刻画了所有无条件收敛的节俭、无提升解析分裂方法，并证明该刻画是精确的。

Result: 发现凸优化场景下存在严格更大的无条件收敛方法类，提供了完整的参数区域刻画，并由此推导出新的ADMM型和Chambolle-Pock型收敛方法。

Conclusion: Douglas-Rachford分裂方法的唯一性在凸优化情况下不成立，存在更广泛的收敛方法类，这为设计新的分裂算法提供了理论基础。

Abstract: The Douglas-Rachford splitting method is a classical and widely used algorithm for solving monotone inclusions involving the sum of two maximally monotone operators. It was recently shown to be the unique frugal, no-lifting resolvent-splitting method that is unconditionally convergent in the general two-operator setting. In this work, we show that this uniqueness does not hold in the convex optimization case: when the operators are subdifferentials of proper, closed, convex functions, a strictly larger class of frugal, no-lifting resolvent-splitting methods is unconditionally convergent. We provide a complete characterization of all such methods in the convex optimization setting and prove that this characterization is sharp: unconditional convergence holds exactly on the identified parameter regions. These results immediately yield new families of convergent ADMM-type and Chambolle-Pock-type methods obtained through their Douglas-Rachford reformulations.

</details>


### [149] [Catalyzing System-level Decarbonization: An Analysis of Carbon Matching As An Accounting Framework](https://arxiv.org/abs/2511.19666)
*Nikky Avila,Hank He,Reza Rastegar,Jamie Tolan,Tobias Tiecke,Brian White*

Main category: math.OC

TL;DR: 提出了一种基于边际排放率的碳匹配方法，将碳排放追踪单位从能源消耗改为吨碳排放，并证明可以完全分配电网各环节的排放。


<details>
  <summary>Details</summary>
Motivation: 改进企业碳核算方法，通过追踪排放而非能源消耗来更准确地进行碳核算，填补碳核算专业知识与电力系统建模之间的空白。

Method: 使用边际排放率进行数学推导，以吨碳排放为匹配单位，在模拟的电动巴士网络上计算边际排放，并证明可以完全分配电网各环节的排放。

Result: 碳匹配是一个准确的碳核算框架，能够为电网各环节（传输、存储、发电、消费）分配不同的排放率，完全分配电网排放。

Conclusion: 碳匹配方法能够激发有雄心和影响力的行动，通过结合碳核算和电力系统建模，为分配电力系统排放提供了有效的替代方法。

Abstract: Carbon matching aims to improve corporate carbon accounting by tracking emissions rather than energy consumption and production. We present a mathematical derivation of carbon matching using marginal emission rates, where the unit of matching is tons of carbon emitted. We present analysis and open source notebooks showing how marginal emissions can be calculated on simulated electric bus networks. Importantly, we prove mathematically that distinct emissions rates can be assigned to all aspects of the electric grid - including transmission, storage, generation, and consumption - completely allocating electric grid emissions. We show that carbon matching is an accurate carbon accounting framework that can inspire ambitious and impactful action. This research fills a gap by blending carbon accounting expertise and power systems modeling to consider the effectiveness of alternative methodologies for allocating electric system emissions.

</details>


### [150] [Anytime-Feasible First-Order Optimization via Safe Sequential QCQP](https://arxiv.org/abs/2511.19675)
*Jiarui Wang,Mahyar Fazlyab*

Main category: math.OC

TL;DR: 提出SS-QCQP算法，一种保证每次迭代可行性的非凸优化一阶方法，通过连续时间动力学系统和自适应步长的欧拉离散化实现O(1/t)收敛率，并开发了计算效率更高的主动集变体SS-QCQP-AS。


<details>
  <summary>Details</summary>
Motivation: 解决非凸不等式约束优化问题，确保每次迭代都保持可行性，同时提供理论收敛保证，并提高算法的可扩展性。

Method: 基于连续时间动力学系统，通过求解凸QCQP确保目标函数单调下降和可行集前向不变性，采用自适应步长的欧拉离散化，并开发主动集变体选择性执行边界约束。

Result: 数值实验表明SS-QCQP和SS-QCQP-AS能保持可行性，展现预期的收敛行为，解质量与SQP、IPOPT等二阶求解器相当。

Conclusion: SS-QCQP算法为约束非凸优化提供了一种理论保证强、计算效率高的解决方案，特别适合大规模问题。

Abstract: This paper presents the Safe Sequential Quadratically Constrained Quadratic Programming (SS-QCQP) algorithm, a first-order method for smooth inequality-constrained nonconvex optimization that guarantees feasibility at every iteration. The method is derived from a continuous-time dynamical system whose vector field is obtained by solving a convex QCQP that enforces monotonic descent of the objective and forward invariance of the feasible set. The resulting continuous-time dynamics achieve an $O(1/t)$ convergence rate to first-order stationary points under standard constraint qualification conditions. We then propose a safeguarded Euler discretization with adaptive step-size selection that preserves this convergence rate while maintaining both descent and feasibility in discrete time. To enhance scalability, we develop an active-set variant (SS-QCQP-AS) that selectively enforces constraints near the boundary, substantially reducing computational cost without compromising theoretical guarantees. Numerical experiments on a multi-agent nonlinear optimal control problem demonstrate that SS-QCQP and SS-QCQP-AS maintain feasibility, exhibit the predicted convergence behavior, and deliver solution quality comparable to second-order solvers such as SQP and IPOPT.

</details>


### [151] [Optimal dividend and capital injection under self-exciting claims](https://arxiv.org/abs/2511.19701)
*Paulin Aubert,Etienne Chevalier,Vathana Ly Vath*

Main category: math.OC

TL;DR: 本文研究了在具有Hawkes过程索赔到达的Cramér-Lundberg模型中的最优分红和资本注入问题，建立了价值函数的关键分析性质，并通过显式阈值刻画了最优资本注入策略。


<details>
  <summary>Details</summary>
Motivation: 保险投资组合中经常观察到索赔的聚集效应，传统模型无法充分捕捉这种自激励的动态特性，因此需要研究在Hawkes过程下的最优分红和资本注入策略。

Method: 建立了价值函数的分析性质，通过HJB变分不等式刻画最优策略，使用单调有限差分法和Howard策略迭代计算基准解，并开发了基于策略梯度和演员-评论家方法的强化学习方法。

Result: 学习到的策略与PDE基准解高度匹配，在不同初始条件下保持稳定，证明了策略梯度技术在自激励索赔动态下的分红优化中的有效性。

Conclusion: 研究结果凸显了策略梯度技术在自激励索赔动态下分红优化中的相关性，并为更高维度的扩展提供了可扩展的方法。

Abstract: In this paper, we study an optimal dividend and capital-injection problem in a Cramér--Lundberg model where claim arrivals follow a Hawkes process, capturing clustering effects often observed in insurance portfolios. We establish key analytical properties of the value function and characterise the optimal capital-injection strategy through an explicit threshold. We also show that the value function is the unique viscosity solution of the associated HJB variational inequality. For numerical purposes, we first compute a benchmark solution via a monotone finite-difference scheme with Howard's policy iteration. We then develop a reinforcement learning approach based on policy-gradient and actor-critic methods. The learned strategies closely match the PDE benchmark and remain stable across initial conditions. The results highlight the relevance of policy-gradient techniques for dividend optimisation under self-exciting claim dynamics and point toward scalable methods for higher-dimensional extensions.

</details>


### [152] [An Accelerated Distributed Algorithm with Equality and Inequality Coupling Constraints](https://arxiv.org/abs/2511.19708)
*Chenyang Qiu,Yangyang Qian,Zongli Lin,Yacov A. Shamash*

Main category: math.OC

TL;DR: 该论文研究具有仿射等式和非线性不等式耦合的分布式凸优化问题，通过设计加速线性化算法，在相同通信预算下比现有方法更快地减少最优性误差和可行性残差。


<details>
  <summary>Details</summary>
Motivation: 研究分布式凸优化中同时存在仿射等式和非线性不等式耦合约束的问题，通过对偶分析来有效解决这类复杂约束优化问题。

Method: 首先将耦合约束问题的对偶形式化为连接网络上的共识优化问题，然后设计加速线性化算法，结合前瞻线性化、拉普拉斯约束的二次惩罚、近邻步骤和迭代聚合。

Result: 理论证明了原始最优性误差和可行性误差的非遍历收敛率，数值实验显示在相同通信预算下，比增强拉格朗日跟踪和分布式次梯度基线方法更快地减少最优性误差和可行性残差。

Conclusion: 提出的加速线性化算法能有效解决具有复杂约束的分布式凸优化问题，在理论和实验上都表现出优于现有方法的性能。

Abstract: This paper studies distributed convex optimization with both affine equality and nonlinear inequality couplings through the duality analysis. We first formulate the dual of the coupling-constraint problem and reformulate it as a consensus optimization problem over a connected network. To efficiently solve this dual problem and hence the primal problem, we design an accelerated linearized algorithm that, at each round, a look-ahead linearization of the separable objective is combined with a quadratic penalty on the Laplacian constraint, a proximal step, and an aggregation of iterations. On the theory side, we prove non-ergodic rates for both the primal optimality error and the feasibility error. On the other hand, numerical experiments show a faster decrease of optimality error and feasibility residual than augmented-Lagrangian tracking and distributed subgradient baselines under the same communication budget.

</details>


### [153] [Non-Ergodic Convergence Algorithms for Distributed Consensus and Coupling-Constrained Optimization](https://arxiv.org/abs/2511.19714)
*Chenyang Qiu,Zongli Lin*

Main category: math.OC

TL;DR: 提出了一种用于分布式凸优化的线性化乘子法，在非光滑非强凸条件下实现了非遍历的O(1/√k)收敛率，并在IEEE 118总线系统上展示了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 研究具有共识约束和全局仿射等式约束的分布式凸优化问题，这类问题在电力系统经济调度等应用中普遍存在。

Method: 设计了线性化乘子法，首先用于共识优化问题，然后通过对偶性将其应用于经济调度问题的对偶共识形式。

Result: 在非光滑非强凸条件下，实现了目标函数最优性和共识违反的非遍历O(1/√k)收敛率；在IEEE 118总线系统上相比现有方法能更快减少目标误差和可行性误差。

Conclusion: 所提出的线性化乘子法能有效解决具有共识和等式约束的分布式优化问题，在对偶变量达到全网共识的同时实现快速收敛。

Abstract: We study distributed convex optimization with two ubiquitous forms of coupling: consensus constraints and global affine equalities. We first design a linearized method of multipliers for the consensus optimization problem. Without smoothness or strong convexity, we establish non-ergodic sublinear rates of order O(1/\sqrt{k}) for both the objective optimality and the consensus violation. Leveraging duality, we then show that the economic dispatch problem admits a dual consensus formulation, and that applying the same algorithm to the dual economic dispatch yields non-ergodic O(1/\sqrt{k}) decay for the error of the summation of the cost over the network and the equality-constraint residual under convexity and Slater's condition. Numerical results on the IEEE 118-bus system demonstrate faster reduction of both objective error and feasibility error relative to the state-of-the-art baselines, while the dual variables reach network-wide consensus.

</details>


### [154] [A Distributed Gradient-based Algorithm for Optimization Problems with Coupled Equality Constraints](https://arxiv.org/abs/2511.19723)
*Chenyang Qiu,Zongli Lin*

Main category: math.OC

TL;DR: 提出了一种新的分布式梯度算法，避免每次迭代求解局部优化问题，通过一阶近似和投影到局部可行集来提高计算效率


<details>
  <summary>Details</summary>
Motivation: 现有分布式算法依赖argmin算子求解局部子问题，当局部成本函数复杂时计算负担重或难以处理

Method: 基于一阶近似和投影到局部可行集的分布式梯度算法，仅需局部通信，不交换梯度或原始变量

Result: 严格证明了在一般凸成本函数下的次线性收敛性，以及在强凸性和光滑性条件下的线性收敛性

Conclusion: 在IEEE 118总线系统上的数值仿真表明，该方法相比现有分布式优化算法具有优越的计算效率和可扩展性

Abstract: This paper studies a class of distributed optimization problems with coupled equality constraints in networked systems. Many existing distributed algorithms rely on solving local subproblems via the $\operatorname{argmin}$ operator in each iteration. Such approaches become computationally burdensome or intractable when local cost functions are complex. To address this challenge, we propose a novel distributed gradient-based algorithm that avoids solving a local optimization problem at each iteration by leveraging first-order approximations and projection onto local feasible sets. The algorithm operates in a fully distributed manner, requiring only local communication without exchanging gradients or primal variables. We rigorously establish sublinear convergence for general convex cost functions and linear convergence under strong convexity and smoothness conditions. Numerical simulation on the IEEE 118-bus system demonstrates the superior computational efficiency and scalability of the proposed method compared to several state-of-the-art distributed optimization algorithms.

</details>


### [155] [Convexification of classes of mixed-integer sets with L$^\natural$-convexity](https://arxiv.org/abs/2511.19754)
*Qimeng Yu,Simge Küçükyavuz*

Main category: math.OC

TL;DR: 本文研究了L$^\natural$-凸函数及其混合整数扩展的最小化问题，提出了有效的线性不等式、凸包描述、分离方法和复杂性分析，并在混合集中发现了隐藏的L$^\natural$-凸性。


<details>
  <summary>Details</summary>
Motivation: L$^\natural$-凸函数在整数域上涵盖了大量非线性函数，在现实应用中广泛存在，需要开发有效的优化方法来处理这类函数的最小化问题。

Method: 针对L$^\natural$-凸函数、多函数共同变量最小化以及混合整数扩展，提出了有效的线性不等式，给出了凸包描述，讨论了面条件，开发了精确分离方法，并分析了分离问题的复杂性。

Result: 发现了混合集和连续混合集中隐藏的L$^\natural$-凸性，研究结果包含了这些集合的现有多面体结果，并为连续混合集的多容量变体建立了新结果。

Conclusion: 本文为L$^\natural$-凸函数及其扩展的最小化问题提供了系统的多面体理论框架，揭示了混合整数结构中隐藏的凸性结构，扩展了现有理论结果。

Abstract: L$^\natural$ (natural)-convex functions encompass a large class of nonlinear functions over general integer domains and arise in a wide range of real-world applications. We explore the minimization of L$^\natural$-convex functions, of multiple L$^\natural$-convex functions with common variables, and of a mixed-integer extension of L$^\natural$-convex functions -- functions defined over a mixed-integer domain with properties that resemble L$^\natural$-convexity. For each of these families of minimization problems, we propose valid linear inequalities and provide convex hull descriptions for the corresponding epigraphs. For all classes of proposed inequalities, we discuss their facet conditions, develop exact separation methods, and analyze the complexity of the separation problem. We discover hidden L$^\natural$-convexity in well-known mixed-integer structures in the integer programming literature, namely the (general integer) mixing set and the continuous mixing set. We show that our findings subsume the existing polyhedral results for these sets and establish new results for the multi-capacity variant of the continuous mixing set.

</details>


### [156] [Heterogeneous Mean Field Games and Local Well-posedness](https://arxiv.org/abs/2511.19766)
*Bixing Qiao*

Main category: math.OC

TL;DR: 本文提出了异质平均场博弈（HMFG）的一般框架，包含图平均场博弈的不同表述。HMFG通过密度集合实现玩家与群体的互动，形成无限维前向-后向随机微分方程系统，证明了局部适定性、均衡存在性及其在N玩家博弈中的近似均衡性质，并推导了无限维测度流的Itô公式和主方程。


<details>
  <summary>Details</summary>
Motivation: 受近期对非对称平均场博弈兴趣的推动，旨在构建一个包含图平均场博弈不同表述的通用框架，研究玩家通过密度集合与群体互动的博弈系统。

Method: 建立异质平均场博弈（HMFG）框架，将问题转化为无限维前向-后向随机微分方程系统，证明其局部适定性，并推导无限维测度流的Itô公式和主方程作为解耦场。

Result: 证明了HMFG系统的无限维FBSDE局部适定，存在唯一均衡；该均衡是对应N玩家博弈的良好近似均衡；成功推导了无限维测度流的Itô公式和主方程。

Conclusion: HMFG框架为异质平均场博弈提供了统一的理论基础，证明了均衡存在性和近似性质，并建立了无限维FBSDE系统与主方程的联系，为相关研究提供了有力工具。

Abstract: Motivated by the recent interests in asymmetric mean field games, this paper provides a general framework of Heterogeneous Mean Field Game (HMFG) that subsumes different formulations of graphon mean field games. The key feature of the HMFG is that the players interact with the population through the density ensemble. In this case, the HMFG system becomes an infinite-dimensional Forward-Backward SDE (FBSDE) system. We show that the FBSDE is locally well-posed, thus the HMFG has a unique equilibrium. In addition, we show that the equilibrium of HMFG is a good approximate equilibrium of the corresponding N-Player Game. Lastly, we derive the Itô formula of infinite-dimensional measure flow and use it to obtain the master equation for HMFG as a decoupling field of the infinite-dimensional FBSDE system.

</details>


### [157] [On the Fundamental Limit of Stochastic Gradient Identification Algorithm Under Non-Persistent Excitation](https://arxiv.org/abs/2511.19981)
*Senhan Yao,Longxu Zhang*

Main category: math.OC

TL;DR: 本文证明了随机梯度算法在非持续激励条件下，当Fisher信息矩阵条件数满足κ(∑φ_iφ_i^⊤)=O((log r_n)^α)时，对于整个范围0≤α<1都能保证强一致性，几乎完全解决了Chen和Guo(1986)提出的四十年猜想。


<details>
  <summary>Details</summary>
Motivation: 随机梯度方法在系统辨识和机器学习中具有基础重要性，但在非持续激励条件下，已知的最佳收敛结果要求α≤1/3，而α>1时已知会失效。Chen和Guo在1986年提出的猜想存在理论空白，需要填补从α≤1/3到几乎整个可行范围的理论差距。

Method: 通过新颖的代数框架，获得了显著更尖锐的矩阵范数界限，从而建立了强一致性结果。

Result: 证明了对于整个范围0≤α<1，随机梯度算法都能保证强一致性，这几乎完全解决了四十年来的猜想。

Conclusion: 该研究通过创新的代数方法，将随机梯度算法的强一致性理论从α≤1/3扩展到几乎整个可行范围0≤α<1，为系统辨识和机器学习中的参数估计提供了更完整的理论基础。

Abstract: Stochastic gradient methods are of fundamental importance in system identification and machine learning, enabling online parameter estimation for large-scale and data-streaming processes. The stochastic gradient algorithm stands as a classical identification method that has been extensively studied for decades. Under non-persistent excitation, the best known convergence result requires the condition number of the Fisher information matrix to satisfy $κ(\sum_{i=1}^n \varphi_i \varphi_i^\top) = O((\log r_n)^α)$, where $r_n = 1 + \sum_{i=1}^n \|\varphi_i\|^2$, with strong consistency guaranteed for $α\leq 1/3$ but known to fail for $α> 1$. This paper establishes that strong consistency in fact holds for the entire range $0 \leq α< 1$, achieved through a novel algebraic framework that yields substantially sharper matrix norm bounds. Our result nearly resolves the four-decade-old conjecture of Chen and Guo (1986), bridging the theoretical gap from $α\leq 1/3$ to nearly the entire feasible range.

</details>


### [158] [A Local Parametrization of the State-Feedback Matrices in the Pole Assignment Problem](https://arxiv.org/abs/2511.20029)
*I. Baragaña,F. Puerta,I. Zaballa*

Main category: math.OC

TL;DR: 该论文研究了可控系统的状态反馈增益矩阵集合的几何结构，证明了该集合具有微分流形结构，并提供了局部参数化和坐标系。


<details>
  <summary>Details</summary>
Motivation: 研究可控系统中使闭环系统状态矩阵处于特定相似类的反馈增益矩阵集合的几何特性，为控制系统设计提供理论基础。

Method: 通过李群作用在截断可观测性矩阵轨道空间上的微分同胚，构建反馈增益矩阵集合的局部参数化和坐标系。

Result: 证明了该反馈增益矩阵集合具有微分流形结构，计算了其维度，并建立了局部参数化方法。

Conclusion: 为可控系统的反馈增益设计提供了几何框架，揭示了反馈增益矩阵集合的内在流形结构。

Abstract: Given a controllable system $(F,G)$, a local parametrization is obtained for the set of feedback gain matrices $K$ such that the state matrix, $F+GK$, of the closed loop system is in a prescribed similarity class. It is shown that this set can be endowed with the structure of a differentiable manifold whose dimension is also computed. Then a local parametrization and a local system of coordinates is provided using a diffeomorphism between this set of state feedback matrices and the orbit space of a set of truncated observability matrices via de action of a Lie group.

</details>


### [159] [On the differentiability of the value function of switched linear systems under arbitrary and controlled switching](https://arxiv.org/abs/2511.20037)
*Guillaume O. Berger*

Main category: math.OC

TL;DR: 本文研究了切换线性系统在任意切换和控制切换下的值函数可微性，证明了这些值函数在成本函数Lipschitz连续时也是Lipschitz连续的，但即使在光滑Lipschitz连续成本函数下，这些值函数在状态空间的稠密子集上可能不可微。


<details>
  <summary>Details</summary>
Motivation: 研究切换线性系统值函数的可微性，这对最优控制和强化学习具有重要意义，因为值函数的可微性影响计算方法和收敛性。

Method: 通过理论分析和构造反例，证明值函数在稠密子集上的不可微性，即使成本函数是光滑且Lipschitz连续的。

Result: 证明了值函数是Lipschitz连续的，但可能在整个状态空间的稠密子集上不可微，这对精确计算值函数提出了挑战。

Conclusion: 切换线性系统的值函数计算需要处理在稠密子集上不可微的函数模板，这对最优控制和强化学习算法的设计有重要影响。

Abstract: This paper studies the differentiability of the value function of switched linear systems under arbitrary switching and controlled switching, referred to as worst-case and optimal value functions respectively. First, we show that the value functions are Lipschitz continuous, when the cost function is Lipschitz continuous. Then, as the central contribution of this work, we show with examples that each of these functions can be non-differentiable on dense subsets of the state space, even if the cost function is smooth and Lipschitz continuous. This has implications for optimal control and reinforcement learning since it implies that the exact computation of these value functions requires templates involving functions that are non-differentiable on dense subsets.

</details>


### [160] [Scaling limits of multi-period distributionally robust optimization problems](https://arxiv.org/abs/2511.20126)
*Max Nendel,Ariel Neufeld,Kyunghyun Park,Alessandro Sgarabottolo*

Main category: math.OC

TL;DR: 该论文研究了多周期分布鲁棒优化（DRO）问题的尺度极限，通过半群方法证明了当周期长度趋于零时，多周期DRO收敛于一个强连续单调半群，其生成元包含参考过程的生成元加上由Wasserstein不确定性引起的扰动项。


<details>
  <summary>Details</summary>
Motivation: 研究多周期分布鲁棒优化问题在连续时间极限下的行为，理解不确定性对优化问题的影响，并建立与非线性PDE的联系。

Method: 使用半群方法分析多周期DRO问题的尺度极限，考虑每个周期在参考过程转移概率的Wasserstein球内进行最坏情况分布最大化，并研究其序列组合。

Result: 证明了多周期DRO的尺度极限是一个强连续单调半群，其生成元等于非鲁棒尺度极限的生成元加上由Wasserstein不确定性诱导的扰动项。当参考过程为Itô过程时，相关非线性PDE的粘性解与参数不确定性下的连续时间鲁棒优化问题的值一致。

Conclusion: 该研究为分布鲁棒优化问题提供了连续时间极限的理论基础，建立了多周期DRO与非线性PDE之间的深刻联系，为鲁棒优化理论的发展做出了贡献。

Abstract: We examine the scaling limit of multi-period distributionally robust optimization (DRO) problems via a semigroup approach. Each period involves a worst-case maximization over distributions in a Wasserstein ball around the transition probability of a reference process with radius proportional to the length of the period, and the multi-period DRO problem arises through its sequential composition. We show that the scaling limit of the multi-period DRO, as the length of each period tends to zero, is a strongly continuous monotone semigroup on $\mathrm{C_b}$. Furthermore, we show that its infinitesimal generator is equal to the generator associated with the non-robust scaling limit plus an additional perturbation term induced by the Wasserstein uncertainty. As an application, we show that when the reference process follows an Itô process, the viscosity solution of the associated nonlinear PDE coincides with the value of continuous-time robust optimization problems under parametric uncertainty.

</details>


### [161] [Stochastic Sequential Quadratic Programming for Optimization with Functional Constraints](https://arxiv.org/abs/2511.20178)
*Panchajanya Sanyal,Srujan Teja Thomdapu,Ketan Rajawat*

Main category: math.OC

TL;DR: 提出了三种新的随机序列二次规划算法（SSQP、SSQP-Skip、VARAS），用于解决带非线性函数约束的随机凸优化问题，无需投影到可行域，无需有界梯度假设，在标准光滑性和凸性假设下达到最优的oracle复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统投影随机梯度下降方法在处理非线性函数约束时效率低下，现有的一阶方法通常依赖有界梯度假设，这在许多实际应用中过于严格。

Method: 提出SSQP算法在原始域工作，避免投影到可行域；SSQP-Skip在大多数迭代中跳过二次子问题；VARAS是加速的方差缩减版本。

Result: 算法在标准光滑性和凸性假设下达到最优的oracle复杂度，数值实验在真实数据集上展示了优越性能。

Conclusion: 所提出的算法为带非线性函数约束的随机凸优化问题提供了高效解决方案，无需有界梯度假设，性能优于现有方法。

Abstract: Stochastic convex optimization problems with nonlinear functional constraints are ubiquitous in machine learning applications, including multi-task learning, structured prediction, and multi-view learning. The presence of nonlinear functional constraints renders the traditional projected stochastic gradient descent and related projection-based methods inefficient, and motivates the use of first-order methods. However, existing first-order methods, including primal and primal-dual algorithms, typically rely on a bounded (sub-)gradient assumption, which may be too restrictive in many settings.
  We propose a stochastic sequential quadratic programming (SSQP) algorithm that works entirely in the primal domain, avoids projecting onto the feasible region, obviates the need for bounded gradients, and achieves state-of-the-art oracle complexity under standard smoothness and convexity assumptions. A faster version, namely SSQP-Skip, is also proposed where the quadratic subproblems can be skipped in most iterations. Finally, we develop an accelerated variance-reduced version of SSQP (VARAS), whose oracle complexity bounds match those for solving unconstrained finite-sum convex optimization problems. The superior performance of the proposed algorithms is demonstrated via numerical experiments on real datasets.

</details>


### [162] [Adaptive SGD with Line-Search and Polyak Stepsizes: Nonconvex Convergence and Accelerated Rates](https://arxiv.org/abs/2511.20207)
*Haotian Wu*

Main category: math.OC

TL;DR: 将AdaSLS和AdaSPS的自适应步长方法扩展到非凸优化，证明了在非凸光滑函数下的收敛性，包括一般非凸函数的O(1/√T)收敛率，以及在某些条件下的O(1/T)收敛率。


<details>
  <summary>Details</summary>
Motivation: 扩展[Jiang and Stich, 2024]中AdaSLS和AdaSPS的收敛分析到非凸优化设置，为自适应步长的随机梯度下降方法在非凸问题中提供理论保证。

Method: 使用自适应Armijo线搜索(AdaSLS)和Polyak步长(AdaSPS)的随机梯度下降方法，在非凸优化框架下进行统一收敛分析。

Result: 证明了三种收敛率：(1)一般非凸光滑函数的O(1/√T)收敛率；(2)拟星凸和插值条件下的O(1/T)收敛率；(3)强增长条件下一般非凸函数的O(1/T)收敛率。

Conclusion: 成功将自适应步长方法的收敛分析扩展到非凸优化，为AdaSLS和AdaSPS在更广泛问题中的应用提供了理论支撑。

Abstract: We extend the convergence analysis of AdaSLS and AdaSPS in [Jiang and Stich, 2024] to the nonconvex setting, presenting a unified convergence analysis of stochastic gradient descent with adaptive Armijo line-search (AdaSLS) and Polyak stepsize (AdaSPS) for nonconvex optimization. Our contributions include: (1) an $\mathcal{O}(1/\sqrt{T})$ convergence rate for general nonconvex smooth functions, (2) an $\mathcal{O}(1/T)$ rate under quasar-convexity and interpolation, and (3) an $\mathcal{O}(1/T)$ rate under the strong growth condition for general nonconvex functions.

</details>


### [163] [Scaled relative graphs for pairs of operators beyond classical monotonicity](https://arxiv.org/abs/2511.20209)
*Jan Quan,Alexander Bodard,Konstantinos Oikonomidis,Panagiotis Patrinos*

Main category: math.OC

TL;DR: 提出了一种广义的缩放相对图(SRG)框架，用于可视化算子对的相对增量特性，为基于配对单调性条件的非线性解析研究提供几何对应。


<details>
  <summary>Details</summary>
Motivation: 现有的SRG方法在处理非线性算子对的相对特性时存在局限，需要一种能够可视化算子对相对增量特性的通用框架，特别是在处理多值、非光滑和高度非单调电路时。

Method: 将缩放相对图(SRG)推广到算子对，建立非线性解析的几何对应，基于配对单调性条件分析线性算子与单调映射的组合。

Result: 该框架能够处理包含NPN晶体管等元件的多值、非光滑和高度非单调电路，成功计算了这类电路的响应特性。

Conclusion: 广义SRG框架为分析非线性算子对的相对特性提供了有效的几何工具，特别适用于复杂电路系统的建模和分析。

Abstract: We introduce a generalization of the scaled relative graph (SRG) to pairs of operators, enabling the visualization of their relative incremental properties. This novel SRG framework provides the geometric counterpart for the study of nonlinear resolvents based on paired monotonicity conditions. We demonstrate that these conditions apply to linear operators composed with monotone mappings, a class that notably includes NPN transistors, allowing us to compute the response of multivalued, nonsmooth and highly nonmonotone electrical circuits.

</details>


### [164] [Nonlinearly preconditioned gradient flows](https://arxiv.org/abs/2511.20370)
*Konstantinos Oikonomidis,Alexander Bodard,Jan Quan,Panagiotis Patrinos*

Main category: math.OC

TL;DR: 该论文研究了非线性预条件梯度方法的连续时间动力学系统，建立了全局解的存在性和基于Lyapunov的收敛保证，揭示了与镜像下降的对偶关系，并连接了非欧几里得优化中的连续时间模型。


<details>
  <summary>Details</summary>
Motivation: 研究非线性预条件梯度方法的连续时间极限，以理解其优化行为和结构特性，并建立与已知非欧几里得优化模型的联系。

Method: 通过分析连续时间动力学系统，使用Lyapunov方法建立收敛性，并利用对偶关系连接镜像下降方法。

Result: 证明了凸成本下的次线性衰减和广义梯度支配条件下的指数收敛，揭示了系统解决无限时域最优控制问题的特性。

Conclusion: 非线性预条件梯度流具有明确的优化行为结构，与镜像下降存在对偶关系，为理解非欧几里得优化提供了新的连续时间视角。

Abstract: We study a continuous-time dynamical system which arises as the limit of a broad class of nonlinearly preconditioned gradient methods. Under mild assumptions, we establish existence of global solutions and derive Lyapunov-based convergence guarantees. For convex costs, we prove a sublinear decay in a geometry induced by some reference function, and under a generalized gradient-dominance condition we obtain exponential convergence. We further uncover a duality connection with mirror descent, and use it to establish that the flow of interest solves an infinite-horizon optimal-control problem of which the value function is the Bregman divergence generated by the cost. These results clarify the structure and optimization behavior of nonlinearly preconditioned gradient flows and connect them to known continuous-time models in non-Euclidean optimization.

</details>


### [165] [Self-Identifying Internal Model-Based Online Optimization](https://arxiv.org/abs/2511.20411)
*Wouter J. A. van Weerelt,Lantian Zhang,Silun Zhang,Nicola Bastianello*

Main category: math.OC

TL;DR: 提出了一种结合控制理论和系统识别的新型在线优化算法，通过识别在线问题的内部模型来实现自适应优化


<details>
  <summary>Details</summary>
Motivation: 现有在线优化算法往往缺乏对问题内部模型的利用，而实际中这种先验知识通常不可得，需要动态学习

Method: 基于控制理论设计算法框架，结合系统识别方法动态学习问题内部模型，从二次型问题扩展到一般问题

Result: 在二次型情况下证明了算法渐近收敛到最优解轨迹，数值实验显示在非二次型问题上也有良好表现

Conclusion: 该控制理论与系统识别结合的算法能够有效适应内部模型变化，在多种在线优化问题中表现优异

Abstract: In this paper, we propose a novel online optimization algorithm built by combining ideas from control theory and system identification. The foundation of our algorithm is a control-based design that makes use of the internal model of the online problem. Since such prior knowledge of this internal model might not be available in practice, we incorporate an identification routine that learns this model on the fly. The algorithm is designed starting from quadratic online problems but can be applied to general problems. For quadratic cases, we characterize the asymptotic convergence to the optimal solution trajectory. We compare the proposed algorithm with existing approaches, and demonstrate how the identification routine ensures its adaptability to changes in the underlying internal model. Numerical results also indicate strong performance beyond the quadratic setting.

</details>


### [166] [PAC-Bayes Meets Online Contextual Optimization](https://arxiv.org/abs/2511.20413)
*Zhuojun Xie,Adam Abdin,Yiping Fang*

Main category: math.OC

TL;DR: 提出了首个贝叶斯在线上下文优化框架，基于PAC-Bayes理论和贝叶斯更新原则，通过吉布斯后验实现有界和可混合损失的O(√T)遗憾，使用序贯蒙特卡洛采样器消除对梯度的依赖，适用于不可微问题。


<details>
  <summary>Details</summary>
Motivation: 现有预测-优化范式主要基于频率主义方法，严重依赖梯度策略，使用确定性预测器在实践中可能产生高方差，尽管有渐近保证。

Method: 基于PAC-Bayes理论和贝叶斯更新原则，使用吉布斯后验和序贯蒙特卡洛采样器，构建贝叶斯在线上下文优化框架。

Result: 实现了有界和可混合损失的O(√T)遗憾，消除了对梯度的依赖，能够处理不可微问题。理论发展和数值实验验证了这些主张。

Conclusion: 该贝叶斯框架为在线上下文优化提供了新的解决方案，克服了现有频率主义方法的局限性，具有更好的实践适用性。

Abstract: The predict-then-optimize paradigm bridges online learning and contextual optimization in dynamic environments. Previous works have investigated the sequential updating of predictors using feedback from downstream decisions to minimize regret in the full-information settings. However, existing approaches are predominantly frequentist, rely heavily on gradient-based strategies, and employ deterministic predictors that could yield high variance in practice despite their asymptotic guarantees. This work introduces, to the best of our knowledge, the first Bayesian online contextual optimization framework. Grounded in PAC-Bayes theory and general Bayesian updating principles, our framework achieves $\mathcal{O}(\sqrt{T})$ regret for bounded and mixable losses via a Gibbs posterior, eliminates the dependence on gradients through sequential Monte Carlo samplers, and thereby accommodates nondifferentiable problems. Theoretical developments and numerical experiments substantiate our claims.

</details>
