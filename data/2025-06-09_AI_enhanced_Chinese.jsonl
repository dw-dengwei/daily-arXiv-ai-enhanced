{"id": "2506.05566", "pdf": "https://arxiv.org/pdf/2506.05566", "abs": "https://arxiv.org/abs/2506.05566", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "AI": {"title_translation": "ScaleRTL：利用推理数据和测试时计算扩展LLM以实现准确的RTL代码生成", "tldr": "ScaleRTL是一个推理型LLM，通过大规模高质量推理数据和测试时计算，显著提升了RTL代码生成性能，在VerilogEval和RTLLM上实现了最先进的结果。", "motivation": "现有的大型语言模型（LLMs）在RTL代码生成方面的有效性有限，主要原因是高质量训练数据的稀缺，以及缺乏对测试时扩展的支持。", "method": "本文引入了ScaleRTL，这是第一个用于RTL编码的推理型LLM。该方法包括：1. 收集和整理了一个包含3.5B tokens的、多样化的长链式思维推理轨迹数据集，平均每个轨迹56K tokens，捕获了丰富的RTL知识。2. 在此大规模语料库上对通用推理模型进行微调，使其具备深度RTL推理能力。3. 进一步通过一种新颖的测试时扩展策略增强性能，该策略通过迭代反思和自我纠正先前的推理步骤来扩展推理过程。", "result": "ScaleRTL在VerilogEval和RTLLM基准测试上均取得了最先进的性能。在VerilogEval上，它超越了18个竞争基线高达18.4%；在RTLLM上，性能提升高达12.7%。", "conclusion": "ScaleRTL通过结合大规模高质量推理数据和创新的测试时计算策略，成功克服了RTL代码生成领域的数据稀缺瓶颈，并显著提升了大型语言模型在该领域的准确性和性能，达到了当前最先进的水平。", "translation": "大型语言模型（LLM）的最新进展已使软件编码基准测试达到接近人类的性能，但由于高质量训练数据的稀缺，它们在RTL代码生成方面的有效性仍然有限。虽然之前的努力已经针对RTL任务对LLM进行了微调，但它们未能从根本上克服数据瓶颈，并且由于其非推理性质而缺乏对测试时扩展的支持。在这项工作中，我们引入了ScaleRTL，这是第一个用于RTL编码的推理型LLM，它同时扩展了高质量推理数据和测试时计算。具体来说，我们整理了一组多样化的长链式思维推理轨迹，平均每个56K token，形成了一个包含3.5B token的数据集，捕获了丰富的RTL知识。在此语料库上对通用推理模型进行微调，生成了能够进行深度RTL推理的ScaleRTL。随后，我们通过一种新颖的测试时扩展策略进一步增强了ScaleRTL的性能，该策略通过迭代反思和自我纠正先前的推理步骤来扩展推理过程。实验结果表明，ScaleRTL在VerilogEval和RTLLM上取得了最先进的性能，在VerilogEval上超越18个竞争基线高达18.4%，在RTLLM上高达12.7%。", "summary": "本文介绍了ScaleRTL，一个专为RTL代码生成设计的推理型大型语言模型。为解决高质量RTL数据稀缺问题，ScaleRTL构建了一个包含3.5B tokens的大规模链式思维推理数据集，并在此基础上对通用推理模型进行微调。此外，它还采用了一种创新的测试时扩展策略，通过迭代反思和自我纠正来深化推理过程。实验结果表明，ScaleRTL在VerilogEval和RTLLM基准测试上均实现了最先进的性能，显著优于现有方法。", "keywords": "RTL代码生成, 大型语言模型, 推理数据, 测试时计算, 链式思维", "comments": "ScaleRTL的创新之处在于其双重扩展策略：一是通过构建大规模高质量推理数据克服了RTL领域特有的数据稀缺瓶颈，这对专业领域LLM的发展具有重要意义；二是通过引入测试时自我修正的推理策略，有效提升了模型的泛化能力和准确性。这为未来LLM在专业编码领域的应用提供了新的思路和范式。"}}
{"id": "2506.05682", "pdf": "https://arxiv.org/pdf/2506.05682", "abs": "https://arxiv.org/abs/2506.05682", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "categories": ["cs.AR"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "AI": {"title_translation": "Lumina：通过利用计算冗余实现实时移动神经渲染", "tldr": "Lumina是一个软硬件协同设计的系统，通过S^2算法、辐射缓存机制和LuminCore加速器，显著提升了移动设备上3D Gaussian Splatting神经渲染的效率和能效。", "motivation": "3D Gaussian Splatting（3DGS）虽然极大地推进了神经渲染的进展，但在当今的移动系统级芯片（SoC）上仍然计算量巨大，面临性能挑战。", "method": "本文提出了Lumina，一个软硬件协同设计的系统，它整合了两种主要优化：1. S^2算法，利用渲染中的时间相干性来减少计算开销。2. 辐射缓存（RC）机制，利用3DGS的颜色积分过程来降低密集光栅化计算的频率。此外，还提出了一个加速器架构LuminCore，以进一步加速缓存查找并解决光栅化中的基本低效率问题。", "result": "Lumina在合成和真实世界数据集上，与移动Volta GPU相比，实现了4.5倍的加速和5.3倍的能耗降低，同时图像质量损失很小（峰值信噪比降低< 0.2 dB）。", "conclusion": "Lumina通过软硬件协同设计，有效解决了移动设备上神经渲染计算量大的问题，显著提高了性能和能效，同时保持了高质量的渲染效果。", "translation": "3D Gaussian Splatting (3DGS) 极大地推进了神经渲染的步伐，但对于当今的移动系统级芯片 (SoC) 而言，其计算需求仍然很高。为了解决这一挑战，我们提出了 Lumina，一个软硬件协同设计的系统，它整合了两个主要优化：一种新颖的算法 S^2 和一种辐射缓存机制 RC，以提高神经渲染的效率。S^2 算法利用渲染中的时间相干性来减少计算开销，而 RC 则利用 3DGS 的颜色积分过程来降低密集光栅化计算的频率。结合这些技术，我们提出了一种加速器架构 LuminCore，以进一步加速缓存查找并解决光栅化中的基本低效率问题。我们展示了 Lumina 在合成和真实世界数据集上，与移动 Volta GPU 相比，实现了 4.5 倍的加速和 5.3 倍的能耗降低，同时图像质量损失很小（峰值信噪比降低 < 0.2 dB）。", "summary": "本文介绍了Lumina，一个针对移动设备上3D Gaussian Splatting神经渲染效率问题提出的软硬件协同设计系统。Lumina通过结合S^2算法（利用时间相干性减少计算）和辐射缓存机制（降低光栅化频率），并配合专门的LuminCore加速器，显著提升了渲染速度和能效。实验结果表明，Lumina在保持高渲染质量的同时，实现了数倍的性能和能效提升。", "keywords": "神经渲染, 3D Gaussian Splatting, 移动计算, 软硬件协同设计, 实时渲染", "comments": "Lumina的创新之处在于其软硬件协同设计的方法，特别是在移动平台上优化3D Gaussian Splatting。S^2算法和辐射缓存机制从算法层面减少了计算冗余，而LuminCore加速器则从硬件层面提供了专门支持，共同解决了实时移动神经渲染的效率瓶颈。其在速度和能效上的显著提升，对于未来移动AR/VR和实时3D应用具有重要意义。"}}
{"id": "2506.05588", "pdf": "https://arxiv.org/pdf/2506.05588", "abs": "https://arxiv.org/abs/2506.05588", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "categories": ["cs.NE", "cs.AR", "cs.ET"], "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "AI": {"title_translation": "忆阻器储层计算在图像识别中的预处理方法", "tldr": "本研究系统比较了忆阻器储层计算（RC）中不同的预处理方法，并提出了一种基于奇偶校验的预处理方法，该方法能将图像识别的准确率提高2-6%，同时对设备数量的增加要求不高。", "motivation": "尽管忆阻器储层计算（RC）在简化训练方面具有优势，但在实现高性能方面仍面临挑战，这主要取决于输入预处理方法和储层大小。目前缺乏对这些因素影响的全面评估。", "method": "本研究系统地比较了忆阻器储层计算系统中各种预处理方法，评估它们对准确性和能耗的影响。此外，还提出了一种基于奇偶校验的预处理方法。", "result": "提出的基于奇偶校验的预处理方法将准确率提高了2-6%，同时相比其他方法只增加了适度的设备数量。研究结果强调了知情预处理策略对于提高忆阻器RC系统效率和可扩展性的重要性。", "conclusion": "知情的预处理策略对于提高忆忆阻器储层计算（RC）系统的效率和可扩展性至关重要。", "translation": "储层计算（RC）作为一种高效的循环神经网络架构，因其简化的训练（仅需训练其最后一个感知器读出层）而备受关注。当与忆阻器结合实现时，RC系统受益于其动态特性，这使得它们非常适合储层构建。然而，在基于忆阻器的RC中实现高性能仍然具有挑战性，因为它关键性地取决于输入预处理方法和储层大小。尽管兴趣日益增长，但目前仍缺乏量化这些因素影响的全面评估。本文系统地比较了忆阻器RC系统中的各种预处理方法，评估它们对准确性和能耗的影响。我们还提出了一种基于奇偶校验的预处理方法，该方法能将准确率提高2-6%，同时相比其他方法仅需适度增加设备数量。我们的研究结果强调了知情预处理策略对于提高忆阻器RC系统效率和可扩展性的重要性。", "summary": "本研究系统地比较了忆阻器储层计算（RC）系统中用于图像识别的各种预处理方法，旨在解决其性能对输入预处理和储层大小的依赖性。研究评估了这些方法对准确性和能耗的影响，并提出了一种新的基于奇偶校验的预处理方法。实验结果表明，该方法能够将准确率提高2-6%，且仅需适度增加设备数量。这凸显了恰当的预处理策略对于提升忆阻器RC系统效率和可扩展性的关键作用。", "keywords": "忆阻器储层计算, 预处理方法, 图像识别, 准确率, 能耗", "comments": "该论文创新性地提出了一种基于奇偶校验的预处理方法，有效提升了忆阻器储层计算在图像识别任务中的准确性。其重要性在于为未来忆阻器RC系统的设计和优化提供了有价值的指导，特别是在资源受限或需要高效率的边缘计算场景下。局限性可能在于未深入探讨其他影响因素（如储层大小）或更复杂的预处理策略。"}}
{"id": "2506.05994", "pdf": "https://arxiv.org/pdf/2506.05994", "abs": "https://arxiv.org/abs/2506.05994", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "Camélia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "categories": ["cs.LG", "cs.AR", "cs.ET"], "comment": null, "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "AI": {"title_translation": "RETENTION：基于内容寻址存储器的资源高效树形集成模型加速", "tldr": "RETENTION是一个高效框架，通过剪枝算法和树映射方案，显著降低内容寻址存储器（CAM）对树形模型推理的容量需求，大幅提升内存效率，同时保持高精度。", "motivation": "现代树形集成模型在处理结构化数据方面表现优异，但其固有特性对传统加速器构成挑战。尽管基于内容寻址存储器（CAM）的加速方案有前景，但现有设计存在内存消耗过大和利用率低的问题。本文旨在解决这些挑战。", "method": "提出RETENTION端到端框架。1. 一种迭代剪枝算法，具有针对bagging模型（如随机森林）的新颖剪枝准则，可在控制精度下降的同时最小化模型复杂度。2. 一种树映射方案，结合两种创新的数据放置策略，以减轻内容寻址存储器（CAM）中“不关心”状态导致的内存冗余。", "result": "单独实施树映射方案可实现1.46倍至21.30倍的空间效率提升。完整的RETENTION框架可实现4.35倍至207.12倍的改进，且精度损失小于3%。", "conclusion": "RETENTION在降低内容寻址存储器（CAM）容量需求方面非常有效，为树形模型加速提供了一个资源高效的方向。", "translation": "尽管深度学习在从非结构化数据中学习方面展现出卓越的能力，但现代树形集成模型在从结构化数据集中提取相关信息和学习方面仍然表现更优。虽然已经有一些努力来加速树形模型，但模型的固有特性对传统加速器提出了重大挑战。最近利用内容寻址存储器（CAM）的研究为加速树形模型提供了一个有前景的解决方案，然而现有设计存在内存消耗过大和利用率低的问题。这项工作通过引入RETENTION来解决这些挑战，RETENTION是一个端到端框架，显著降低了树形模型推理对内容寻址存储器（CAM）的容量需求。我们提出了一种迭代剪枝算法，该算法具有针对基于bagging的模型（例如随机森林）的新颖剪枝准则，可在确保控制精度下降的同时最小化模型复杂度。此外，我们提出了一种树映射方案，该方案结合了两种创新的数据放置策略，以减轻内容寻址存储器（CAM）中“不关心”状态广泛使用所导致的内存冗余。实验结果表明，单独实施树映射方案可实现1.46倍至21.30倍的空间效率提升，而完整的RETENTION框架可实现4.35倍至207.12倍的改进，且精度损失小于3%。这些结果表明，RETENTION在降低内容寻址存储器（CAM）容量需求方面非常有效，为树形模型加速提供了一个资源高效的方向。", "summary": "RETENTION是一个旨在加速树形集成模型推理的端到端框架。它通过迭代剪枝算法降低模型复杂度，并采用创新的树映射方案减少内容寻址存储器（CAM）的内存冗余。实验证明，该框架能大幅提高空间效率（最高达207.12倍），同时保持极低的精度损失（小于3%），为资源高效的树形模型加速提供了新方向。", "keywords": "树形模型, 内容寻址存储器, 模型加速, 资源高效, 剪枝, 树映射", "comments": "创新点在于解决了基于内容寻址存储器（CAM）的树模型加速器中内存消耗过高和利用率低的关键问题。提出的迭代剪枝算法和新颖的树映射方案是解决这些问题的创新方法。其重要性在于，树形模型对于结构化数据仍然至关重要，提高其加速效率对实际应用具有重要意义。这种资源高效的方法非常有价值。"}}
