{"id": "2506.05566", "pdf": "https://arxiv.org/pdf/2506.05566", "abs": "https://arxiv.org/abs/2506.05566", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "AI": {"title_translation": "ScaleRTL：通过推理数据和测试时计算扩展大型语言模型以实现精确的RTL代码生成", "tldr": "ScaleRTL是一个结合了高质量推理数据和测试时计算的推理型大型语言模型，显著提高了RTL代码生成的准确性，并在VerilogEval和RTLLM上达到了最先进的性能。", "motivation": "尽管大型语言模型在软件编码方面表现出色，但由于高质量训练数据稀缺以及缺乏测试时扩展支持（非推理特性），它们在RTL代码生成方面的效果有限。先前的努力未能从根本上解决数据瓶颈。", "method": "本文引入了ScaleRTL，第一个用于RTL编码的推理型大型语言模型。它通过以下方式实现：1. 策划了一个包含平均56K token的长链式思维推理轨迹的多样化数据集，总计3.5B token，捕获了丰富的RTL知识。2. 在此语料库上对通用推理模型进行微调。3. 通过一种新颖的测试时扩展策略进一步增强性能，该策略通过迭代反思和自我纠正先前的推理步骤来扩展推理过程。", "result": "ScaleRTL在VerilogEval和RTLLM上实现了最先进的性能，在VerilogEval上比18个有竞争力的基线高出18.4%，在RTLLM上高出12.7%。", "conclusion": "ScaleRTL通过结合高质量的推理数据和创新的测试时计算策略，成功解决了RTL代码生成中数据稀缺和推理能力不足的问题，显著提升了大型语言模型在该领域的性能。", "translation": "大型语言模型（LLMs）的最新进展使得它们在软件编码基准上达到了接近人类的性能，但由于高质量训练数据的稀缺，它们在RTL代码生成方面的有效性仍然有限。尽管先前的努力已经针对RTL任务对LLMs进行了微调，但它们并未从根本上克服数据瓶颈，并且由于其非推理性质而缺乏对测试时扩展的支持。在这项工作中，我们引入了ScaleRTL，这是第一个用于RTL编码的推理型LLM，它同时扩展了高质量推理数据和测试时计算。具体来说，我们策划了一组多样化的长链式思维推理轨迹，平均每个56K token，从而形成了一个包含3.5B token的数据集，捕获了丰富的RTL知识。在此语料库上对通用推理模型进行微调，产生了能够进行深度RTL推理的ScaleRTL。随后，我们通过一种新颖的测试时扩展策略进一步增强了ScaleRTL的性能，该策略通过迭代反思和自我纠正先前的推理步骤来扩展推理过程。实验结果表明，ScaleRTL在VerilogEval和RTLLM上实现了最先进的性能，在VerilogEval上比18个有竞争力的基线高出18.4%，在RTLLM上高出12.7%。", "summary": "本研究提出了ScaleRTL，一个专为RTL代码生成设计的推理型大型语言模型，旨在克服现有模型因高质量训练数据稀缺和缺乏测试时扩展能力而导致的局限性。ScaleRTL通过构建一个包含大量链式思维推理轨迹的高质量数据集，并在此基础上微调通用推理模型来实现深度RTL推理。此外，它采用了一种新颖的测试时扩展策略，通过迭代反思和自我纠正来提升性能。实验结果表明，ScaleRTL在VerilogEval和RTLLM基准测试中均取得了最先进的性能，显著超越了现有基线。", "keywords": "RTL代码生成, 大型语言模型, 推理数据, 测试时计算, VerilogEval", "comments": "ScaleRTL的创新之处在于它首次将推理数据和测试时计算相结合，以解决RTL代码生成中大型语言模型面临的数据瓶颈和推理能力不足的问题。通过构建大规模高质量的链式思维推理数据集和采用迭代自我修正的测试时策略，该方法显著提升了RTL代码生成的准确性，为硬件设计自动化领域带来了重要突破。"}}
{"id": "2506.05682", "pdf": "https://arxiv.org/pdf/2506.05682", "abs": "https://arxiv.org/abs/2506.05682", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "categories": ["cs.AR"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "AI": {"title_translation": "Lumina: 利用计算冗余实现实时移动神经渲染", "tldr": "Lumina 是一种软硬件协同设计的系统，通过 S^2 算法和辐射缓存机制，显著提高了移动设备上 3D 高斯泼溅神经渲染的效率和能效。", "motivation": "3D 高斯泼溅 (3DGS) 虽然极大地推动了神经渲染的进步，但在当前的移动 SoC 上仍然计算量巨大，难以实现实时性能。", "method": "本文提出了 Lumina，一个软硬件协同设计的系统。它集成了两个主要优化：一种新颖的 S^2 算法和一种辐射缓存 (RC) 机制。S^2 算法利用渲染中的时间相干性来减少计算开销，而 RC 则利用 3DGS 的颜色积分过程来降低密集光栅化计算的频率。此外，还提出了一种加速器架构 LuminCore，以进一步加速缓存查找并解决光栅化中的基本低效率问题。", "result": "Lumina 在合成和真实世界数据集上，与移动 Volta GPU 相比，实现了 4.5 倍的加速和 5.3 倍的能耗降低，且质量损失极小（峰值信噪比降低 < 0.2 dB）。", "conclusion": "Lumina 通过软硬件协同设计，成功解决了移动设备上 3D 高斯泼溅神经渲染的计算挑战，显著提升了实时性能和能效，同时保持了高质量渲染。", "translation": "3D 高斯泼溅（3DGS）极大地推动了神经渲染的进展，但其在当今的移动 SoC 上仍然计算量巨大。为了解决这一挑战，我们提出了 Lumina，一个软硬件协同设计的系统，它集成了两项主要优化：一种新颖的 S^2 算法和一种辐射缓存（RC）机制，以提高神经渲染的效率。S^2 算法利用渲染中的时间相干性来减少计算开销，而 RC 则利用 3DGS 的颜色积分过程来降低密集光栅化计算的频率。结合这些技术，我们提出了一种加速器架构 LuminCore，以进一步加速缓存查找并解决光栅化中的基本低效率问题。我们展示了 Lumina 在合成和真实世界数据集上，与移动 Volta GPU 相比，实现了 4.5 倍的加速和 5.3 倍的能耗降低，且质量损失极小（峰值信噪比降低 < 0.2 dB）。", "summary": "Lumina 是一种针对移动设备实时神经渲染的软硬件协同设计系统，旨在解决 3D 高斯泼溅在移动 SoC 上计算量大的问题。它引入了 S^2 算法来利用时间相干性减少计算，以及辐射缓存机制来降低光栅化频率。结合专门的 LuminCore 加速器架构，Lumina 在性能和能效方面均取得了显著提升，且对渲染质量影响甚微。", "keywords": "神经渲染, 3D 高斯泼溅, 移动计算, 软硬件协同设计, 计算冗余", "comments": "Lumina 的创新之处在于其软硬件协同设计方法，特别是 S^2 算法和辐射缓存机制，以及定制的 LuminCore 加速器，这些都直接解决了移动设备上神经渲染的性能瓶颈。其显著的加速和能效提升预示着神经渲染技术在边缘设备上的广泛应用潜力，对移动 AR/VR 和其他实时图形应用具有重要意义。"}}
{"id": "2506.05588", "pdf": "https://arxiv.org/pdf/2506.05588", "abs": "https://arxiv.org/abs/2506.05588", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "categories": ["cs.NE", "cs.AR", "cs.ET"], "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "AI": {"title_translation": "用于图像识别的忆阻器储层计算预处理方法", "tldr": "本研究系统比较了忆阻器储层计算的各种预处理方法，并提出了一种基于奇偶校验的预处理方法，该方法在图像识别中可提高准确性并优化能耗。", "motivation": "忆阻器储层计算（RC）在实现高性能方面面临挑战，其性能关键取决于输入预处理方法和储层大小。目前缺乏对这些因素影响的全面评估。", "method": "本研究系统比较了用于忆阻器储层计算系统的各种预处理方法，评估了它们对准确性和能耗的影响。此外，还提出了一种基于奇偶校验的预处理方法。", "result": "提出的基于奇偶校验的预处理方法可将准确性提高2-6%，同时与现有方法相比，仅需适度增加设备数量。研究结果强调了知情预处理策略对于提高忆阻器储层计算系统效率和可扩展性的重要性。", "conclusion": "知情预处理策略对于提高忆阻器储层计算系统的效率和可扩展性至关重要。", "translation": "储层计算（RC）作为一种高效的循环神经网络架构，因其简化的训练（仅需训练其最后一个感知器读出层）而备受关注。当使用忆阻器实现时，RC系统受益于其动态特性，这使得它们成为储层构建的理想选择。然而，在基于忆阻器的RC中实现高性能仍然具有挑战性，因为它关键取决于输入预处理方法和储层大小。尽管兴趣日益增长，但仍缺乏量化这些因素影响的全面评估。本文系统地比较了忆阻器RC系统的各种预处理方法，评估了它们对准确性和能耗的影响。我们还提出了一种基于奇偶校验的预处理方法，该方法在仅需适度增加设备数量的情况下，将准确性提高了2-6%。我们的研究结果强调了知情预处理策略对于提高忆阻器RC系统效率和可扩展性的重要性。", "summary": "本论文系统地比较了用于忆阻器储层计算（RC）的各种预处理方法，旨在解决其在图像识别中实现高性能的挑战。研究评估了不同预处理方法对准确性和能耗的影响，并提出了一种新的基于奇偶校验的预处理方法，该方法在仅适度增加设备数量的情况下，将准确性提高了2-6%。研究结果强调了选择合适的预处理策略对于提升忆阻器RC系统效率和可扩展性的重要性。", "keywords": "忆阻器储层计算, 预处理方法, 图像识别, 准确性, 能耗", "comments": "本文通过系统比较和提出新方法，解决了忆阻器储层计算中预处理策略的关键问题，对提高其在图像识别中的性能和能效具有重要意义。所提出的奇偶校验方法在准确性提升和设备成本之间取得了良好平衡，具有一定的创新性。"}}
{"id": "2506.05994", "pdf": "https://arxiv.org/pdf/2506.05994", "abs": "https://arxiv.org/abs/2506.05994", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "Camélia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "categories": ["cs.LG", "cs.AR", "cs.ET"], "comment": null, "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "AI": {"title_translation": "RETENTION: 基于内容可寻址存储器的资源高效树集成模型加速", "tldr": "RETENTION框架通过迭代剪枝和树映射方案，显著降低内容可寻址存储器（CAM）对树集成模型加速的容量需求，同时保持高精度。", "motivation": "现代树集成模型在结构化数据学习上优于深度学习，但其固有特性对传统加速器构成挑战。利用内容可寻址存储器（CAM）加速树模型虽有前景，但现有设计存在内存消耗过大和利用率低的问题。", "method": "本文提出了RETENTION端到端框架，旨在显著降低树模型推理对CAM容量的需求。该框架包含：1) 针对基于bagging的模型（如随机森林）的迭代剪枝算法，其具有新颖的剪枝标准，旨在最小化模型复杂性并确保可控的精度下降。2) 树映射方案，该方案结合了两种创新的数据放置策略，以缓解CAM中广泛使用的“don't care”状态导致的内存冗余。", "result": "实验结果表明，仅实施树映射方案即可实现1.46倍至21.30倍的空间效率提升。完整的RETENTION框架可带来4.35倍至207.12倍的改进，且精度损失低于3%。", "conclusion": "RETENTION框架在降低内容可寻址存储器（CAM）容量需求方面非常有效，为树集成模型的加速提供了一个资源高效的方向。", "translation": "尽管深度学习在从非结构化数据中学习方面展示了卓越的能力，但现代树集成模型在提取相关信息和从结构化数据集中学习方面仍然更胜一筹。虽然已经为加速树模型做出了多项努力，但模型的固有特性对传统加速器构成了重大挑战。最近利用内容可寻址存储器（CAM）的研究为加速树模型提供了一个有前景的解决方案，然而现有设计存在内存消耗过大和利用率低的问题。这项工作通过引入RETENTION来应对这些挑战，RETENTION是一个端到端框架，显著降低了树模型推理对CAM容量的需求。我们提出了一种迭代剪枝算法，该算法具有为基于bagging的模型（例如随机森林）量身定制的新颖剪枝标准，可在确保可控精度下降的同时最小化模型复杂性。此外，我们提出了一种树映射方案，该方案结合了两种创新的数据放置策略，以缓解CAM中广泛使用的“don't care”状态导致的内存冗余。实验结果表明，仅实施树映射方案即可实现1.46倍至21.30倍的空间效率提升，而完整的RETENTION框架则带来了4.35倍至207.12倍的改进，且精度损失低于3%。这些结果表明，RETENTION在降低CAM容量需求方面非常有效，为树集成模型加速提供了资源高效的方向。", "summary": "本文提出了RETENTION框架，旨在解决基于内容可寻址存储器（CAM）加速树集成模型时存在的内存消耗过大和利用率低的问题。该框架通过一个迭代剪枝算法和创新的树映射方案，显著降低了CAM的容量需求。实验证明，RETENTION在大幅提高空间效率的同时，将精度损失控制在3%以内，为树集成模型的资源高效加速提供了有效途径。", "keywords": "树集成模型加速, 内容可寻址存储器, 资源高效, 迭代剪枝, 树映射", "comments": "本文针对树集成模型在内容可寻址存储器（CAM）加速中面临的内存效率问题，提出了一个创新的端到端框架RETENTION。其迭代剪枝算法和树映射方案有效地降低了CAM容量需求，同时保持了高精度，这对于资源受限的边缘设备或大规模部署具有重要意义。该工作在硬件加速与算法优化结合方面提供了新的思路。"}}
