{"id": "2506.05566", "pdf": "https://arxiv.org/pdf/2506.05566", "abs": "https://arxiv.org/abs/2506.05566", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "AI": {"title_translation": "错误：AI分析失败。", "tldr": null, "motivation": null, "method": null, "result": null, "conclusion": null, "translation": null, "summary": null, "comments": null, "keywords": null}}
{"id": "2506.05682", "pdf": "https://arxiv.org/pdf/2506.05682", "abs": "https://arxiv.org/abs/2506.05682", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "categories": ["cs.AR"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "AI": {"title_translation": "Lumina：通过利用计算冗余实现实时移动神经渲染", "tldr": "Lumina是一个软硬件协同设计的系统，通过S²算法、辐射缓存和LuminCore加速器，显著提高了移动设备上神经渲染的效率和能效，同时保持了高质量。", "motivation": "3D Gaussian Splatting (3DGS) 尽管极大地推进了神经渲染的进展，但在当前的移动系统级芯片（SoC）上仍然计算量巨大，难以实现实时性能。", "method": "本文提出了Lumina，一个软硬件协同设计的系统。该系统集成了两项主要优化：一种新颖的S²算法和一种辐射缓存（RC）机制。S²算法利用渲染中的时间一致性来减少计算开销，而RC则利用3DGS的颜色积分过程来降低密集光栅化计算的频率。此外，Lumina还提出了一种名为LuminCore的加速器架构，以进一步加速缓存查找并解决光栅化中的基本效率低下问题。", "result": "Lumina 在合成和真实世界数据集上，相对于移动Volta GPU，实现了4.5倍的速度提升和5.3倍的能耗降低，同时图像质量损失很小（峰值信噪比降低小于0.2 dB）。", "conclusion": "Lumina通过软硬件协同设计，成功解决了移动设备上神经渲染的计算密集型挑战，实现了显著的性能和能效提升，同时保持了高渲染质量。", "translation": "3D高斯泼溅（3DGS）极大地推动了神经渲染的进展，但它在当今的移动SoC上仍然计算量巨大。为了解决这一挑战，我们提出了Lumina，一个软硬件协同设计的系统，它集成了两项主要优化：一种新颖的S²算法和一种辐射缓存（RC）机制，以提高神经渲染的效率。S²算法利用渲染中的时间一致性来减少计算开销，而RC则利用3DGS的颜色积分过程来降低密集光栅化计算的频率。结合这些技术，我们提出了一种加速器架构LuminCore，以进一步加速缓存查找并解决光栅化中的根本性效率低下问题。我们表明，Lumina在合成和真实世界数据集上，相对于移动Volta GPU，实现了4.5倍的速度提升和5.3倍的能耗降低，同时质量损失微乎其微（峰值信噪比降低小于0.2 dB）。", "summary": "本文介绍了Lumina，一个针对移动设备实时神经渲染的软硬件协同设计系统。该系统通过引入S²算法（利用时间一致性）和辐射缓存（RC，减少光栅化频率）来优化3D Gaussian Splatting (3DGS) 的计算效率。此外，Lumina还包含一个专用的LuminCore加速器架构，以进一步提升缓存查找和解决光栅化效率问题。实验结果表明，Lumina在保持高图像质量的同时，实现了比移动Volta GPU快4.5倍的速度和5.3倍的能效。", "keywords": "神经渲染, 3D Gaussian Splatting, 移动计算, 硬件加速, 计算冗余", "comments": "这篇论文的创新点在于其软硬件协同设计的方法，特别是在移动平台上优化神经渲染。S²算法和辐射缓存机制的结合，以及定制的LuminCore加速器，共同解决了3DGS在移动SoC上计算量大的核心问题。其显著的性能和能效提升，同时保持了可接受的质量损失，表明了该方法在推动实时移动神经渲染方面的巨大潜力。这对于AR/VR等移动应用领域具有重要意义。"}}
{"id": "2506.05588", "pdf": "https://arxiv.org/pdf/2506.05588", "abs": "https://arxiv.org/abs/2506.05588", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "categories": ["cs.NE", "cs.AR", "cs.ET"], "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "AI": {"title_translation": "忆阻器储层计算用于图像识别的预处理方法", "tldr": "本文系统比较了忆阻器储层计算中各种预处理方法对精度和能耗的影响，并提出了一种基于奇偶校验的预处理方法，可将精度提高2-6%。", "motivation": "忆阻器储层计算（RC）在图像识别中实现高性能仍面临挑战，这主要取决于输入预处理方法和储层大小，目前缺乏对这些因素影响的全面评估。", "method": "本文系统比较了忆阻器储层计算系统中各种预处理方法，评估了它们对精度和能耗的影响。此外，还提出了一种基于奇偶校验的预处理方法。", "result": "本文提出的基于奇偶校验的预处理方法可将精度提高2-6%，同时与其他方法相比，所需的器件数量仅适度增加。研究结果强调了明智的预处理策略对于提高忆阻器储层计算系统效率和可扩展性的重要性。", "conclusion": "明智的预处理策略对于提高忆阻器储层计算系统的效率和可扩展性至关重要。", "translation": "储层计算（RC）作为一种高效的循环神经网络架构，因其简化的训练（仅需训练其最后一个感知器读出层）而备受关注。当使用忆阻器实现时，RC系统受益于其动态特性，这使得它们非常适合储层构建。然而，在基于忆阻器的RC中实现高性能仍然具有挑战性，因为它关键地取决于输入预处理方法和储层大小。尽管兴趣日益增长，但目前仍缺乏量化这些因素影响的全面评估。本文系统比较了忆阻器RC系统中各种预处理方法，评估了它们对精度和能耗的影响。我们还提出了一种基于奇偶校验的预处理方法，该方法可将精度提高2-6%，同时与其他方法相比，所需的器件数量仅适度增加。我们的研究结果强调了明智的预处理策略对于提高忆阻器RC系统效率和可扩展性的重要性。", "summary": "本研究系统地比较了忆阻器储层计算（RC）中不同的预处理方法，以评估它们对图像识别精度和能耗的影响。文章指出，尽管忆阻器RC因其动态特性而具有潜力，但其性能受预处理方法和储层大小的显著影响。为解决这一问题，研究者提出了一种基于奇偶校验的预处理方法，该方法在仅适度增加器件数量的情况下，将系统精度提高了2-6%。研究结果强调了选择合适的预处理策略对于提升忆阻器RC系统效率和可扩展性的关键作用。", "keywords": "忆阻器储层计算, 图像识别, 预处理方法, 精度, 能耗", "comments": "本文通过对忆阻器储层计算中预处理方法的系统比较，并提出了一种新的基于奇偶校验的预处理方法，为提高忆阻器RC系统的性能和效率提供了有价值的见解。其创新点在于量化了不同预处理方法的影响，并提供了一个可行的解决方案。"}}
{"id": "2506.05994", "pdf": "https://arxiv.org/pdf/2506.05994", "abs": "https://arxiv.org/abs/2506.05994", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "Camélia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "categories": ["cs.LG", "cs.AR", "cs.ET"], "comment": null, "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "AI": {"title_translation": "RETENTION: 基于内容寻址存储器的资源高效树形集成模型加速", "tldr": "RETENTION通过剪枝和高效数据放置显著降低了树形模型推理对CAM容量的需求，以极小的精度损失实现了显著的空间效率提升。", "motivation": "现代树形集成模型在处理结构化数据集方面表现出色，但其固有特性对传统加速器构成挑战。尽管基于内容寻址存储器（CAM）的解决方案有前景，但现有设计存在内存消耗过大和利用率低的问题。", "method": "提出了RETENTION，一个端到端框架。该框架提出了一种针对袋装模型（如随机森林）的迭代剪枝算法，该算法具有新颖的剪枝准则，旨在最小化模型复杂性同时确保可控的精度下降。此外，它还提出了一种树映射方案，该方案结合了两种创新的数据放置策略，以减轻CAM中广泛使用“无关状态”导致的内存冗余。", "result": "单独实施树映射方案可实现1.46倍至21.30倍的空间效率提升；而完整的RETENTION框架则在精度损失小于3%的情况下，实现了4.35倍至207.12倍的改进。", "conclusion": "RETENTION在降低CAM容量需求方面非常有效，为树形模型加速提供了一个资源高效的方向。", "translation": "尽管深度学习在从非结构化数据中学习方面展现出卓越的能力，但现代树形集成模型在从结构化数据集中提取相关信息和学习方面仍然更胜一筹。尽管已经有一些努力来加速树形模型，但模型固有的特性对传统加速器构成了重大挑战。最近利用内容寻址存储器（CAM）的研究为加速树形模型提供了一个有前景的解决方案，然而现有设计存在内存消耗过大和利用率低的问题。这项工作通过引入RETENTION来解决这些挑战，RETENTION是一个端到端框架，它显著降低了树形模型推理对CAM容量的需求。我们提出了一种迭代剪枝算法，该算法具有针对袋装模型（例如随机森林）的新颖剪枝准则，可以在最小化模型复杂性的同时确保可控的精度下降。此外，我们提出了一种树映射方案，该方案结合了两种创新的数据放置策略，以减轻CAM中广泛使用“无关状态”导致的内存冗余。实验结果表明，单独实施树映射方案可实现1.46倍至21.30倍的空间效率提升，而完整的RETENTION框架则在精度损失小于3%的情况下，实现了4.35倍至207.12倍的改进。这些结果表明RETENTION在降低CAM容量需求方面非常有效，为树形模型加速提供了一个资源高效的方向。", "summary": "本文介绍了RETENTION，一个端到端框架，旨在通过解决现有CAM设计中内存消耗过大和利用率低的问题，利用内容寻址存储器（CAM）加速树形集成模型。该框架提出了一种针对袋装模型的迭代剪枝算法，以在可控的精度损失下降低模型复杂性，并提出了一种具有新颖数据放置策略的树映射方案，以减轻CAM内存冗余。实验结果表明，在精度损失极小（<3%）的情况下，空间效率显著提高（单独映射方案可达21.30倍，完整框架可达207.12倍）。", "keywords": "树形模型, 内容寻址存储器 (CAM), 模型加速, 剪枝, 数据放置", "comments": "该论文的创新之处在于通过模型剪枝和优化数据放置的组合方法，解决了CAM在树模型加速中的内存效率问题。这为在硬件加速器上部署树形模型提供了一个实用且资源高效的解决方案。"}}
