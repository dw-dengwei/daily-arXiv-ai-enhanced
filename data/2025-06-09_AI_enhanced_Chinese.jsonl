{"id": "2506.05566", "pdf": "https://arxiv.org/pdf/2506.05566", "abs": "https://arxiv.org/abs/2506.05566", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "AI": {"title_translation": "ScaleRTL: 利用推理数据和测试时计算扩展LLM以实现准确的RTL代码生成", "tldr": "ScaleRTL通过高质量推理数据和测试时计算，显著提升LLM在RTL代码生成方面的性能，达到最先进水平。", "motivation": "尽管大型语言模型（LLM）在软件编码方面表现出色，但在RTL代码生成方面仍受限于高质量训练数据的稀缺性，且现有方法未能从根本上克服数据瓶颈或支持测试时扩展。", "method": "本文引入了ScaleRTL，第一个用于RTL编码的推理LLM。它通过以下方式实现：首先，整理并构建了一个包含35亿tokens的多样化长链式思维推理轨迹数据集，捕获了丰富的RTL知识。其次，在此语料库上对通用推理模型进行微调，使其具备深度RTL推理能力。最后，通过一种新颖的测试时扩展策略进一步增强性能，该策略通过迭代反思和自我纠正先前的推理步骤来扩展推理过程。", "result": "实验结果表明，ScaleRTL在VerilogEval和RTLLM上取得了最先进的性能，在VerilogEval上超越18个竞争基线高达18.4%，在RTLLM上超越12.7%。", "conclusion": "ScaleRTL通过结合大规模高质量推理数据和创新的测试时计算策略，显著提升了大型语言模型在RTL代码生成领域的准确性和性能，成功克服了现有方法的局限性。", "translation": "大型语言模型（LLM）的最新进展已使其在软件编码基准测试中达到接近人类的性能，但由于高质量训练数据的稀缺性，它们在RTL代码生成方面的有效性仍然有限。尽管之前的努力已经针对RTL任务对LLM进行了微调，但它们未能从根本上克服数据瓶颈，并且由于其非推理性质，缺乏对测试时扩展的支持。在这项工作中，我们引入了ScaleRTL，这是第一个用于RTL编码的推理LLM，它同时扩展了高质量推理数据和测试时计算。具体来说，我们整理了一组多样化的长链式思维推理轨迹，每条平均56K tokens，形成了一个包含35亿tokens的数据集，捕获了丰富的RTL知识。在此语料库上对通用推理模型进行微调，得到了能够进行深度RTL推理的ScaleRTL。随后，我们通过一种新颖的测试时扩展策略进一步提升了ScaleRTL的性能，该策略通过迭代反思和自我纠正先前的推理步骤来扩展推理过程。实验结果表明，ScaleRTL在VerilogEval和RTLLM上取得了最先进的性能，在VerilogEval上超越18个竞争基线高达18.4%，在RTLLM上超越12.7%。", "summary": "ScaleRTL是一种新型的RTL代码生成LLM，旨在解决现有模型在RTL领域面临的数据稀缺和推理能力不足问题。该模型通过构建一个包含35亿tokens的大规模高质量链式思维推理数据集进行训练，并引入了一种新颖的测试时扩展策略，通过迭代反思和自我纠正来增强推理过程。实验证明，ScaleRTL在VerilogEval和RTLLM等基准测试中取得了最先进的性能，显著超越了现有竞争基线。", "keywords": "RTL代码生成, LLM, 推理数据, 测试时计算, 链式思维", "comments": "该论文的创新点在于结合了高质量的链式思维推理数据和新颖的测试时自我修正策略，有效解决了RTL代码生成中数据稀缺和推理能力不足的问题。其重要性在于为未来LLM在专业领域（如硬件设计）的应用提供了新的范式，展示了数据质量和推理过程扩展的重要性。"}}
{"id": "2506.05682", "pdf": "https://arxiv.org/pdf/2506.05682", "abs": "https://arxiv.org/abs/2506.05682", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "categories": ["cs.AR"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "AI": {"title_translation": "Lumina：通过利用计算冗余实现实时移动神经渲染", "tldr": "Lumina是一个软硬件协同设计系统，通过S^2算法和辐射缓存机制，以及LuminCore加速器，显著提高了移动设备上神经渲染的效率，同时保持了图像质量。", "motivation": "3D Gaussian Splatting (3DGS) 尽管推进了神经渲染，但在今天的移动SoC上计算开销仍然很大。", "method": "提出Lumina，一个软硬件协同设计的系统，它整合了S^2算法和辐射缓存（RC）机制，以提高神经渲染效率。S^2算法利用渲染中的时间相干性减少计算开销，而RC则利用3DGS的颜色积分过程降低密集光栅化计算的频率。此外，还提出了LuminCore加速器架构，以进一步加速缓存查找并解决光栅化的基本低效率问题。", "result": "Lumina相对于移动Volta GPU实现了4.5倍的速度提升和5.3倍的能耗降低，同时在合成和真实世界数据集上仅有微小的质量损失（峰值信噪比降低<0.2 dB）。", "conclusion": "Lumina通过软硬件协同设计，成功解决了移动设备上神经渲染的计算挑战，实现了显著的性能和能效提升，同时保持了高质量。", "translation": "3D高斯泼溅（3DGS）极大地推动了神经渲染的进步，但它在当今的移动SoC上仍然计算量巨大。为了解决这一挑战，我们提出了Lumina，一个软硬件协同设计的系统，它整合了两个主要优化：一个新颖的算法S^2和一种辐射缓存机制RC，以提高神经渲染的效率。S^2算法利用渲染中的时间相干性来减少计算开销，而RC则利用3DGS的颜色积分过程来降低密集光栅化计算的频率。结合这些技术，我们提出了一种加速器架构LuminCore，以进一步加速缓存查找并解决光栅化中的根本低效率问题。我们展示了Lumina相对于移动Volta GPU实现了4.5倍的速度提升和5.3倍的能耗降低，同时在合成和真实世界数据集上仅有微小的质量损失（峰值信噪比降低<0.2 dB）。", "summary": "本文提出了Lumina，一个软硬件协同设计的系统，旨在解决3D高斯泼溅（3DGS）在移动设备上计算量大的问题。Lumina集成了S^2算法和辐射缓存（RC）机制，S^2利用时间相干性减少计算，RC则通过颜色积分减少光栅化频率。此外，Lumina还包含一个名为LuminCore的加速器架构，以进一步优化缓存查找和光栅化效率。实验结果表明，Lumina在保持极低图像质量损失的前提下，相比移动Volta GPU实现了4.5倍的速度提升和5.3倍的能耗降低。", "keywords": "神经渲染, 3D高斯泼溅, 移动计算, 硬件加速, 计算冗余", "comments": "这篇论文通过软硬件协同设计，为移动设备上的实时神经渲染提供了一个创新的解决方案。其S^2算法和辐射缓存机制有效减少了计算冗余，而LuminCore加速器则针对性地解决了光栅化效率问题。这种全面的优化方法对于推动神经渲染在边缘设备上的应用具有重要意义，尤其是在能耗和性能受限的移动平台上。"}}
{"id": "2506.05588", "pdf": "https://arxiv.org/pdf/2506.05588", "abs": "https://arxiv.org/abs/2506.05588", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "categories": ["cs.NE", "cs.AR", "cs.ET"], "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "AI": {"title_translation": "忆阻器储层计算在图像识别中的预处理方法", "tldr": "本文系统比较了忆阻器储层计算（RC）中不同的预处理方法，并提出了一种基于奇偶校验的预处理方法，可提高图像识别精度并优化能耗。", "motivation": "尽管忆阻器储层计算（RC）在简化训练方面具有优势，但其性能仍受输入预处理方法和储层大小的显著影响。目前缺乏对这些因素影响的全面评估，这阻碍了忆阻器RC系统实现高性能。", "method": "本文系统地比较了忆阻器RC系统中各种预处理方法，评估它们对精度和能耗的影响。此外，本文还提出了一种基于奇偶校验的预处理方法。", "result": "研究结果表明，所提出的基于奇偶校验的预处理方法可以将精度提高2-6%，同时与其他方法相比，仅需适度增加器件数量。", "conclusion": "研究强调了知情的预处理策略对于提高忆阻器储层计算系统效率和可扩展性的重要性。", "translation": "储层计算（RC）作为一种高效的循环神经网络架构，因其简化的训练（仅需训练其最后一个感知器读出层）而备受关注。当使用忆阻器实现时，RC系统受益于其动态特性，这使得它们非常适合储层构建。然而，在基于忆阻器的RC中实现高性能仍然具有挑战性，因为它关键性地取决于输入预处理方法和储层大小。尽管兴趣日益增长，但仍然缺乏量化这些因素影响的全面评估。本文系统地比较了忆阻器RC系统中各种预处理方法，评估它们对精度和能耗的影响。我们还提出了一种基于奇偶校验的预处理方法，可将精度提高2-6%，同时与其他方法相比，仅需适度增加器件数量。我们的发现强调了知情的预处理策略对于提高忆阻器RC系统效率和可扩展性的重要性。", "summary": "本文针对忆阻器储层计算（RC）在图像识别中的性能挑战，系统比较了多种预处理方法对精度和能耗的影响。研究提出了一种新的基于奇偶校验的预处理方法，该方法在仅适度增加器件数量的前提下，可将系统精度提升2-6%。研究结果强调了优化预处理策略对于提高忆阻器RC系统效率和可扩展性的关键作用。", "keywords": "忆阻器，储层计算，预处理，图像识别，能耗", "comments": "本文的创新点在于系统地比较了忆阻器储层计算中的多种预处理方法，并提出了一种新型的基于奇偶校验的预处理方法，有效提升了图像识别精度。这项工作对于推动忆阻器RC系统的实际应用和性能优化具有重要意义，尤其是在能源效率和可扩展性方面。"}}
{"id": "2506.05994", "pdf": "https://arxiv.org/pdf/2506.05994", "abs": "https://arxiv.org/abs/2506.05994", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "Camélia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "categories": ["cs.LG", "cs.AR", "cs.ET"], "comment": null, "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "AI": {"title_translation": "RETENTION：利用内容寻址存储器加速资源高效的树型集成模型", "tldr": "RETENTION是一个端到端框架，通过迭代剪枝算法和创新的树映射方案，显著减少了树型集成模型推理对内容寻址存储器（CAM）的容量需求，同时保持高精度。", "motivation": "尽管深度学习在非结构化数据学习方面表现出色，但现代树型集成模型在结构化数据的信息提取和学习方面仍具优势。然而，现有加速树型模型的方法面临挑战，特别是内容寻址存储器（CAM）的现有设计存在内存消耗过大和利用率低的问题。", "method": "本文提出了RETENTION框架，旨在显著降低树型模型推理所需的CAM容量。该框架包含一个带有新颖剪枝准则的迭代剪枝算法，专为基于Bagging的模型（如随机森林）设计，以最小化模型复杂性并控制精度下降。此外，还提出了一种树映射方案，结合了两种创新的数据放置策略，以减轻CAM中广泛使用“无关状态”导致的内存冗余。", "result": "实验结果表明，仅实施树映射方案即可实现1.46倍至21.30倍的空间效率提升；而完整的RETENTION框架则可实现4.35倍至207.12倍的提升，且精度损失小于3%。", "conclusion": "RETENTION在降低CAM容量需求方面非常有效，为树型模型加速提供了一个资源高效的方向。", "translation": "尽管深度学习在从非结构化数据中学习方面表现出卓越的能力，但现代树型集成模型在提取相关信息和从结构化数据集中学习方面仍然更胜一筹。虽然已经做出了多项努力来加速树型模型，但模型的固有特性对传统加速器构成了重大挑战。最近利用内容寻址存储器（CAM）的研究为加速树型模型提供了一个有前景的解决方案，但现有设计存在内存消耗过大和利用率低的问题。这项工作通过引入RETENTION来解决这些挑战，RETENTION是一个端到端框架，显著降低了树型模型推理所需的CAM容量。我们提出了一种迭代剪枝算法，其剪枝准则专为基于Bagging的模型（例如随机森林）量身定制，最大限度地降低了模型复杂性，同时确保了可控的精度下降。此外，我们提出了一种树映射方案，其中包含了两种创新的数据放置策略，以减轻CAM中广泛使用“无关状态”导致的内存冗余。实验结果表明，仅实施树映射方案即可实现1.46倍至21.30倍的空间效率提升，而完整的RETENTION框架则可实现4.35倍至207.12倍的提升，且精度损失小于3%。这些结果表明RETENTION在降低CAM容量需求方面非常有效，为树型模型加速提供了一个资源高效的方向。", "summary": "本文介绍了RETENTION，一个旨在加速树型集成模型的端到端框架，通过优化内容寻址存储器（CAM）的使用，解决现有设计中内存消耗过大和利用率低的问题。RETENTION采用迭代剪枝算法和创新的树映射方案，显著减少了树型模型推理所需的CAM容量，同时确保精度损失可控。实验证明，该框架在空间效率和整体性能上均实现了显著提升。", "keywords": "树型集成模型, 内容寻址存储器, 模型加速, 剪枝算法, 内存效率", "comments": "该论文的创新点在于提出了一个端到端的框架RETENTION，特别针对树型集成模型在内容寻址存储器（CAM）上的加速问题。其迭代剪枝算法和创新的树映射方案有效解决了CAM的内存冗余和低利用率问题，实现了显著的资源效率提升。这对于在资源受限环境下部署高性能树型模型具有重要意义。"}}
